{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RLLTE: Long-Term Evolution Project of Reinforcement Learning","text":"<p>Inspired by the long-term evolution (LTE) standard project in telecommunications, aiming to provide development components and standards for advancing RL research and applications. Beyond delivering top-notch algorithm implementations, RLLTE also serves as a toolkit for developing algorithms.</p>  An introduction to RLLTE."},{"location":"#why-rllte","title":"Why RLLTE?","text":"<ul> <li>\ud83e\uddec Long-term evolution for providing latest algorithms and tricks;</li> <li>\ud83c\udfde\ufe0f Complete ecosystem for task design, model training, evaluation, and deployment (TensorRT, CANN, ...);</li> <li>\ud83e\uddf1 Module-oriented design for complete decoupling of RL algorithms;</li> <li>\ud83d\ude80 Optimized workflow for full hardware acceleration;</li> <li>\u2699\ufe0f Support custom environments and modules;</li> <li>\ud83d\udda5\ufe0f Support multiple computing devices like GPU and NPU;</li> <li>\ud83d\udcbe Large number of reusable benchmarks (RLLTE Hub);</li> <li>\ud83d\udc68\u200d\u2708\ufe0f Large language model-empowered copilot (RLLTE Copilot).</li> </ul>"},{"location":"#a-pytorch-for-rl","title":"A <code>PyTorch</code> for RL","text":"<p>RLLTE decouples RL algorithms into minimum primitives and provide standard modules for development. </p> <p>See Fast Algorithm Development for detailed examples.</p>"},{"location":"#project-evolution","title":"Project Evolution","text":"<p>RLLTE selects RL algorithms based on the following tenet:</p> <ul> <li>Generality is the most important;</li> <li>Improvements in sample efficiency or generalization ability;</li> <li>Excellent performance on recognized benchmarks;</li> <li>Promising tools for RL.</li> </ul>"},{"location":"#cite-us","title":"Cite Us","text":"<p>If you use RLLTE in your research, please cite this project like this: <pre><code>@article{yuan2023rllte,\n  title={RLLTE: Long-Term Evolution Project of Reinforcement Learning}, \n  author={Mingqi Yuan and Zequn Zhang and Yang Xu and Shihao Luo and Bo Li and Xin Jin and Wenjun Zeng},\n  year={2023},\n  journal={arXiv preprint arXiv:2309.16382}\n}\n</code></pre></p>"},{"location":"README-zh-Hans/","title":"README zh Hans","text":"RLLTE: \u5f3a\u5316\u5b66\u4e60\u957f\u671f\u6f14\u8fdb\u8ba1\u5212    \u8bba\u6587  |  \u6587\u6863  |  \u793a\u4f8b  |  \u8bba\u575b  |  \u57fa\u7ebf     | [English](README.md) | [\u4e2d\u6587](docs/README-zh-Hans.md) |"},{"location":"README-zh-Hans/#contents","title":"Contents","text":"<ul> <li>\u6982\u8ff0</li> <li>\u5feb\u901f\u5165\u95e8</li> <li>\u5b89\u88c5</li> <li>\u5feb\u901f\u8bad\u7ec3<ul> <li>\u8fd0\u7528NVIDIA GPU</li> <li>\u8fd0\u7528HUAWEI NPU</li> </ul> </li> <li>\u4e09\u6b65\u521b\u5efa\u60a8\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53</li> <li>\u7b97\u6cd5\u89e3\u8026\u4e0e\u6a21\u5757\u66ff\u4ee3</li> <li>\u529f\u80fd\u5217\u8868 (\u90e8\u5206)</li> <li>\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53</li> <li>\u5185\u5728\u5956\u52b1\u6a21\u5757</li> <li>RLLTE\u751f\u6001\u73af\u5883</li> <li>API \u6587\u6863</li> <li>\u5f15\u7528\u9879\u76ee</li> <li>\u5982\u4f55\u8d21\u732e</li> <li>\u81f4\u8c22</li> </ul>"},{"location":"README-zh-Hans/#_1","title":"\u6982\u8ff0","text":"<p>\u53d7\u901a\u4fe1\u9886\u57df\u957f\u671f\u6f14\u8fdb\uff08LTE\uff09\u6807\u51c6\u9879\u76ee\u7684\u542f\u53d1\uff0cRLLTE\u65e8\u5728\u63d0\u4f9b\u7528\u4e8e\u63a8\u8fdbRL\u7814\u7a76\u548c\u5e94\u7528\u7684\u5f00\u53d1\u7ec4\u4ef6\u548c\u5de5\u7a0b\u6807\u51c6\u3002\u9664\u4e86\u63d0\u4f9b\u4e00\u6d41\u7684\u7b97\u6cd5\u5b9e\u73b0\u5916\uff0cRLLTE\u8fd8\u80fd\u591f\u5145\u5f53\u5f00\u53d1\u7b97\u6cd5\u7684\u5de5\u5177\u5305\u3002</p>  RLLTE\u7b80\u4ecb.  <p>RLLTE\u9879\u76ee\u7279\u8272\uff1a - \ud83e\uddec \u957f\u671f\u6f14\u8fdb\u4ee5\u63d0\u4f9b\u6700\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0e\u6280\u5de7\uff1b - \ud83c\udfde\ufe0f \u4e30\u5bcc\u5b8c\u5907\u7684\u9879\u76ee\u751f\u6001\uff0c\u652f\u6301\u4efb\u52a1\u8bbe\u8ba1\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u6a21\u578b\u8bc4\u4f30\u4ee5\u53ca\u6a21\u578b\u90e8\u7f72 (TensorRT, CANN, ...)\uff1b - \ud83e\uddf1 \u9ad8\u5ea6\u6a21\u5757\u5316\u7684\u8bbe\u8ba1\u4ee5\u5b9e\u73b0RL\u7b97\u6cd5\u7684\u5b8c\u5168\u89e3\u8026\uff1b - \ud83d\ude80 \u4f18\u5316\u7684\u5de5\u4f5c\u6d41\u7528\u4e8e\u786c\u4ef6\u52a0\u901f\uff1b - \u2699\ufe0f \u652f\u6301\u81ea\u5b9a\u4e49\u73af\u5883\u548c\u6a21\u5757\uff1b - \ud83d\udda5\ufe0f \u652f\u6301\u5305\u62ecGPU\u548cNPU\u7684\u591a\u79cd\u7b97\u529b\u8bbe\u5907\uff1b - \ud83d\udcbe \u5927\u91cf\u53ef\u91cd\u7528\u7684\u57fa\u7ebf\u6570\u636e (rllte-hub)\uff1b - \ud83d\udc68\u200d\u2708\ufe0f \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u6253\u9020\u7684Copilot\u3002</p> <p>\u9879\u76ee\u7ed3\u6784\u5982\u4e0b:</p> <p>\u6709\u5173\u8fd9\u4e9b\u6a21\u5757\u7684\u8be6\u7ec6\u63cf\u8ff0\uff0c\u8bf7\u53c2\u9605API\u6587\u6863\u3002</p>"},{"location":"README-zh-Hans/#_2","title":"\u5feb\u901f\u5165\u95e8","text":""},{"location":"README-zh-Hans/#_3","title":"\u5b89\u88c5","text":"<ul> <li>\u524d\u7f6e\u6761\u4ef6</li> </ul> <p>\u5f53\u524d\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528<code>Python&gt;=3.8</code>\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u521b\u5efa\u865a\u62df\u73af\u5883\uff1a <pre><code>conda create -n rllte python=3.8\n</code></pre></p> <ul> <li>\u901a\u8fc7 <code>pip</code></li> </ul> <p>\u6253\u5f00\u7ec8\u7aef\u901a\u8fc7<code>pip</code>\u5b89\u88c5 rllte\uff1a <pre><code>pip install rllte-core # \u5b89\u88c5\u57fa\u672c\u6a21\u5757\npip install rllte-core[envs] # \u5b89\u88c5\u9884\u8bbe\u7684\u4efb\u52a1\u73af\u5883\n</code></pre></p> <ul> <li>\u901a\u8fc7 <code>git</code></li> </ul> <p>\u5f00\u542f\u7ec8\u7aef\u4ece[GitHub]\u4e2d\u590d\u5236\u4ed3\u5e93(https://github.com/RLE-Foundation/rllte)\uff1a <pre><code>git clone https://github.com/RLE-Foundation/rllte.git\n</code></pre> \u5728\u8fd9\u4e4b\u540e, \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u884c\u5b89\u88c5\u6240\u9700\u7684\u5305\uff1a <pre><code>pip install -e . # \u5b89\u88c5\u57fa\u672c\u6a21\u5757\npip install -e .[envs] # \u5b89\u88c5\u9884\u8bbe\u7684\u4efb\u52a1\u73af\u5883\n</code></pre></p> <p>\u66f4\u8be6\u7ec6\u7684\u5b89\u88c5\u8bf4\u660e, \u8bf7\u53c2\u9605, \u5165\u95e8\u6307\u5357.</p>"},{"location":"README-zh-Hans/#_4","title":"\u5feb\u901f\u8bad\u7ec3\u5185\u7f6e\u7b97\u6cd5","text":"<p>RLLTE\u4e3a\u5e7f\u53d7\u8ba4\u53ef\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u5b9e\u73b0\uff0c\u5e76\u4e14\u8bbe\u8ba1\u4e86\u7b80\u5355\u53cb\u597d\u7684\u754c\u9762\u7528\u4e8e\u5e94\u7528\u6784\u5efa\u3002</p>"},{"location":"README-zh-Hans/#nvidia-gpu","title":"\u4f7f\u7528NVIDIA GPU","text":"<p>\u5047\u5982\u6211\u4eec\u8981\u7528 DrQ-v2\u7b97\u6cd5\u89e3\u51b3 DeepMind Control Suite\u4efb\u52a1, \u53ea\u9700\u7f16\u5199\u5982\u4e0b <code>train.py</code>\u6587\u4ef6\uff1a</p> <p><pre><code># import `env` and `agent` module\nfrom rllte.env import make_dmc_env \nfrom rllte.agent import DrQv2\n\nif __name__ == \"__main__\":\n    device = \"cuda:0\"\n    # \u521b\u5efa env, `eval_env` \u53ef\u9009\n    env = make_dmc_env(env_id=\"cartpole_balance\", device=device)\n    eval_env = make_dmc_env(env_id=\"cartpole_balance\", device=device)\n    # \u521b\u5efa agent\n    agent = DrQv2(env=env, eval_env=eval_env, device=device, tag=\"drqv2_dmc_pixel\")\n    # \u5f00\u59cb\u8bad\u7ec3\n    agent.train(num_train_steps=500000, log_interval=1000)\n</code></pre> \u8fd0\u884c<code>train.py</code>\u6587\u4ef6\uff0c\u5c06\u4f1a\u5f97\u5230\u5982\u4e0b\u8f93\u51fa\uff1a</p>"},{"location":"README-zh-Hans/#huawei-npu","title":"\u4f7f\u7528HUAWEI NPU","text":"<p>\u4e0e\u4e0a\u8ff0\u6848\u4f8b\u7c7b\u4f3c, \u5982\u679c\u9700\u8981\u5728 HUAWEI NPU \u4e0a\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u53ea\u9700\u5c06 <code>cuda</code> \u66ff\u6362\u4e3a <code>npu</code>\uff1a <pre><code>device = \"cuda:0\" -&gt; device = \"npu:0\"\n</code></pre></p>"},{"location":"README-zh-Hans/#_5","title":"\u4e09\u6b65\u521b\u5efa\u60a8\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53","text":"<p>\u501f\u52a9RLLTE\uff0c\u5f00\u53d1\u8005\u53ea\u9700\u4e09\u6b65\u5c31\u53ef\u4ee5\u5b9e\u73b0\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002\u63a5\u4e0b\u6765\u8fd9\u4e2a\u4f8b\u5b50\u5c06\u5c55\u793a\u5982\u4f55\u5b9e\u73b0 Advantage Actor-Critic (A2C) \u7b97\u6cd5\u7528\u4e8e\u89e3\u51b3 Atari \u6e38\u620f\uff1a  - \u9996\u5148\uff0c\u8c03\u7528\u7b97\u6cd5\u539f\u578b\uff1a <pre><code>from rllte.common.prototype import OnPolicyAgent\n</code></pre> - \u5176\u6b21\uff0c\u5bfc\u5165\u5fc5\u8981\u7684\u6a21\u5757\uff1a <pre><code>from rllte.xploit.encoder import MnihCnnEncoder\nfrom rllte.xploit.policy import OnPolicySharedActorCritic\nfrom rllte.xploit.storage import VanillaRolloutStorage\nfrom rllte.xplore.distribution import Categorical\n</code></pre> - \u8fd0\u884c\u9009\u5b9a\u7b56\u7565\u7684 <code>.describe</code> \u51fd\u6570\uff0c\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a <pre><code>OnPolicySharedActorCritic.describe()\n# Output:\n# ================================================================================\n# Name       : OnPolicySharedActorCritic\n# Structure  : self.encoder (shared by actor and critic), self.actor, self.critic\n# Forward    : obs -&gt; self.encoder -&gt; self.actor -&gt; actions\n#            : obs -&gt; self.encoder -&gt; self.critic -&gt; values\n#            : actions -&gt; log_probs\n# Optimizers : self.optimizers['opt'] -&gt; (self.encoder, self.actor, self.critic)\n# ================================================================================\n</code></pre> \u8fd9\u5c06\u4f1a\u5c55\u793a\u5f53\u524d\u7b56\u7565\u7684\u6570\u636e\u7ed3\u6784\u3002\u6700\u540e\uff0c\u5c06\u4e0a\u8ff0\u6a21\u5757\u6574\u5408\u5230\u4e00\u8d77\u5e76\u4e14\u7f16\u5199 <code>.update</code> \u51fd\u6570: <pre><code>from torch import nn\nimport torch as th\n\nclass A2C(OnPolicyAgent):\n    def __init__(self, env, tag, seed, device, num_steps) -&gt; None:\n        super().__init__(env=env, tag=tag, seed=seed, device=device, num_steps=num_steps)\n        # \u521b\u5efa\u6a21\u5757\n        encoder = MnihCnnEncoder(observation_space=env.observation_space, feature_dim=512)\n        policy = OnPolicySharedActorCritic(observation_space=env.observation_space,\n                                           action_space=env.action_space,\n                                           feature_dim=512,\n                                           opt_class=th.optim.Adam,\n                                           opt_kwargs=dict(lr=2.5e-4, eps=1e-5),\n                                           init_fn=\"xavier_uniform\"\n                                           )\n        storage = VanillaRolloutStorage(observation_space=env.observation_space,\n                                        action_space=env.action_space,\n                                        device=device,\n                                        storage_size=self.num_steps,\n                                        num_envs=self.num_envs,\n                                        batch_size=256\n                                        )\n        # \u8bbe\u5b9a\u6240\u6709\u6a21\u5757\n        self.set(encoder=encoder, policy=policy, storage=storage, distribution=Categorical)\n\n    def update(self):\n        for _ in range(4):\n            for batch in self.storage.sample():\n                # \u8bc4\u4f30\u91c7\u6837\u7684\u52a8\u4f5c\n                new_values, new_log_probs, entropy = self.policy.evaluate_actions(obs=batch.observations, actions=batch.actions)\n                # \u7b56\u7565\u635f\u5931\n                policy_loss = - (batch.adv_targ * new_log_probs).mean()\n                # \u4ef7\u503c\u635f\u5931\n                value_loss = 0.5 * (new_values.flatten() - batch.returns).pow(2).mean()\n                # \u66f4\u65b0\n                self.policy.optimizers['opt'].zero_grad(set_to_none=True)\n                (value_loss * 0.5 + policy_loss - entropy * 0.01).backward()\n                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n                self.policy.optimizers['opt'].step()\n</code></pre> \u7136\u540e\uff0c\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u8bad\u7ec3\u8be5\u667a\u80fd\u4f53\uff1a <pre><code>from rllte.env import make_atari_env\nif __name__ == \"__main__\":\n    device = \"cuda\"\n    env = make_atari_env(\"PongNoFrameskip-v4\", num_envs=8, seed=0, device=device)\n    agent = A2C(env=env, tag=\"a2c_atari\", seed=0, device=device, num_steps=128)\n    agent.train(num_train_steps=10000000)\n</code></pre> \u4e0a\u8ff0\u4f8b\u5b50\u8868\u660e\uff0c\u5229\u7528 RLLTE \u53ea\u9700\u5c11\u6570\u51e0\u884c\u4ee3\u7801\u4fbf\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u3002</p>"},{"location":"README-zh-Hans/#_6","title":"\u7b97\u6cd5\u89e3\u8026\u4e0e\u6a21\u5757\u66ff\u4ee3","text":"<p>RLLTE \u8bb8\u53ef\u5f00\u53d1\u8005\u5c06\u9884\u8bbe\u597d\u7684\u6a21\u5757\u66ff\u6362\uff0c\u4ee5\u4fbf\u4e8e\u8fdb\u884c\u7b97\u6cd5\u6027\u80fd\u6bd4\u8f83\u548c\u4f18\u5316\u3002\u5f00\u53d1\u8005\u53ef\u4ee5\u5c06\u9884\u8bbe\u6a21\u5757\u66ff\u6362\u6210\u522b\u7684\u7c7b\u578b\u7684\u5185\u7f6e\u6a21\u5757\u6216\u8005\u81ea\u5b9a\u4e49\u6a21\u5757\u3002\u5047\u8bbe\u6211\u4eec\u60f3\u8981\u5bf9\u6bd4\u4e0d\u540c\u7f16\u7801\u5668\u7684\u6548\u679c\uff0c\u53ea\u9700\u8981\u8c03\u7528\u5176\u4e2d <code>.set</code> \u51fd\u6570\uff1a <pre><code>from rllte.xploit.encoder import EspeholtResidualEncoder\nencoder = EspeholtResidualEncoder(...)\nagent.set(encoder=encoder)\n</code></pre> RLLTE \u6846\u67b6\u5341\u5206\u7b80\u4fbf\uff0c\u7ed9\u4e88\u5f00\u53d1\u8005\u4eec\u6700\u5927\u7a0b\u5ea6\u7684\u81ea\u7531\u3002\u66f4\u591a\u8be6\u7ec6\u8bf4\u660e\u8bf7\u53c2\u8003\u6559\u7a0b\u3002</p>"},{"location":"README-zh-Hans/#_7","title":"\u529f\u80fd\u5217\u8868 (\u90e8\u5206)","text":""},{"location":"README-zh-Hans/#_8","title":"\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53","text":"\u7c7b\u578b \u7b97\u6cd5 \u8fde\u7eed \u79bb\u6563 \u591a\u91cd\u4e8c\u5143 \u591a\u91cd\u79bb\u6563 \u591a\u8fdb\u7a0b NPU \ud83d\udcb0 \ud83d\udd2d On-Policy A2C \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c On-Policy PPO \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c On-Policy DrAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f On-Policy DAAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c On-Policy DrDAAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f On-Policy PPG \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy DQN \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy DDPG \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy SAC \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy TD3 \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy DrQ-v2 \u2714\ufe0f \u274c \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Distributed IMPALA \u2714\ufe0f \u2714\ufe0f \u274c \u274c \u2714\ufe0f \u274c \u274c \u274c <ul> <li>\ud83d\udc0c\uff1a\u5f00\u53d1\u4e2d\uff1b</li> <li>\ud83d\udcb0\uff1a\u652f\u6301\u5185\u5728\u5956\u52b1\u5851\u9020\uff1b</li> <li>\ud83d\udd2d\uff1a\u652f\u6301\u89c2\u6d4b\u589e\u5f3a\u3002</li> </ul>"},{"location":"README-zh-Hans/#_9","title":"\u5185\u5728\u5956\u52b1\u6a21\u5757","text":"\u7c7b\u578b \u6a21\u5757 Count-based PseudoCounts, RND Curiosity-driven ICM, GIRM, RIDE Memory-based NGU Information theory-based RE3, RISE, REVD <p>\u8be6\u7ec6\u6848\u4f8b\u8bf7\u53c2\u8003 Tutorials: Use Intrinsic Reward and Observation Augmentation\u3002</p>"},{"location":"README-zh-Hans/#rllte","title":"RLLTE \u751f\u6001\u73af\u5883","text":"<p>\u63a2\u7d22RLLTE\u751f\u6001\u4ee5\u52a0\u901f\u60a8\u7684\u7814\u7a76\uff1a</p> <ul> <li>Hub\uff1a\u63d0\u4f9b\u5feb\u901f\u8bad\u7ec3\u7684 API \u63a5\u53e3\u4ee5\u53ca\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\uff1b</li> <li>Evaluation\uff1a\u63d0\u4f9b\u53ef\u4fe1\u8d56\u7684\u6a21\u578b\u8bc4\u4f30\u6807\u51c6\uff1b</li> <li>Env\uff1a\u63d0\u4f9b\u5c01\u88c5\u5b8c\u5584\u7684\u73af\u5883\uff1b</li> <li>Deployment\uff1a\u63d0\u4f9b\u4fbf\u6377\u7684\u7b97\u6cd5\u90e8\u7f72\u63a5\u53e3\uff1b</li> <li>Pre-training\uff1a\u63d0\u4f9b\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u9884\u8bad\u7ec3\u7684\u65b9\u5f0f\uff1b</li> <li>Copilot\uff1a\u63d0\u4f9b\u5927\u8bed\u8a00\u6a21\u578b copilot\u3002</li> </ul>"},{"location":"README-zh-Hans/#api","title":"API \u6587\u6863","text":"<p>\u8bf7\u53c2\u9605\u6211\u4eec\u4fbf\u6377\u7684 API \u6587\u6863\uff1ahttps://docs.rllte.dev/</p>"},{"location":"README-zh-Hans/#_10","title":"\u5982\u4f55\u8d21\u732e","text":"<p>\u6b22\u8fce\u53c2\u4e0e\u8d21\u732e\u6211\u4eec\u7684\u9879\u76ee\uff01\u5728\u60a8\u51c6\u5907\u7f16\u7a0b\u4e4b\u524d\uff0c\u8bf7\u5148\u53c2\u9605CONTRIBUTING.md\u3002</p>"},{"location":"README-zh-Hans/#_11","title":"\u5f15\u7528\u9879\u76ee","text":"<p>\u5982\u679c\u60a8\u60f3\u5728\u7814\u7a76\u4e2d\u5f15\u7528 RLLTE\uff0c\u8bf7\u53c2\u8003\u5982\u4e0b\u683c\u5f0f\uff1a <pre><code>@software{rllte,\n  author = {Mingqi Yuan, Zequn Zhang, Yang Xu, Shihao Luo, Bo Li, Xin Jin, and Wenjun Zeng},\n  title = {RLLTE: Long-Term Evolution Project of Reinforcement Learning},\n  url = {https://github.com/RLE-Foundation/rllte},\n  year = {2023},\n}\n</code></pre></p>"},{"location":"README-zh-Hans/#_12","title":"\u81f4\u8c22","text":"<p>\u8be5\u9879\u76ee\u7531 \u9999\u6e2f\u7406\u5de5\u5927\u5b66\uff0c\u4e1c\u65b9\u7406\u5de5\u9ad8\u7b49\u7814\u7a76\u9662\uff0c\u4ee5\u53ca FLW-Foundation\u8d5e\u52a9\u3002 \u4e1c\u65b9\u7406\u5de5\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u5fc3 \u63d0\u4f9b\u4e86 GPU \u8ba1\u7b97\u5e73\u53f0, \u534e\u4e3a\u5f02\u817e \u63d0\u4f9b\u4e86 NPU \u8ba1\u7b97\u5e73\u53f0\u3002\u8be5\u9879\u76ee\u7684\u90e8\u5206\u4ee3\u7801\u53c2\u8003\u4e86\u5176\u4ed6\u4f18\u79c0\u7684\u5f00\u6e90\u9879\u76ee\uff0c\u8bf7\u53c2\u89c1 ACKNOWLEDGMENT.md\u3002</p>"},{"location":"api/","title":"Overview","text":""},{"location":"api/#architecture","title":"Architecture","text":""},{"location":"api/#agent-implemented-rl-algorithms-using-rllte-modules","title":"Agent: Implemented RL algorithms using RLLTE modules.","text":"Type Algo. Box <code>Dis.</code> <code>M.B.</code> <code>M.D.</code> <code>M.P.</code> NPU \ud83d\udcb0 \ud83d\udd2d On-Policy A2C \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c On-Policy PPO \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c On-Policy DrAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f On-Policy DAAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c On-Policy DrDAAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f On-Policy PPG \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy DQN \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy DDPG \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy SAC \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy SAC-Discrete \u274c \u2714\ufe0f \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy TD3 \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy DrQ-v2 \u2714\ufe0f \u274c \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Distributed IMPALA \u2714\ufe0f \u2714\ufe0f \u274c \u274c \u2714\ufe0f \u274c \u274c \u274c <ul> <li><code>Dis., M.B., M.D.</code>: <code>Discrete</code>, <code>MultiBinary</code>, and <code>MultiDiscrete</code> action space;</li> <li><code>M.P.</code>: Multi processing;</li> <li>\ud83d\udc0c: Developing;</li> <li>\ud83d\udcb0: Support intrinsic reward shaping;</li> <li>\ud83d\udd2d: Support observation augmentation. </li> </ul>"},{"location":"api/#xploit-modules-that-focus-on-exploitation-in-rl","title":"Xploit: Modules that focus on exploitation in RL.","text":"<p>Policy: Policies for interaction and learning.</p> Module Type Remark OnPolicySharedActorCritic On-policy Actor-Critic networks with a shared encoder. OnPolicyDecoupledActorCritic On-policy Actor-Critic networks with two separate encoders. OffPolicyDoubleQNetwork Off-policy Double Q-network. OffPolicyDoubleActorDoubleCritic Off-policy Double deterministic actor network and double-critic network. OffPolicyDetActorDoubleCritic Off-policy Deterministic actor network and double-critic network. OffPolicyStochActorDoubleCritic Off-policy Stochastic actor network and double-critic network. DistributedActorLearner Distributed Memory-shared actor and learner networks <p>Encoder: Neural nework-based encoders for processing observations.</p> Module Input Reference Target Task EspeholtResidualEncoder Images Paper Atari or Procgen games MnihCnnEncoder Images Paper Atari games TassaCnnEncoder Images Paper DeepMind Control Suite: pixel PathakCnnEncoder Images Paper Atari or MiniGrid games IdentityEncoder States N/A DeepMind Control Suite: state VanillaMlpEncoder States N/A DeepMind Control Suite: state RaffinCombinedEncoder Dict Paper Highway <ul> <li>Naming Rule: <code>Surname of the first author</code> + <code>Backbone</code> + <code>Encoder</code></li> <li>Target Task: The testing tasks in their paper or potential tasks.</li> </ul> <p>Storage: Experience storage and sampling.</p> Module Type Remark VanillaRolloutStorage On-policy DictRolloutStorage On-policy VanillaReplayStorage Off-policy DictReplayStorage Off-policy NStepReplayStorage Off-policy PrioritizedReplayStorage Off-policy HerReplayStorage Off-policy VanillaDistributedStorage Distributed"},{"location":"api/#xplore-modules-that-focus-on-exploration-in-rl","title":"Xplore: Modules that focus on exploration in RL.","text":"<p>Augmentation: PyTorch.nn-like modules for observation augmentation.</p> Module Input Reference GaussianNoise States Paper RandomAmplitudeScaling States Paper GrayScale Images Paper RandomColorJitter Images Paper RandomConvolution Images Paper RandomCrop Images Paper RandomCutout Images Paper RandomCutoutColor Images Paper RandomFlip Images Paper RandomRotate Images Paper RandomShift Images Paper RandomTranslate Images Paper <p>Distribution: Distributions for sampling actions.</p> Module Type Reference NormalNoise Noise Paper OrnsteinUhlenbeckNoise Noise Paper TruncatedNormalNoise Noise Paper Bernoulli Distribution Paper Categorical Distribution Paper MultiCategorical Distribution Paper DiagonalGaussian Distribution Paper SquashedNormal Distribution Paper <ul> <li>In RLLTE, the action noise is implemented via a <code>Distribution</code> manner to realize unification.</li> </ul> <p>Reward: Intrinsic reward modules for enhancing exploration.</p> Type Modules Count-based PseudoCounts, RND Curiosity-driven ICM, GIRM, RIDE Memory-based NGU Information theory-based RE3, RISE, REVD <p>See Tutorials: Use Intrinsic Reward and Observation Augmentation for usage examples.</p>"},{"location":"api/#env-packaged-environments-eg-atari-games-for-fast-invocation","title":"Env: Packaged environments (e.g., Atari games) for fast invocation.","text":"Function Name Remark Reference make_atari_env Atari Games Discrete control Paper make_bullet_env PyBullet Robotics Environments Continuous control Paper make_dmc_env DeepMind Control Suite Continuous control Paper make_minigrid_env MiniGrid Games Discrete control Paper make_procgen_env Procgen Games Discrete control Paper make_robosuite_env Robosuite Robotics Environments Continuous control Paper"},{"location":"api/#copilot-large-language-model-empowered-copilot","title":"Copilot: Large language model-empowered copilot.","text":"<p>See Copilot.</p>"},{"location":"api/#hub-fast-training-apis-and-reusable-benchmarks","title":"Hub: Fast training APIs and reusable benchmarks.","text":"<p>See Benchmarks.</p>"},{"location":"api/#evaluation-reasonable-and-reliable-metrics-for-algorithm-evaluation","title":"Evaluation: Reasonable and reliable metrics for algorithm evaluation.","text":"<p>See Tutorials: Model Evaluation.</p>"},{"location":"api/#pre-training-methods-of-pre-training-in-rl","title":"Pre-training: Methods of pre-training in RL.","text":"<p>See Tutorials: Pre-training.</p>"},{"location":"api/#deployment-convenient-apis-for-model-deployment","title":"Deployment: Convenient APIs for model deployment.","text":"<p>See Tutorials: Model Deployment.</p>"},{"location":"api_old/","title":"Api old","text":""},{"location":"api_old/#common-auxiliary-modules-like-trainer-and-logger","title":"Common: Auxiliary modules like trainer and logger.","text":"<ul> <li>Engine: Engine for building Hsuanwu application.</li> <li>Logger: Logger for managing output information.</li> </ul>"},{"location":"api_old/#xploit-modules-that-focus-on-exploitation-in-rl","title":"Xploit: Modules that focus on exploitation in RL.","text":"<ul> <li>Agent: Agent for interacting and learning.</li> </ul> Type Algorithm On-Policy A2C<sup>\ud83d\udda5\ufe0f\u26d3\ufe0f\ud83d\udcb0</sup>,PPO<sup>\ud83d\udda5\ufe0f\u26d3\ufe0f\ud83d\udcb0</sup> DAAC<sup>\ud83d\udda5\ufe0f\u26d3\ufe0f\ud83d\udcb0</sup>,DrAC<sup>\ud83d\udda5\ufe0f\u26d3\ufe0f\ud83d\udcb0\ud83d\udd2d</sup>,DrDAAC<sup>\ud83d\udda5\ufe0f\u26d3\ufe0f\ud83d\udcb0\ud83d\udd2d</sup> Off-Policy DQN<sup>\ud83d\udda5\ufe0f\u26d3\ufe0f\ud83d\udcb0</sup>,DDPG<sup>\ud83d\udda5\ufe0f\u26d3\ufe0f\ud83d\udcb0</sup>,SAC<sup>\ud83d\udda5\ufe0f\u26d3\ufe0f\ud83d\udcb0</sup> DrQ-v2<sup>\ud83d\udda5\ufe0f\u26d3\ufe0f\ud83d\udcb0\ud83d\udd2d</sup> Distributed IMPALA<sup>\u26d3\ufe0f</sup> <ul> <li>\ud83d\udda5\ufe0f: Support Neural-network processing unit.</li> <li>\u26d3\ufe0f: Multi Processing.</li> <li>\ud83d\udcb0: Support intrinsic reward shaping.</li> <li>\ud83d\udd2d: Support observation augmentation.</li> </ul> Module Recurrent Box Discrete MultiBinary Multi Processing NPU Paper Citations SAC \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f Link 5077\u2b50 DrQ \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f Link 433\u2b50 DDPG \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f Link 11819\u2b50 DrQ-v2 \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f Link 100\u2b50 DAAC \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Link 56\u2b50 PPO \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Link 11155\u2b50 DrAC \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Link 29\u2b50 IMPALA \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f Link 1219\u2b50 <p>Tips of Agent</p> <ul> <li>\ud83d\udc0c: Developing.</li> <li>NPU: Support Neural-network processing unit.</li> <li>Recurrent: Support recurrent neural network.</li> <li>Box: A N-dimensional box that containes every point in the action space.</li> <li>Discrete: A list of possible actions, where each timestep only one of the actions can be used.</li> <li>MultiBinary: A list of possible actions, where each timestep any of the actions can be used in any combination.</li> </ul> <ul> <li>Encoder: Neural nework-based encoder for processing observations.</li> </ul> Module Input Reference Target Task EspeholtResidualEncoder Images IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures Atari or Procgen games. IdentityEncoder States N/A DeepMind Control Suite: state MnihCnnEncoder Images Playing Atari with Deep Reinforcement Learning Atari games. TassaCnnEncoder Images DeepMind Control Suite DeepMind Control Suite: pixel PathakCnnEncoder Images Curiosity-Driven Exploration by Self-Supervised Prediction Atari or MiniGrid games VanillaMlpEncoder States N/A DeepMind Control Suite: state <p>Tips of Encoder</p> <ul> <li>Naming Rule: 'Surname of the first author' + 'Backbone' + 'Encoder'</li> <li>Input: Input type.</li> <li>Target Task: The testing tasks in their paper or potential tasks.</li> </ul> <ul> <li>Storage: Storge for storing collected experiences.</li> </ul> Module Remark VanillaRolloutStorage On-Policy RL VanillaReplayStorage Off-Policy RL NStepReplayStorage Off-Policy RL PrioritizedReplayStorage Off-Policy RL DistributedStorage Distributed RL"},{"location":"api_old/#xplore-modules-that-focus-on-exploration-in-rl","title":"Xplore: Modules that focus on exploration in RL.","text":"<ul> <li>Augmentation: PyTorch.nn-like modules for observation augmentation.</li> </ul> Module Input Reference GaussianNoise States Reinforcement Learning with Augmented Data RandomAmplitudeScaling States Reinforcement Learning with Augmented Data GrayScale Images Reinforcement Learning with Augmented Data RandomColorJitter Images Reinforcement Learning with Augmented Data RandomConvolution Images Reinforcement Learning with Augmented Data RandomCrop Images Reinforcement Learning with Augmented Data RandomCutout Images Reinforcement Learning with Augmented Data RandomCutoutColor Images Reinforcement Learning with Augmented Data RandomFlip Images Reinforcement Learning with Augmented Data RandomRotate Images Reinforcement Learning with Augmented Data RandomShift Images Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning RandomTranslate Images Reinforcement Learning with Augmented Data <ul> <li>Distribution: Distributions for sampling actions.</li> </ul> Module Type Reference NormalNoise Noise torch.distributions OrnsteinUhlenbeckNoise Noise Continuous Control with Deep Reinforcement Learning TruncatedNormalNoise Noise Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning Bernoulli Distribution torch.distributions Categorical Distribution torch.distributions DiagonalGaussian Distribution torch.distributions SquashedNormal Distribution torch.distributions <p>Tips of Distribution</p> <ul> <li>In Hsuanwu, the action noise is implemented via a <code>Distribution</code> manner to realize unification.</li> </ul> <ul> <li>Reward: Intrinsic reward modules for enhancing exploration.</li> </ul> Module Remark Repr. Visual Reference PseudoCounts Count-Based exploration \u2714\ufe0f \u2714\ufe0f Never Give Up: Learning Directed Exploration Strategies ICM Curiosity-driven exploration \u2714\ufe0f \u2714\ufe0f Curiosity-Driven Exploration by Self-Supervised Prediction RND Count-based exploration \u274c \u2714\ufe0f Exploration by Random Network Distillation GIRM Curiosity-driven exploration \u2714\ufe0f \u2714\ufe0f Intrinsic Reward Driven Imitation Learning via Generative Model NGU Memory-based exploration \u2714\ufe0f \u2714\ufe0f Never Give Up: Learning Directed Exploration Strategies RIDE Procedurally-generated environment \u2714\ufe0f \u2714\ufe0f RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments RE3 Entropy Maximization \u274c \u2714\ufe0f State Entropy Maximization with Random Encoders for Efficient Exploration RISE Entropy Maximization \u274c \u2714\ufe0f R\u00e9nyi State Entropy Maximization for Exploration Acceleration in Reinforcement Learning REVD Divergence Maximization \u274c \u2714\ufe0f Rewarding Episodic Visitation Discrepancy for Exploration in Reinforcement Learning <p>Tips of Reward</p> <ul> <li>\ud83d\udc0c: Developing.</li> <li>Repr.: The method involves representation learning.</li> <li>Visual: The method works well in visual RL.</li> </ul> <p>See Tutorials: Use intrinsic reward and observation augmentation for usage examples.</p>"},{"location":"api_old/#evaluation-reasonable-and-reliable-metrics-for-algorithm-evaluation","title":"Evaluation: Reasonable and reliable metrics for algorithm evaluation.","text":"<p>See Tutorials: Evaluate your model.</p>"},{"location":"api_old/#env-packaged-environments-eg-atari-games-for-fast-invocation","title":"Env: Packaged environments (e.g., Atari games) for fast invocation.","text":"Module Name Remark Reference make_atari_env Atari Games Discrete control The Arcade Learning Environment: An Evaluation Platform for General Agents make_bullet_env PyBullet Robotics Environments Continuous control Pybullet: A Python Module for Physics Simulation for Games, Robotics and Machine Learning make_dmc_env DeepMind Control Suite Continuous control DeepMind Control Suite make_minigrid_env MiniGrid Games Discrete control Minimalistic Gridworld Environment for Gymnasium make_procgen_env Procgen Games Discrete control Leveraging Procedural Generation to Benchmark Reinforcement Learning make_robosuite_env Robosuite Robotics Environments Continuous control Robosuite: A Modular Simulation Framework and Benchmark for Robot Learning"},{"location":"api_old/#pre-training-methods-of-pre-training-in-rl","title":"Pre-training: Methods of pre-training in RL.","text":"<p>See Tutorials: Pre-training in Hsuanwu.</p>"},{"location":"api_old/#deployment-methods-of-model-deployment-in-rl","title":"Deployment: Methods of model deployment in RL.","text":"<p>See Tutorials: Deploy your model in inference devices.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>rllte-hub provides a large number of reusable datasets and models of representative RL benchmarks. All the files  are deposited on the Hugging Face platform, view them by </p> <ul> <li>https://hub.rllte.dev/ or</li> <li>https://huggingface.co/RLE-Foundation.</li> </ul> Module Remark <code>rllte.hub.datasets</code> Provide test scores and learning cures of various RL algorithms on different benchmarks. <code>rllte.hub.models</code> Provide trained models of various RL algorithms on different benchmarks. <code>rllte.hub.applications</code> Provide fast-APIs for training RL agents on recognized benchmarks."},{"location":"benchmarks/#support-list","title":"Support list","text":"Benchmark Algorithm Remark Reference Atari Games PPO 50M, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 Paper SAC 1M, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 Paper DeepMind Control (Pixel) DrQ-v2 1M, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 Paper DeepMind Control (State) SAC 10M, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 DDPG 10M, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 Procgen Games PPO 25M, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 Paper DAAC 25M, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 Paper MiniGrid Games <p>Tip</p> <ul> <li>\ud83d\udc0c: Incoming.</li> <li>(25M): 25 million training steps.</li> <li>\ud83d\udcafScores: Available final scores.</li> <li>\ud83d\udccaCurves: Available training curves.</li> <li>\ud83e\udd16Models: Available trained models.</li> </ul>"},{"location":"benchmarks/#datasets","title":"Datasets","text":""},{"location":"benchmarks/#load_scores","title":"<code>.load_scores</code>","text":"<p>Suppose we want to evaluate algorithm performance on the Procgen benchmark. Here is an example:</p> <p>example.py<pre><code>from rllte.hub.datasets import Procgen\n\nprocgen = Procgen()\nprocgen_scores = procgen.load_scores()\nprint(procgen_scores['ppo'].shape)\n\n# Output:\n# (10, 16)\n</code></pre> For each algorithm, this will return a <code>NdArray</code> of size (<code>10</code> x <code>16</code>) where <code>scores[n][m]</code> represent the score on run <code>n</code> of task <code>m</code>.</p>"},{"location":"benchmarks/#load_curves","title":"<code>.load_curves</code>","text":"<p>Meanwhile, <code>.load_curves</code> will return the learning curves by a Python <code>Dict</code> like:</p> <p><pre><code>curves = {\n    \"ppo\": {\n        \"train\": {\"bigfish\": np.ndarray(shape=(Number of seeds, Number of points)), ...}, \n        \"eval\": {\"bigfish\": np.ndarray(shape=(Number of seeds, Number of points)), ...}, \n    },\n    \"daac\": {\n        \"train\": {\"bigfish\": np.ndarray(shape=(Number of seeds, Number of points)), ...}, \n        \"eval\": {\"bigfish\": np.ndarray(shape=(Number of seeds, Number of points)), ...}, \n    },\n    ...\n}\n</code></pre> A code example for loading curves of the Procgen benchmark: example.py<pre><code>from rllte.hub.datasets import Procgen\n\nif __name__ == \"__main__\":\n    # load data\n    procgen = Procgen()\n    curves = procgen.load_curves()\n\n    print(curves['ppo']['train']['bigfish'].shape)\n    print(curves['ppo']['eval']['bigfish'].shape)\n\n# Output:\n# (10, 1525)\n# (10, 153)\n</code></pre></p>"},{"location":"benchmarks/#models","title":"Models","text":"<p>Suppose we want to load an <code>PPO</code> agent trained on Procgen benchmark, here is an example:</p> example.py<pre><code>from rllte.hub.models import Procgen\nfrom rllte.env import make_procgen_env\nimport torch as th\nimport numpy as np\n\nif __name__ == \"__main__\":\n    # env setup\n    device = \"cuda:0\"\n    env_id = \"starpilot\"\n    seed = 1\n    # download the model\n    procgen = Procgen()\n    agent = procgen.load_models(agent=\"ppo\",\n                                env_id=env_id,\n                                seed=seed,\n                                device=device)\n    # create env\n    env = make_procgen_env(env_id=env_id, device=device, num_envs=1, seed=seed)\n    # evaluate the model\n    obs, infos = env.reset(seed=seed)\n    # run the model\n    episode_rewards, episode_steps = list(), list()\n    while len(episode_rewards) &lt; 10:\n        # the exported model outputs logits of the action distribution\n        action = th.softmax(agent(obs), dim=1).argmax(dim=1)\n        obs, rewards, terminateds, truncateds, infos = env.step(action)\n\n        if \"episode\" in infos:\n            indices = np.nonzero(infos[\"episode\"][\"l\"])\n            episode_rewards.extend(infos[\"episode\"][\"r\"][indices].tolist())\n            episode_steps.extend(infos[\"episode\"][\"l\"][indices].tolist())\n\n    print(f\"mean episode reward: {np.mean(episode_rewards)}\")\n    print(f\"mean episode length: {np.mean(episode_steps)}\")\n\n# output:\nmean episode reward: 30.0\nmean episode length: 296.1\n</code></pre>"},{"location":"benchmarks/#applications","title":"Applications","text":"<p>Suppose we want to train an <code>PPO</code> agent on Procgen benchmark, it suffices to write a <code>train.py</code> like: <pre><code>from rllte.hub.applications import Procgen\n\napp = Procgen(agent=\"PPO\", env_id=\"coinrun\", seed=1, device=\"cuda\")\napp.train(num_train_steps=2.5e+7)\n</code></pre> All the results of <code>rllte.hub.datasets</code> and <code>rllte.hub.models</code> were trained via <code>rllte.hub.applications</code>, and all the hyper-parameters can be found in the reference.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001","title":"v0.0.1","text":"<p>05/05/2023 (Version 0.0.1)</p> <p>Version 0.0.1 published.</p> <ul> <li>New features:<ul> <li>Agent: SAC, DrQ, DDPG, DrQ-v2, PPO, DAAC, DrAC, PPG, IMPALA</li> <li>Encoder: EspeholtResidualEncoder, IdentityEncoder, MnihCnnEncoder, TassaCnnEncoder, VanillaMlpEncoder</li> <li>Storage: DecoupledRolloutStorage, VanillaRolloutStorage, VanillaReplayStorage, NStepReplayStorage, PrioritizedReplayStorage, DistributedStorage</li> <li>Augmentation: GaussianNoise, RandomAmplitudeScaling, RandomShift, ...</li> <li>Distribution: TruncatedNormalNoise, Bernoulli, Categorical, DiagonalGaussian, ...</li> <li>Reward: PseudoCounts, ICM, RND, RE3, ...</li> </ul> </li> </ul>"},{"location":"changelog/#initialization","title":"Initialization","text":"<p>19/01/2023</p> <ul> <li>Repository initialization and first commit.</li> </ul>"},{"location":"contributing/","title":"Contributing to rllte","text":"<p>Thank you for using and contributing to rllte project!!!\ud83d\udc4b\ud83d\udc4b\ud83d\udc4b Before you begin writing code, it is important that you share your intention to contribute with the team, based on the type of contribution:</p> <ol> <li> <p>You want to propose a new feature and implement it:</p> <ul> <li>Post about your intended feature in an issue, and we shall discuss the design and implementation. Once we agree that the plan looks good, go ahead and implement it.</li> </ul> </li> <li> <p>You want to implement a feature or bug-fix for an outstanding issue:</p> <ul> <li>Search for your issue in the rllte issue list.</li> <li>Pick an issue and comment that you'd like to work on the feature or bug-fix.</li> <li>If you need more context on a particular issue, please ask and we shall provide.</li> </ul> </li> </ol> <p>Once you implement and test your feature or bug-fix, please submit a Pull Request to https://github.com/RLE-Foundation/rllte.</p>"},{"location":"contributing/#get-rllte","title":"Get rllte","text":"<p>Open up a terminal and clone the repository from GitHub with <code>git</code>: <pre><code>git clone https://github.com/RLE-Foundation/rllte.git\ncd rllte/\n</code></pre> After that, run the following command to install package and dependencies: <pre><code>pip install -e .[all]\n</code></pre></p>"},{"location":"contributing/#codestyle","title":"Codestyle","text":"<p>We use black codestyle (max line length of 127 characters) together with isort to sort the imports. For the documentation, we use the default line length of 88 characters per line.</p> <p>Please run <code>make format</code> to reformat your code. You can check the codestyle using make <code>check-codestyle</code> and <code>make lint</code>.</p> <p>Please document each function/method and type them using the following Google style docstring template: <pre><code>def function_with_types_in_docstring(param1: type1, param2: type2):\n    \"\"\"Example function with types documented in the docstring.\n\n    `PEP 484`_ type annotations are supported. If attribute, parameter, and\n    return types are annotated according to `PEP 484`_, they do not need to be\n    included in the docstring:\n\n    Args:\n        param1 (type1): The first parameter.\n        param2 (type2): The second parameter.\n\n    Returns:\n        bool: The return value. True for success, False otherwise.\n\n    .. _PEP 484:\n        https://www.python.org/dev/peps/pep-0484/\n\n    \"\"\"\n</code></pre></p>"},{"location":"contributing/#pull-request-pr","title":"Pull Request (PR)","text":"<p>Before proposing a PR, please open an issue, where the feature will be discussed. This prevent from duplicated PR to be proposed and also ease the code review process. Each PR need to be reviewed and accepted by at least one of the maintainers (@yuanmingqi, @ShihaoLuo). A PR must pass the Continuous Integration tests to be merged with the master branch.</p> <p>See the Pull Request Template.</p>"},{"location":"contributing/#tests","title":"Tests","text":"<p>All new features must add tests in the <code>tests/</code> folder ensuring that everything works fine. We use pytest. Also, when a bug fix is proposed, tests should be added to avoid regression.</p> <p>To run tests with <code>pytest</code>:</p> <pre><code>make pytest\n</code></pre> <p>Type checking with <code>pytype</code> and <code>mypy</code>:</p> <pre><code>make type\n</code></pre> <p>Codestyle check with <code>black</code>, <code>isort</code> and <code>ruff</code>:</p> <pre><code>make check-codestyle\nmake lint\n</code></pre> <p>To run <code>type</code>, <code>format</code> and <code>lint</code> in one command: <pre><code>make commit-checks\n</code></pre></p>"},{"location":"contributing/#acknowledgement","title":"Acknowledgement","text":"<p>This contributing guide is based on the stable-Baselines3 one.</p>"},{"location":"copilot/","title":"Copilot","text":"<p>Copilot is the first attempt to integrate an LLM into an RL framework, which aims to help developers reduce the learning cost and facilitate application construction. We follow the design of LocalGPT that interacts privately with documents using the power of GPT. The source documents are first ingested by an instructor embedding tool to create a local vector database. After that, a local LLM is used to understand questions and create answers based on the database. In practice, we utilize Vicuna-7B as the base model and build the database using various corpora, including API documentation, tutorials, and RL references. The powerful understanding ability of the LLM model enables the copilot to accurately answer questions about the use of the framework and any other questions of RL. Moreover, no additional training is required, and users are free to replace the base model according to their computing power. In future work, we will further enrich the corpus and add the code completion function to build a more intelligent copilot for RL.</p> <ul> <li>GitHub Repository: https://github.com/RLE-Foundation/rllte-copilot</li> <li>Hugging Face Space: Coming soon...</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>Currently, we recommend <code>Python&gt;=3.8</code>, and user can create an virtual environment by <pre><code>conda create -n rllte python=3.8\n</code></pre></p>"},{"location":"getting_started/#with-pip-recommended","title":"with pip recommended","text":"<p>RLLTE has been published as a Python package in PyPi and can be installed with <code>pip</code>, ideally by using a virtual environment. Open up a terminal and install RLLTE with:</p> <pre><code>pip install rllte-core # basic installation\npip install rllte-core[envs] # for pre-defined environments\n</code></pre>"},{"location":"getting_started/#with-git","title":"with git","text":"<p>Open up a terminal and clone the repository from GitHub witg <code>git</code>: <pre><code>git clone https://github.com/RLE-Foundation/rllte.git\n</code></pre> After that, run the following command to install package and dependencies: <pre><code>pip install -e . # basic installation\npip install -e .[envs] # for pre-defined environments\n</code></pre></p>"},{"location":"getting_started/#pytorch-installation","title":"PyTorch Installation","text":"<p>RLLTE currently supports two kinds of computing devices for acceleration, namely NVIDIA GPU and HUAWEI NPU. Thus users need to install different versions PyTorch for adapting to different devices.</p>"},{"location":"getting_started/#with-nvidia-gpu","title":"with NVIDIA GPU","text":"<p>Open up a terminal and install PyTorch with: <pre><code>pip3 install torch==2.0.0 torchvision\n</code></pre> More information can be found in Get Started.</p>"},{"location":"getting_started/#with-huawei-npu","title":"with HUAWEI NPU","text":"<p>Tip</p> <p>Ascend NPU only supports aarch64!</p> <ul> <li>Install the dependencies for PyTorch: <pre><code>pip3 install pyyaml wheel\n</code></pre></li> <li> <p>Download the <code>.whl</code> package of PyTorch from Kunpeng file sharing center and install it: <pre><code>wget https://repo.huaweicloud.com/kunpeng/archive/Ascend/PyTorch/torch-1.11.0-cp39-cp39-linux_aarch64.whl\npip3 install torch-1.11.0-cp39-cp39-linux_aarch64.whl\n</code></pre></p> </li> <li> <p>Install <code>torch_npu</code>: <pre><code>wget https://gitee.com/ascend/pytorch/releases/download/v5.0.rc1-pytorch1.11.0/torch_npu-1.11.0-cp39-cp39m-linux_aarch64.whl\npip3 install torch_npu-1.11.0-cp39-cp39m-linux_aarch64.whl\n</code></pre></p> </li> <li> <p>Install <code>apex</code> [Optional]: <pre><code>wget https://gitee.com/ascend/apex/releases/download/v5.0.rc1-pytorch1.11.0/apex-0.1_ascend-cp39-cp39m-linux_aarch64.whl\npip3 install apex-0.1_ascend-cp39-cp39m-linux_aarch64.whl\n</code></pre> Training with mixed precision can improve the model performance. You can introduce the Apex mixed precision module or use the AMP module integrated in AscendPyTorch 1.8.1 or later based on the scenario. The Apex module provides four function modes to suit different training with mixed precision scenarios. AMP is only similar to one function of the Apex module, but can be directly used without being introduced. For details about how to use the AMP and Apex modules, see \"Mixed Precision Description\" in the PyTorch Network Model Porting and Training Guide.</p> </li> </ul>"},{"location":"hub/","title":"RLLTE Hub: Large-Scale and Comprehensive Data Hub for RL","text":""},{"location":"hub/#support-list","title":"Support list","text":"Benchmark Algorithm Remark Reference Atari Games PPO 10M, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 Paper DeepMind Control (Pixel) DrQ-v2 1M, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 Paper DeepMind Control (State) SAC 10M for Humanoid, 2M else, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 DDPG \ud83d\udc0c Procgen Games PPO 25M, \ud83d\udcaf\ud83d\udcca\ud83e\udd16 Paper DAAC \ud83d\udc0c Paper MiniGrid Games \ud83d\udc0c \ud83d\udc0c \ud83d\udc0c <p>Tip</p> <ul> <li>\ud83d\udc0c: Incoming.</li> <li>(25M): 25 million training steps.</li> <li>\ud83d\udcafScores: Available final scores.</li> <li>\ud83d\udccaCurves: Available training curves.</li> <li>\ud83e\udd16Models: Available trained models.</li> </ul>"},{"location":"hub/#trained-rl-models","title":"Trained RL Models","text":"<p>The following example illustrates how to download an <code>PPO</code> agent trained the Atari benchmark:</p> <pre><code>from rllte.hub import Atari\n\nagent = Atari().load_models(agent='ppo',\n                            env_id='BeamRider-v5',\n                            seed=0,\n                            device='cuda')\nprint(agent)\n</code></pre> <p>Use the trained agent to play the game: <pre><code>from rllte.env import make_envpool_atari_env\nfrom rllte.common.utils import get_episode_statistics\nimport numpy as np\n\nenvs = make_envpool_atari_env(env_id=\"BeamRider-v5\",\n                              num_envs=1,\n                              seed=0,\n                              device=\"cuda\",\n                              asynchronous=False)\n\nobs, infos = envs.reset(seed=0)\nepisode_rewards, episode_steps = list(), list()\nwhile len(episode_rewards) &lt; 10:\n    # The agent outputs logits of the action distribution\n    actions = th.softmax(agent(obs), dim=1).argmax(dim=1)\n    obs, rewards, terminateds, truncateds, infos = envs.step(actions)\n\n    eps_r, eps_l = get_episode_statistics(infos)\n    episode_rewards.extend(eps_r)\n    episode_steps.extend(eps_l)    \n\nprint(f\"mean episode reward: {np.mean(episode_rewards)}\")\nprint(f\"mean episode length: {np.mean(episode_steps)}\")\n\n# Output:\n# mean episode reward: 3249.8\n# mean episode length: 3401.1\n</code></pre></p>"},{"location":"hub/#rl-training-logs","title":"RL Training Logs","text":"<p>Download training logs of various RL algorithms on well-recognized benchmarks for academic research. </p>"},{"location":"hub/#training-curves","title":"Training Curves","text":"<p>The following example illustrates how to download training curves of the <code>SAC</code> agent on the DeepMind Control Suite benchmark:</p> <p><pre><code>from rllte.hub import DMControl\n\ncurves = DMControl().load_curves(agent='sac', env_id=\"cheetah_run\")\n</code></pre> This will return a Python <code>Dict</code> of NumPy array like: <pre><code>curves\n\u251c\u2500\u2500 train: np.ndarray(shape=(N_SEEDS, N_POINTS))\n\u2514\u2500\u2500 eval:  np.ndarray(shape=(N_SEEDS, N_POINTS))\n</code></pre></p> <p>Visualize the training curves:</p>"},{"location":"hub/#test-scores","title":"Test Scores","text":"<p>Similarly, download the final test scores via <pre><code>scores = DMControl().load_scores(agent='sac', env_id=\"cheetah_run\")\n</code></pre> This will return a data array with shape <code>(N_SEEDS, N_POINTS)</code>.</p>"},{"location":"hub/#rl-training-applications","title":"RL Training Applications","text":"<p>Developers can also train RL agents on well-recognized benchmarks rapidly using simple interfaces. Suppose we want to train an <code>PPO</code> agent on Procgen benchmark, it suffices to write a <code>train.py</code> like: <pre><code>from rllte.hub import Procgen\n\napp = Procgen().load_apis(agent=\"PPO\", env_id=\"coinrun\", seed=1, device=\"cuda\")\napp.train(num_train_steps=2.5e+7)\n</code></pre> All the curves, scores, and models were trained via <code>.load_apis()</code>, and all the hyper-parameters can be found in the reference of the support list.</p>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2023 Reinforcement Learning Evolution Foundation</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"verification/","title":"Verification","text":""},{"location":"verification/#software","title":"Software","text":"<p>To ensure that RLLTE is installed correctly, we can verify the installation by running a single training script: <pre><code>python -m rllte.verification\n</code></pre> If successful, you will see the following output: <pre><code>[08/03/2023 07:30:21 PM] - [INFO.] - Invoking RLLTE Engine...\n[08/03/2023 07:30:21 PM] - [INFO.] - ================================================================================\n[08/03/2023 07:30:21 PM] - [INFO.] - Tag               : verification\n[08/03/2023 07:30:21 PM] - [INFO.] - Device            : CPU\n[08/03/2023 07:30:21 PM] - [DEBUG] - Agent             : PPO\n[08/03/2023 07:30:21 PM] - [DEBUG] - Encoder           : IdentityEncoder\n[08/03/2023 07:30:21 PM] - [DEBUG] - Policy            : OnPolicySharedActorCritic\n[08/03/2023 07:30:21 PM] - [DEBUG] - Storage           : VanillaRolloutStorage\n[08/03/2023 07:30:21 PM] - [DEBUG] - Distribution      : Categorical\n[08/03/2023 07:30:21 PM] - [DEBUG] - Augmentation      : False\n[08/03/2023 07:30:21 PM] - [DEBUG] - Intrinsic Reward  : False\n[08/03/2023 07:30:21 PM] - [DEBUG] - ================================================================================\n[08/03/2023 07:30:22 PM] - [TRAIN] - S: 512         | E: 4           | L: 428         | R: -427.000    | FPS: 1457.513  | T: 0:00:00    \n[08/03/2023 07:30:22 PM] - [TRAIN] - S: 640         | E: 5           | L: 428         | R: -427.000    | FPS: 1513.510  | T: 0:00:00    \n[08/03/2023 07:30:22 PM] - [TRAIN] - S: 768         | E: 6           | L: 353         | R: -352.000    | FPS: 1551.423  | T: 0:00:00    \n[08/03/2023 07:30:22 PM] - [TRAIN] - S: 896         | E: 7           | L: 353         | R: -352.000    | FPS: 1581.616  | T: 0:00:00    \n[08/03/2023 07:30:22 PM] - [INFO.] - Training Accomplished!\n[08/03/2023 07:30:22 PM] - [INFO.] - Model saved at: /export/yuanmingqi/code/rllte/logs/verification/2023-08-03-07-30-21/model\nVERIFICATION PASSED!\n</code></pre></p>"},{"location":"verification/#hardware","title":"Hardware","text":"<p>Additionally, to check if your GPU driver and CUDA is enabled and accessible by PyTorch, run the following commands to return whether or not the CUDA driver is enabled: <pre><code>import torch\ntorch.cuda.is_available()\n</code></pre></p> <p>For HUAWEI NPU:</p> <pre><code>import torch\nimport torch_npu\ntorch.npu.is_available()\n</code></pre>"},{"location":"api_docs/","title":"Index","text":"## RLLTE: Long-Term Evolution Project of Reinforcement Learning   <p>Inspired by the long-term evolution (LTE) standard project in telecommunications, aiming to provide development components for and standards for advancing RL research and applications. Beyond delivering top-notch algorithm implementations, RLLTE also serves as a toolkit for developing algorithms.</p> <p>Why RLLTE? - \ud83e\uddec Long-term evolution for providing latest algorithms and tricks; - \ud83c\udfde\ufe0f Complete ecosystem for task design, model training, evaluation, and deployment (TensorRT, CANN, ...); - \ud83e\uddf1 Module-oriented design for complete decoupling of RL algorithms; - \ud83d\ude80 Optimized workflow for full hardware acceleration; - \u2699\ufe0f Support custom environments and modules; - \ud83d\udda5\ufe0f Support multiple computing devices like GPU and NPU; - \ud83d\udcbe Large number of reusable benchmarks (RLLTE Hub); - \ud83e\udd16 Large language model-empowered copilot (RLLTE Copilot).</p> <p>\u26a0\ufe0f Since the construction of RLLTE Hub requires massive computing power, we have to upload the training datasets and model weights gradually. Progress report can be found in Issue#30.</p> <p>See the project structure below:</p> <p>For more detailed descriptions of these modules, see API Documentation.</p>"},{"location":"api_docs/#quick-start","title":"Quick Start","text":""},{"location":"api_docs/#installation","title":"Installation","text":"<ul> <li>with pip <code>recommended</code></li> </ul> <p>Open a terminal and install rllte with <code>pip</code>: <pre><code>conda create -n rllte python=3.8 # create an virtual environment\npip install rllte-core # basic installation\npip install rllte-core[envs] # for pre-defined environments\n</code></pre></p> <ul> <li>with git</li> </ul> <p>Open a terminal and clone the repository from GitHub with <code>git</code>: <pre><code>git clone https://github.com/RLE-Foundation/rllte.git\npip install -e . # basic installation\npip install -e .[envs] # for pre-defined environments\n</code></pre></p> <p>For more detailed installation instruction, see Getting Started.</p>"},{"location":"api_docs/#fast-training-with-built-in-algorithms","title":"Fast Training with Built-in Algorithms","text":"<p>RLLTE provides implementations for well-recognized RL algorithms and simple interface for building applications.</p>"},{"location":"api_docs/#on-nvidia-gpu","title":"On NVIDIA GPU","text":"<p>Suppose we want to use DrQ-v2 to solve a task of DeepMind Control Suite, and it suffices to write a <code>train.py</code> like:</p> <p><pre><code># import `env` and `agent` module\nfrom rllte.env import make_dmc_env \nfrom rllte.agent import DrQv2\n\nif __name__ == \"__main__\":\n    device = \"cuda:0\"\n    # create env, `eval_env` is optional\n    env = make_dmc_env(env_id=\"cartpole_balance\", device=device)\n    eval_env = make_dmc_env(env_id=\"cartpole_balance\", device=device)\n    # create agent\n    agent = DrQv2(env=env, eval_env=eval_env, device=device, tag=\"drqv2_dmc_pixel\")\n    # start training\n    agent.train(num_train_steps=500000, log_interval=1000)\n</code></pre> Run <code>train.py</code> and you will see the following output:</p>"},{"location":"api_docs/#on-huawei-npu","title":"On HUAWEI NPU","text":"<p>Similarly, if we want to train an agent on HUAWEI NPU, it suffices to replace <code>cuda</code> with <code>npu</code>: <pre><code>device = \"cuda:0\" -&gt; device = \"npu:0\"\n</code></pre></p>"},{"location":"api_docs/#three-steps-to-create-your-rl-agent","title":"Three Steps to Create Your RL Agent","text":"<p>Developers only need three steps to implement an RL algorithm with RLLTE. The following example illustrates how to write an Advantage Actor-Critic (A2C) agent to solve Atari games.  - Firstly, select a prototype:  Click to expand code <pre><code>from rllte.common.prototype import OnPolicyAgent\n</code></pre> </p> <ul> <li>Secondly, select necessary modules to build the agent:</li> </ul> Click to expand code <pre><code>from rllte.xploit.encoder import MnihCnnEncoder\nfrom rllte.xploit.policy import OnPolicySharedActorCritic\nfrom rllte.xploit.storage import VanillaRolloutStorage\nfrom rllte.xplore.distribution import Categorical\n</code></pre>   - Run the `.describe` function of the selected policy and you will see the following output:   <pre><code>OnPolicySharedActorCritic.describe()\n# Output:\n# ================================================================================\n# Name       : OnPolicySharedActorCritic\n# Structure  : self.encoder (shared by actor and critic), self.actor, self.critic\n# Forward    : obs -&gt; self.encoder -&gt; self.actor -&gt; actions\n#            : obs -&gt; self.encoder -&gt; self.critic -&gt; values\n#            : actions -&gt; log_probs\n# Optimizers : self.optimizers['opt'] -&gt; (self.encoder, self.actor, self.critic)\n# ================================================================================\n</code></pre>   This illustrates the structure of the policy and indicate the optimizable parts.     <ul> <li>Thirdly, merge these modules and write an <code>.update</code> function:</li> </ul> Click to expand code <pre><code>from torch import nn\nimport torch as th\n\nclass A2C(OnPolicyAgent):\n    def __init__(self, env, tag, seed, device, num_steps) -&gt; None:\n        super().__init__(env=env, tag=tag, seed=seed, device=device, num_steps=num_steps)\n        # create modules\n        encoder = MnihCnnEncoder(observation_space=env.observation_space, feature_dim=512)\n        policy = OnPolicySharedActorCritic(observation_space=env.observation_space,\n                                          action_space=env.action_space,\n                                          feature_dim=512,\n                                          opt_class=th.optim.Adam,\n                                          opt_kwargs=dict(lr=2.5e-4, eps=1e-5),\n                                          init_fn=\"xavier_uniform\"\n                                          )\n        storage = VanillaRolloutStorage(observation_space=env.observation_space,\n                                        action_space=env.action_space,\n                                        device=device,\n                                        storage_size=self.num_steps,\n                                        num_envs=self.num_envs,\n                                        batch_size=256\n                                        )\n        dist = Categorical()\n        # set all the modules\n        self.set(encoder=encoder, policy=policy, storage=storage, distribution=dist)\n\n    def update(self):\n        for _ in range(4):\n            for batch in self.storage.sample():\n                # evaluate the sampled actions\n                new_values, new_log_probs, entropy = self.policy.evaluate_actions(obs=batch.observations, actions=batch.actions)\n                # policy loss part\n                policy_loss = - (batch.adv_targ * new_log_probs).mean()\n                # value loss part\n                value_loss = 0.5 * (new_values.flatten() - batch.returns).pow(2).mean()\n                # update\n                self.policy.optimizers['opt'].zero_grad(set_to_none=True)\n                (value_loss * 0.5 + policy_loss - entropy * 0.01).backward()\n                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n                self.policy.optimizers['opt'].step()\n</code></pre> <ul> <li>Finally, train the agent by  Click to expand code <pre><code>from rllte.env import make_atari_env\nif __name__ == \"__main__\":\n    device = \"cuda\"\n    env = make_atari_env(\"PongNoFrameskip-v4\", num_envs=8, seed=0, device=device)\n    agent = A2C(env=env, tag=\"a2c_atari\", seed=0, device=device, num_steps=128)\n    agent.train(num_train_steps=10000000)\n</code></pre> </li> </ul> <p>As shown in this example, only a few dozen lines of code are needed to create RL agents with RLLTE. </p>"},{"location":"api_docs/#algorithm-decoupling-and-module-replacement","title":"Algorithm Decoupling and Module Replacement","text":"<p>RLLTE allows developers to replace settled modules of implemented algorithms to make performance comparison and algorithm improvement, and both  built-in and custom modules are supported. Suppose we want to compare the effect of different encoders, it suffices to invoke the <code>.set</code> function: <pre><code>from rllte.xploit.encoder import EspeholtResidualEncoder\nencoder = EspeholtResidualEncoder(...)\nagent.set(encoder=encoder)\n</code></pre> RLLTE is an extremely open framework that allows developers to try anything. For more detailed tutorials, see Tutorials.</p>"},{"location":"api_docs/#function-list-part","title":"Function List (Part)","text":""},{"location":"api_docs/#rl-agents","title":"RL Agents","text":"Type Algo. Box Dis. M.B. M.D. M.P. NPU \ud83d\udcb0 \ud83d\udd2d On-Policy A2C \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c On-Policy PPO \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c On-Policy DrAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f On-Policy DAAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c On-Policy DrDAAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f On-Policy PPG \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy DQN \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy DDPG \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy SAC \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy SAC-Discrete \u274c \u2714\ufe0f \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy TD3 \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c Off-Policy DrQ-v2 \u2714\ufe0f \u274c \u274c \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f Distributed IMPALA \u2714\ufe0f \u2714\ufe0f \u274c \u274c \u2714\ufe0f \u274c \u274c \u274c <ul> <li><code>Dis., M.B., M.D.</code>: <code>Discrete</code>, <code>MultiBinary</code>, and <code>MultiDiscrete</code> action space;</li> <li><code>M.P.</code>: Multi processing;</li> <li>\ud83d\udc0c: Developing;</li> <li>\ud83d\udcb0: Support intrinsic reward shaping;</li> <li>\ud83d\udd2d: Support observation augmentation. </li> </ul>"},{"location":"api_docs/#intrinsic-reward-modules","title":"Intrinsic Reward Modules","text":"Type Modules Count-based PseudoCounts, RND, E3B Curiosity-driven ICM, GIRM, RIDE, Disagreement Memory-based NGU Information theory-based RE3, RISE, REVD <p>See Tutorials: Use Intrinsic Reward and Observation Augmentation for usage examples.</p>"},{"location":"api_docs/#rllte-ecosystem","title":"RLLTE Ecosystem","text":"<p>Explore the ecosystem of RLLTE to facilitate your project:</p> <ul> <li>Hub: Fast training APIs and reusable benchmarks.</li> <li>Evaluation: Reasonable and reliable metrics for algorithm evaluation.</li> <li>Env: Packaged environments for fast invocation.</li> <li>Deployment: Convenient APIs for model deployment.</li> <li>Pre-training: Methods of pre-training in RL.</li> <li>Copilot: Large language model-empowered copilot.</li> </ul>"},{"location":"api_docs/#how-to-contribute","title":"How To Contribute","text":"<p>Welcome to contribute to this project! Before you begin writing code, please read CONTRIBUTING.md for guide first.</p>"},{"location":"api_docs/#cite-the-project","title":"Cite the Project","text":"<p>To cite this project in publications: <pre><code>@article{yuan2023rllte,\n  title={RLLTE: Long-Term Evolution Project of Reinforcement Learning}, \n  author={Mingqi Yuan and Zequn Zhang and Yang Xu and Shihao Luo and Bo Li and Xin Jin and Wenjun Zeng},\n  year={2023},\n  journal={arXiv preprint arXiv:2309.16382}\n}\n</code></pre></p>"},{"location":"api_docs/#acknowledgment","title":"Acknowledgment","text":"<p>This project is supported by The Hong Kong Polytechnic University, Eastern Institute for Advanced Study, and FLW-Foundation. EIAS HPC provides a GPU computing platform, and HUAWEI Ascend Community provides an NPU computing platform for our testing. Some code of this project is borrowed or inspired by several excellent projects, and we highly appreciate them. See ACKNOWLEDGMENT.md.</p>"},{"location":"api_docs/contributing/","title":"Contributing to rllte","text":"<p>Thank you for using and contributing to rllte project!!!\ud83d\udc4b\ud83d\udc4b\ud83d\udc4b Before you begin writing code, it is important that you share your intention to contribute with the team, based on the type of contribution:</p> <ol> <li> <p>You want to propose a new feature and implement it:</p> <ul> <li>Post about your intended feature in an issue, and we shall discuss the design and implementation. Once we agree that the plan looks good, go ahead and implement it.</li> </ul> </li> <li> <p>You want to implement a feature or bug-fix for an outstanding issue:</p> <ul> <li>Search for your issue in the rllte issue list.</li> <li>Pick an issue and comment that you'd like to work on the feature or bug-fix.</li> <li>If you need more context on a particular issue, please ask and we shall provide.</li> </ul> </li> </ol> <p>Once you implement and test your feature or bug-fix, please submit a Pull Request to https://github.com/RLE-Foundation/rllte.</p>"},{"location":"api_docs/contributing/#get-rllte","title":"Get rllte","text":"<p>Open up a terminal and clone the repository from GitHub with <code>git</code>: <pre><code>git clone https://github.com/RLE-Foundation/rllte.git\ncd rllte/\n</code></pre> After that, run the following command to install package and dependencies: <pre><code>pip install -e .[all]\n</code></pre></p>"},{"location":"api_docs/contributing/#codestyle","title":"Codestyle","text":"<p>We use black codestyle (max line length of 127 characters) together with isort to sort the imports. For the documentation, we use the default line length of 88 characters per line.</p> <p>Please run <code>make format</code> to reformat your code. You can check the codestyle using make <code>check-codestyle</code> and <code>make lint</code>.</p> <p>Please document each function/method and type them using the following Google style docstring template: <pre><code>def function_with_types_in_docstring(param1: type1, param2: type2):\n    \"\"\"Example function with types documented in the docstring.\n\n    `PEP 484`_ type annotations are supported. If attribute, parameter, and\n    return types are annotated according to `PEP 484`_, they do not need to be\n    included in the docstring:\n\n    Args:\n        param1 (type1): The first parameter.\n        param2 (type2): The second parameter.\n\n    Returns:\n        bool: The return value. True for success, False otherwise.\n\n    .. _PEP 484:\n        https://www.python.org/dev/peps/pep-0484/\n\n    \"\"\"\n</code></pre></p>"},{"location":"api_docs/contributing/#pull-request-pr","title":"Pull Request (PR)","text":"<p>Before proposing a PR, please open an issue, where the feature will be discussed. This prevent from duplicated PR to be proposed and also ease the code review process. Each PR need to be reviewed and accepted by at least one of the maintainers (@yuanmingqi, @roger-creus). A PR must pass the Continuous Integration tests to be merged with the master branch.</p> <p>See the Pull Request Template.</p>"},{"location":"api_docs/contributing/#tests","title":"Tests","text":"<p>All new features must add tests in the <code>tests/</code> folder ensuring that everything works fine. We use pytest. Also, when a bug fix is proposed, tests should be added to avoid regression.</p> <p>To run tests with <code>pytest</code>:</p> <pre><code>make pytest\n</code></pre> <p>Type checking with <code>pytype</code> and <code>mypy</code>:</p> <pre><code>make type\n</code></pre> <p>Codestyle check with <code>black</code>, <code>isort</code> and <code>ruff</code>:</p> <pre><code>make check-codestyle\nmake lint\n</code></pre> <p>To run <code>type</code>, <code>format</code> and <code>lint</code> in one command: <pre><code>make commit-checks\n</code></pre></p>"},{"location":"api_docs/contributing/#acknowledgement","title":"Acknowledgement","text":"<p>This contributing guide is based on the stable-Baselines3 one.</p>"},{"location":"api_docs/agent/daac/","title":"DAAC","text":""},{"location":"api_docs/agent/daac/#daac","title":"DAAC","text":"<p>source <pre><code>DAAC(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_steps: int = 128,\n   feature_dim: int = 512, batch_size: int = 256, lr: float = 0.00025, eps: float = 1e-05,\n   hidden_dim: int = 256, clip_range: float = 0.2, clip_range_vf: float = 0.2,\n   policy_epochs: int = 1, value_freq: int = 1, value_epochs: int = 9, vf_coef: float = 0.5,\n   ent_coef: float = 0.01, adv_coef: float = 0.25, max_grad_norm: float = 0.5,\n   discount: float = 0.999, init_fn: str = 'xavier_uniform'\n)\n</code></pre></p> <p>Decoupled Advantage Actor-Critic (DAAC) agent. Based on: https://github.com/rraileanu/idaac</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_steps (int) : The sample length of per rollout.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>clip_range (float) : Clipping parameter.</li> <li>clip_range_vf (float) : Clipping parameter for the value function.</li> <li>policy_epochs (int) : Times of updating the policy network.</li> <li>value_freq (int) : Update frequency of the value network.</li> <li>value_epochs (int) : Times of updating the value network.</li> <li>vf_coef (float) : Weighting coefficient of value loss.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>adv_ceof (float) : Weighting coefficient of advantage loss.</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> <li>discount (float) : Discount factor.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>DAAC agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/daac/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update function that returns training metrics such as policy loss, value loss, etc..</p>"},{"location":"api_docs/agent/drac/","title":"DrAC","text":""},{"location":"api_docs/agent/drac/#drac","title":"DrAC","text":"<p>source <pre><code>DrAC(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_steps: int = 128,\n   feature_dim: int = 512, batch_size: int = 256, lr: float = 0.00025, eps: float = 1e-05,\n   hidden_dim: int = 512, clip_range: float = 0.1, clip_range_vf: float = 0.1,\n   n_epochs: int = 4, vf_coef: float = 0.5, ent_coef: float = 0.01, aug_coef: float = 0.1,\n   max_grad_norm: float = 0.5, discount: float = 0.999, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Data Regularized Actor-Critic (DrAC) agent. Based on: https://github.com/rraileanu/auto-drac</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_steps (int) : The sample length of per rollout.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>clip_range (float) : Clipping parameter.</li> <li>clip_range_vf (float) : Clipping parameter for the value function.</li> <li>n_epochs (int) : Times of updating the policy.</li> <li>vf_coef (float) : Weighting coefficient of value loss.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>aug_coef (float) : Weighting coefficient of augmentation loss.</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> <li>discount (float) : Discount factor.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>DrAC agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/drac/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update function that returns training metrics such as policy loss, value loss, etc..</p>"},{"location":"api_docs/agent/drdaac/","title":"DrDAAC","text":""},{"location":"api_docs/agent/drdaac/#drdaac","title":"DrDAAC","text":"<p>source <pre><code>DrDAAC(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_steps: int = 128,\n   feature_dim: int = 512, batch_size: int = 256, lr: float = 0.00025, eps: float = 1e-05,\n   hidden_dim: int = 256, clip_range: float = 0.2, clip_range_vf: float = 0.2,\n   policy_epochs: int = 1, value_freq: int = 1, value_epochs: int = 9, vf_coef: float = 0.5,\n   ent_coef: float = 0.01, aug_coef: float = 0.1, adv_coef: float = 0.25,\n   max_grad_norm: float = 0.5, discount: float = 0.999, init_fn: str = 'xavier_uniform'\n)\n</code></pre></p> <p>Data-Regularized extension of Decoupled Advantage Actor-Critic (DAAC) agent. Based on: https://github.com/rraileanu/idaac</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_steps (int) : The sample length of per rollout.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>clip_range (float) : Clipping parameter.</li> <li>clip_range_vf (float) : Clipping parameter for the value function.</li> <li>policy_epochs (int) : Times of updating the policy network.</li> <li>value_freq (int) : Update frequency of the value network.</li> <li>value_epochs (int) : Times of updating the value network.</li> <li>vf_coef (float) : Weighting coefficient of value loss.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>aug_coef (float) : Weighting coefficient of augmentation loss.</li> <li>adv_ceof (float) : Weighting coefficient of advantage loss.</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> <li>discount (float) : Discount factor.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>DAAC agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/drdaac/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update function that returns training metrics such as policy loss, value loss, etc..</p>"},{"location":"api_docs/agent/drqv2/","title":"DrQv2","text":""},{"location":"api_docs/agent/drqv2/#drqv2","title":"DrQv2","text":"<p>source <pre><code>DrQv2(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_init_steps: int = 2000,\n   storage_size: int = 1000000, feature_dim: int = 50, batch_size: int = 256,\n   lr: float = 0.0001, eps: float = 1e-08, hidden_dim: int = 1024,\n   critic_target_tau: float = 0.01, update_every_steps: int = 2,\n   stddev_clip: float = 0.3, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Data Regularized Q-v2 (DrQv2) agent. Based on: https://github.com/facebookresearch/drqv2</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_init_steps (int) : Number of initial exploration steps.</li> <li>storage_size (int) : The capacity of the storage.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>critic_target_tau  : The critic Q-function soft-update rate.</li> <li>update_every_steps (int) : The agent update frequency.</li> <li>stddev_clip (float) : The exploration std clip range.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>DrQv2 agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/drqv2/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update the agent and return training metrics such as actor loss, critic_loss, etc.</p>"},{"location":"api_docs/agent/drqv2/#update_critic","title":".update_critic","text":"<p>source <pre><code>.update_critic(\n   obs: th.Tensor, actions: th.Tensor, rewards: th.Tensor, discount: th.Tensor,\n   next_obs: th.Tensor\n)\n</code></pre></p> <p>Update the critic network.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>discounts (th.Tensor) : discounts.</li> <li>next_obs (th.Tensor) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/agent/drqv2/#update_actor","title":".update_actor","text":"<p>source <pre><code>.update_actor(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Update the actor network.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/agent/impala/","title":"IMPALA","text":""},{"location":"api_docs/agent/impala/#impala","title":"IMPALA","text":"<p>source <pre><code>IMPALA(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', num_steps: int = 80, num_actors: int = 45, num_learners: int = 4,\n   num_storages: int = 60, feature_dim: int = 512, batch_size: int = 4, lr: float = 0.0004,\n   eps: float = 0.01, hidden_dim: int = 512, use_lstm: bool = False, ent_coef: float = 0.01,\n   baseline_coef: float = 0.5, max_grad_norm: float = 40, discount: float = 0.99,\n   init_fn: str = 'identity'\n)\n</code></pre></p> <p>Importance Weighted Actor-Learner Architecture (IMPALA). Based on: https://github.com/facebookresearch/torchbeast/blob/main/torchbeast/monobeast.py</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>num_steps (int) : The sample length of per rollout.</li> <li>num_actors (int) : Number of actors.</li> <li>num_learners (int) : Number of learners.</li> <li>num_storages (int) : Number of storages.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>use_lstm (bool) : Use LSTM in the policy network or not.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>baseline_coef (float) : Weighting coefficient of baseline value loss.</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> <li>discount (float) : Discount factor.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>IMPALA agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/impala/#update","title":".update","text":"<p>source <pre><code>.update(\n   batch: Dict, lock = threading.Lock()\n)\n</code></pre></p> <p>Update the learner model.</p> <p>Args</p> <ul> <li>batch (Batch) : Batch samples.</li> <li>lock (Lock) : Thread lock.</li> </ul> <p>Returns</p> <p>Training metrics.</p>"},{"location":"api_docs/agent/ppg/","title":"PPG","text":""},{"location":"api_docs/agent/ppg/#ppg","title":"PPG","text":"<p>source <pre><code>PPG(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_steps: int = 128,\n   feature_dim: int = 512, batch_size: int = 256, lr: float = 0.00025, eps: float = 1e-05,\n   hidden_dim: int = 512, clip_range: float = 0.2, clip_range_vf: float = 0.2,\n   vf_coef: float = 0.5, ent_coef: float = 0.01, max_grad_norm: float = 0.5,\n   policy_epochs: int = 32, aux_epochs: int = 6, kl_coef: float = 1.0,\n   num_aux_mini_batch: int = 4, num_aux_grad_accum: int = 1, discount: float = 0.999,\n   init_fn: str = 'xavier_uniform'\n)\n</code></pre></p> <p>Phasic Policy Gradient (PPG). Based on: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppg_procgen.py</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_steps (int) : The sample length of per rollout.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>clip_range (float) : Clipping parameter.</li> <li>clip_range_vf (float) : Clipping parameter for the value function.</li> <li>vf_coef (float) : Weighting coefficient of value loss.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> <li>policy_epochs (int) : Number of iterations in the policy phase.</li> <li>aux_epochs (int) : Number of iterations in the auxiliary phase.</li> <li>kl_coef (float) : Weighting coefficient of divergence loss.</li> <li>num_aux_grad_accum (int) : Number of gradient accumulation for auxiliary phase update.</li> <li>discount (float) : Discount factor.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>num_aux_mini_batch (int) Number of mini-batches in auxiliary phase.</p> <p>Returns</p> <p>PPG agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/ppg/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update function that returns training metrics such as policy loss, value loss, etc..</p>"},{"location":"api_docs/agent/legacy/a2c/","title":"A2C","text":""},{"location":"api_docs/agent/legacy/a2c/#a2c","title":"A2C","text":"<p>source <pre><code>A2C(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_steps: int = 128,\n   feature_dim: int = 512, batch_size: int = 256, lr: float = 0.00025, eps: float = 1e-05,\n   hidden_dim: int = 512, n_epochs: int = 4, vf_coef: float = 0.5, ent_coef: float = 0.01,\n   max_grad_norm: float = 0.5, discount: float = 0.99, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Advantage Actor-Critic (A2C) agent. Based on: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_steps (int) : The sample length of per rollout.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>n_epochs (int) : Times of updating the policy.</li> <li>vf_coef (float) : Weighting coefficient of value loss.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> <li>discount (float) : Discount factor.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>A2C agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/legacy/a2c/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update function that returns training metrics such as policy loss, value loss, etc..</p>"},{"location":"api_docs/agent/legacy/ddpg/","title":"DDPG","text":""},{"location":"api_docs/agent/legacy/ddpg/#ddpg","title":"DDPG","text":"<p>source <pre><code>DDPG(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_init_steps: int = 2000,\n   storage_size: int = 1000000, feature_dim: int = 50, batch_size: int = 256,\n   lr: float = 0.0001, eps: float = 1e-08, hidden_dim: int = 1024,\n   critic_target_tau: float = 0.01, update_every_steps: int = 2, discount: float = 0.99,\n   stddev_clip: float = 0.3, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Deep Deterministic Policy Gradient (DDPG) agent.</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_init_steps (int) : Number of initial exploration steps.</li> <li>storage_size (int) : The capacity of the storage.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>critic_target_tau  : The critic Q-function soft-update rate.</li> <li>update_every_steps (int) : The agent update frequency.</li> <li>discount (float) : Discount factor.</li> <li>stddev_clip (float) : The exploration std clip range.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>DDPG agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/legacy/ddpg/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update the agent and return training metrics such as actor loss, critic_loss, etc.</p>"},{"location":"api_docs/agent/legacy/ddpg/#update_critic","title":".update_critic","text":"<p>source <pre><code>.update_critic(\n   obs: th.Tensor, actions: th.Tensor, rewards: th.Tensor, terminateds: th.Tensor,\n   truncateds: th.Tensor, next_obs: th.Tensor\n)\n</code></pre></p> <p>Update the critic network.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>terminateds (th.Tensor) : Terminateds.</li> <li>truncateds (th.Tensor) : Truncateds.</li> <li>next_obs (th.Tensor) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/agent/legacy/ddpg/#update_actor","title":".update_actor","text":"<p>source <pre><code>.update_actor(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Update the actor network.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/agent/legacy/dqn/","title":"DQN","text":""},{"location":"api_docs/agent/legacy/dqn/#dqn","title":"DQN","text":"<p>source <pre><code>DQN(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_init_steps: int = 2000,\n   storage_size: int = 10000, feature_dim: int = 50, batch_size: int = 32,\n   lr: float = 0.001, eps: float = 1e-08, hidden_dim: int = 1024, tau: float = 1.0,\n   update_every_steps: int = 4, target_update_freq: int = 1000, discount: float = 0.99,\n   init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Deep Q-Network (DQN) agent.</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_init_steps (int) : Number of initial exploration steps.</li> <li>storage_size (int) : The capacity of the storage.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>tau  : The Q-function soft-update rate.</li> <li>update_every_steps (int) : The update frequency of the policy.</li> <li>target_update_freq (int) : The frequency of target Q-network update.</li> <li>discount (float) : Discount factor.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>DQN agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/legacy/dqn/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update the agent and return training metrics such as actor loss, critic_loss, etc.</p>"},{"location":"api_docs/agent/legacy/ppo/","title":"PPO","text":""},{"location":"api_docs/agent/legacy/ppo/#ppo","title":"PPO","text":"<p>source <pre><code>PPO(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_steps: int = 128,\n   feature_dim: int = 512, batch_size: int = 256, lr: float = 0.00025, eps: float = 1e-05,\n   hidden_dim: int = 512, clip_range: float = 0.1, clip_range_vf: Optional[float] = 0.1,\n   n_epochs: int = 4, vf_coef: float = 0.5, ent_coef: float = 0.01,\n   max_grad_norm: float = 0.5, discount: float = 0.999, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Proximal Policy Optimization (PPO) agent. Based on: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_steps (int) : The sample length of per rollout.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>clip_range (float) : Clipping parameter.</li> <li>clip_range_vf (Optional[float]) : Clipping parameter for the value function.</li> <li>n_epochs (int) : Times of updating the policy.</li> <li>vf_coef (float) : Weighting coefficient of value loss.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> <li>discount (float) : Discount factor.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>PPO agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/legacy/ppo/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update function that returns training metrics such as policy loss, value loss, etc..</p>"},{"location":"api_docs/agent/legacy/sac/","title":"SAC","text":""},{"location":"api_docs/agent/legacy/sac/#sac","title":"SAC","text":"<p>source <pre><code>SAC(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_init_steps: int = 5000,\n   storage_size: int = 10000000, feature_dim: int = 50, batch_size: int = 1024,\n   lr: float = 0.0001, eps: float = 1e-08, hidden_dim: int = 1024,\n   actor_update_freq: int = 1, critic_target_tau: float = 0.005,\n   critic_target_update_freq: int = 2, log_std_range: Tuple[float, ...] = (-5.0, 2),\n   betas: Tuple[float, float] = (0.9, 0.999), temperature: float = 0.1,\n   fixed_temperature: bool = False, discount: float = 0.99, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Soft Actor-Critic (SAC) agent. Based on: https://github.com/denisyarats/pytorch_sac</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_init_steps (int) : Number of initial exploration steps.</li> <li>storage_size (int) : The capacity of the storage.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>actor_update_freq (int) : The actor update frequency (in steps).</li> <li>critic_target_tau (float) : The critic Q-function soft-update rate.</li> <li>critic_target_update_freq (int) : The critic Q-function soft-update frequency (in steps).</li> <li>log_std_range (Tuple[float]) : Range of std for sampling actions.</li> <li>betas (Tuple[float]) : Coefficients used for computing running averages of gradient and its square.</li> <li>temperature (float) : Initial temperature coefficient.</li> <li>fixed_temperature (bool) : Fixed temperature or not.</li> <li>discount (float) : Discount factor.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>PPO agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/legacy/sac/#alpha","title":".alpha","text":"<p>source <pre><code>.alpha()\n</code></pre></p> <p>Get the temperature coefficient.</p>"},{"location":"api_docs/agent/legacy/sac/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update the agent and return training metrics such as actor loss, critic_loss, etc.</p>"},{"location":"api_docs/agent/legacy/sac/#update_critic","title":".update_critic","text":"<p>source <pre><code>.update_critic(\n   obs: th.Tensor, actions: th.Tensor, rewards: th.Tensor, terminateds: th.Tensor,\n   truncateds: th.Tensor, next_obs: th.Tensor\n)\n</code></pre></p> <p>Update the critic network.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>terminateds (th.Tensor) : Terminateds.</li> <li>truncateds (th.Tensor) : Truncateds.</li> <li>next_obs (th.Tensor) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/agent/legacy/sac/#update_actor_and_alpha","title":".update_actor_and_alpha","text":"<p>source <pre><code>.update_actor_and_alpha(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Update the actor network and temperature.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/agent/legacy/sacd/","title":"SACDiscrete","text":""},{"location":"api_docs/agent/legacy/sacd/#sacdiscrete","title":"SACDiscrete","text":"<p>source <pre><code>SACDiscrete(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_init_steps: int = 10000,\n   storage_size: int = 100000, feature_dim: int = 50, batch_size: int = 256,\n   lr: float = 0.0005, eps: float = 1e-08, hidden_dim: int = 256,\n   actor_update_freq: int = 1, critic_target_tau: float = 0.01,\n   critic_target_update_freq: int = 4, betas: Tuple[float, float] = (0.9, 0.999),\n   temperature: float = 0.0, fixed_temperature: bool = False,\n   target_entropy_ratio: float = 0.98, discount: float = 0.99,\n   init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Soft Actor-Critic Discrete (SAC-Discrete) agent.</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on the pre-training mode.</li> <li>num_init_steps (int) : Number of initial exploration steps.</li> <li>storage_size (int) : The capacity of the storage.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>actor_update_freq (int) : The actor update frequency (in steps).</li> <li>critic_target_tau (float) : The critic Q-function soft-update rate.</li> <li>critic_target_update_freq (int) : The critic Q-function soft-update frequency (in steps).</li> <li>betas (Tuple[float]) : Coefficients used for computing running averages of gradient and its square.</li> <li>temperature (float) : Initial temperature coefficient.</li> <li>fixed_temperature (bool) : Fixed temperature or not.</li> <li>target_entropy_ratio (float) : Target entropy ratio.</li> <li>discount (float) : Discount factor.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>PPO agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/agent/legacy/sacd/#alpha","title":".alpha","text":"<p>source <pre><code>.alpha()\n</code></pre></p> <p>Get the temperature coefficient.</p>"},{"location":"api_docs/agent/legacy/sacd/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update the agent and return training metrics such as actor loss, critic_loss, etc.</p>"},{"location":"api_docs/agent/legacy/sacd/#deal_with_zero_probs","title":".deal_with_zero_probs","text":"<p>source <pre><code>.deal_with_zero_probs(\n   action_probs: th.Tensor\n)\n</code></pre></p> <p>Deal with situation of 0.0 probabilities.</p> <p>Args</p> <ul> <li>action_probs (th.Tensor) : Action probabilities.</li> </ul> <p>Returns</p> <p>Action probabilities and its log values.</p>"},{"location":"api_docs/agent/legacy/sacd/#update_critic","title":".update_critic","text":"<p>source <pre><code>.update_critic(\n   obs: th.Tensor, actions: th.Tensor, rewards: th.Tensor, terminateds: th.Tensor,\n   truncateds: th.Tensor, next_obs: th.Tensor\n)\n</code></pre></p> <p>Update the critic network.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>terminateds (th.Tensor) : Terminateds.</li> <li>truncateds (th.Tensor) : Truncateds.</li> <li>next_obs (th.Tensor) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/agent/legacy/sacd/#update_actor_and_alpha","title":".update_actor_and_alpha","text":"<p>source <pre><code>.update_actor_and_alpha(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Update the actor network and temperature.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/initialization/","title":"Initialization","text":""},{"location":"api_docs/common/initialization/#get_init_fn","title":"get_init_fn","text":"<p>source <pre><code>.get_init_fn(\n   method: str = 'orthogonal'\n)\n</code></pre></p> <p>Returns a network initialization function.</p> <p>Args</p> <ul> <li>method (str) : Initialization method name.</li> </ul> <p>Returns</p> <p>Initialization function.</p>"},{"location":"api_docs/common/initialization/#_xavier_normal","title":"_xavier_normal","text":"<p>source <pre><code>._xavier_normal(\n   m\n)\n</code></pre></p> <p>Xavier normal initialization.</p>"},{"location":"api_docs/common/initialization/#_xavier_uniform","title":"_xavier_uniform","text":"<p>source <pre><code>._xavier_uniform(\n   m\n)\n</code></pre></p> <p>Xavier uniform initialization.</p>"},{"location":"api_docs/common/initialization/#_orthogonal","title":"_orthogonal","text":"<p>source <pre><code>._orthogonal(\n   m\n)\n</code></pre></p> <p>Orthogonal initialization.</p>"},{"location":"api_docs/common/initialization/#_identity","title":"_identity","text":"<p>source <pre><code>._identity(\n   m\n)\n</code></pre></p> <p>Identity initialization.</p>"},{"location":"api_docs/common/logger/","title":"Logger","text":""},{"location":"api_docs/common/logger/#logger","title":"Logger","text":"<p>source <pre><code>Logger(\n   log_dir: Path\n)\n</code></pre></p> <p>The logger class.</p> <p>Args</p> <ul> <li>log_dir  : The logging location.</li> </ul> <p>Returns</p> <p>Logger instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/logger/#record","title":".record","text":"<p>source <pre><code>.record(\n   key: str, value: Any\n)\n</code></pre></p> <p>Record the metric.</p> <p>Args</p> <ul> <li>key (str) : The key of the metric.</li> <li>value (Any) : The value of the metric.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/logger/#parse_train_msg","title":".parse_train_msg","text":"<p>source <pre><code>.parse_train_msg(\n   msg: Dict\n)\n</code></pre></p> <p>Parse the training message.</p> <p>Args</p> <ul> <li>msg (Dict) : The training message.</li> </ul> <p>Returns</p> <p>The formatted string.</p>"},{"location":"api_docs/common/logger/#parse_eval_msg","title":".parse_eval_msg","text":"<p>source <pre><code>.parse_eval_msg(\n   msg: Dict\n)\n</code></pre></p> <p>Parse the evaluation message.</p> <p>Args</p> <ul> <li>msg (Dict) : The evaluation message.</li> </ul> <p>Returns</p> <p>The formatted string.</p>"},{"location":"api_docs/common/logger/#time_stamp","title":".time_stamp","text":"<p>source <pre><code>.time_stamp()\n</code></pre></p> <p>Return the current time stamp.</p>"},{"location":"api_docs/common/logger/#info","title":".info","text":"<p>source <pre><code>.info(\n   msg: str\n)\n</code></pre></p> <p>Output msg with 'info' level.</p> <p>Args</p> <ul> <li>msg (str) : Message to be printed.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/logger/#debug","title":".debug","text":"<p>source <pre><code>.debug(\n   msg: str\n)\n</code></pre></p> <p>Output msg with 'debug' level.</p> <p>Args</p> <ul> <li>msg (str) : Message to be printed.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/logger/#error","title":".error","text":"<p>source <pre><code>.error(\n   msg: str\n)\n</code></pre></p> <p>Output msg with 'error' level.</p> <p>Args</p> <ul> <li>msg (str) : Message to be printed.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/logger/#train","title":".train","text":"<p>source <pre><code>.train(\n   msg: Dict\n)\n</code></pre></p> <p>Output msg with 'train' level.</p> <p>Args</p> <ul> <li>msg (Dict) : Message to be printed.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/logger/#eval","title":".eval","text":"<p>source <pre><code>.eval(\n   msg: Dict\n)\n</code></pre></p> <p>Output msg with 'eval' level.</p> <p>Args</p> <ul> <li>msg (Dict) : Message to be printed.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/preprocessing/","title":"Preprocessing","text":""},{"location":"api_docs/common/preprocessing/#process_observation_space","title":"process_observation_space","text":"<p>source <pre><code>.process_observation_space(\n   observation_space: gym.Space\n)\n</code></pre></p> <p>Process the observation space.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> </ul> <p>Returns</p> <p>Information of the observation space.</p>"},{"location":"api_docs/common/preprocessing/#process_action_space","title":"process_action_space","text":"<p>source <pre><code>.process_action_space(\n   action_space: gym.Space\n)\n</code></pre></p> <p>Get the dimension of the action space.</p> <p>Args</p> <ul> <li>action_space (gym.Space) : Action space.</li> </ul> <p>Returns</p> <p>Information of the action space.</p>"},{"location":"api_docs/common/preprocessing/#get_flattened_obs_dim","title":"get_flattened_obs_dim","text":"<p>source <pre><code>.get_flattened_obs_dim(\n   observation_space: spaces.Space\n)\n</code></pre></p> <p>Get the dimension of the observation space when flattened. It does not apply to image observation space. Borrowed from: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/preprocessing.py#L169</p> <p>Args</p> <ul> <li>observation_space (spaces.Space) : Observation space.</li> </ul> <p>Returns</p> <p>The dimension of the observation space when flattened.</p>"},{"location":"api_docs/common/preprocessing/#is_image_space_channels_first","title":"is_image_space_channels_first","text":"<p>source <pre><code>.is_image_space_channels_first(\n   observation_space: spaces.Box\n)\n</code></pre></p> <p>Check if an image observation space (see <code>is_image_space</code>) is channels-first (CxHxW, True) or channels-last (HxWxC, False). Use a heuristic that channel dimension is the smallest of the three. If second dimension is smallest, raise an exception (no support).</p> <p>Borrowed from: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/preprocessing.py#L10</p> <p>Args</p> <ul> <li>observation_space (spaces.Box) : Observation space.</li> </ul> <p>Returns</p> <p>True if observation space is channels-first image, False if channels-last.</p>"},{"location":"api_docs/common/preprocessing/#is_image_space","title":"is_image_space","text":"<p>source <pre><code>.is_image_space(\n   observation_space: gym.Space, check_channels: bool = False,\n   normalized_image: bool = False\n)\n</code></pre></p> <p>Check if a observation space has the shape, limits and dtype of a valid image. The check is conservative, so that it returns False if there is a doubt. Valid images: RGB, RGBD, GrayScale with values in [0, 255]</p> <p>Borrowed from: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/preprocessing.py#L27</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>check_channels (bool) : Whether to do or not the check for the number of channels.     e.g., with frame-stacking, the observation space may have more channels than expected.</li> <li>normalized_image (bool) : Whether to assume that the image is already normalized     or not (this disables dtype and bounds checks): when True, it only checks that     the space is a Box and has 3 dimensions.     Otherwise, it checks that it has expected dtype (uint8) and bounds (values in [0, 255]).</li> </ul> <p>Returns</p> <p>True if observation space is channels-first image, False if channels-last.</p>"},{"location":"api_docs/common/preprocessing/#preprocess_obs","title":"preprocess_obs","text":"<p>source <pre><code>.preprocess_obs(\n   obs: th.Tensor, observation_space: gym.Space\n)\n</code></pre></p> <p>Observations preprocessing function. Borrowed from: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/preprocessing.py#L92</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observation.</li> <li>observation_space (gym.Space) : Observation space.</li> </ul> <p>Returns</p> <p>A function to preprocess observations.</p>"},{"location":"api_docs/common/timer/","title":"Timer","text":""},{"location":"api_docs/common/timer/#timer","title":"Timer","text":"<p>source </p> <p>The calculagraph class.</p> <p>Methods:</p>"},{"location":"api_docs/common/timer/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the calculagraph.</p>"},{"location":"api_docs/common/timer/#total_time","title":".total_time","text":"<p>source <pre><code>.total_time()\n</code></pre></p> <p>Get the total time.</p>"},{"location":"api_docs/common/prototype/base_agent/","title":"BaseAgent","text":""},{"location":"api_docs/common/prototype/base_agent/#baseagent","title":"BaseAgent","text":"<p>source <pre><code>BaseAgent(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'auto', pretraining: bool = False\n)\n</code></pre></p> <p>Base class of the agent.</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on pre-training model or not.</li> </ul> <p>Returns</p> <p>Base agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/prototype/base_agent/#freeze","title":".freeze","text":"<p>source <pre><code>.freeze(\n   **kwargs\n)\n</code></pre></p> <p>Freeze the agent and get ready for training.</p>"},{"location":"api_docs/common/prototype/base_agent/#check","title":".check","text":"<p>source <pre><code>.check()\n</code></pre></p> <p>Check the compatibility of selected modules.</p>"},{"location":"api_docs/common/prototype/base_agent/#set","title":".set","text":"<p>source <pre><code>.set(\n   encoder: Optional[Encoder] = None, policy: Optional[Policy] = None,\n   storage: Optional[Storage] = None, distribution: Optional[Distribution] = None,\n   augmentation: Optional[Augmentation] = None,\n   reward: Optional[IntrinsicRewardModule] = None\n)\n</code></pre></p> <p>Set a module for the agent.</p> <p>Args</p> <ul> <li>encoder (Optional[Encoder]) : An encoder of <code>rllte.xploit.encoder</code> or a custom encoder.</li> <li>policy (Optional[Policy]) : A policy of <code>rllte.xploit.policy</code> or a custom policy.</li> <li>storage (Optional[Storage]) : A storage of <code>rllte.xploit.storage</code> or a custom storage.</li> <li>distribution (Optional[Distribution]) : A distribution of <code>rllte.xplore.distribution</code>     or a custom distribution.</li> <li>augmentation (Optional[Augmentation]) : An augmentation of <code>rllte.xplore.augmentation</code>     or a custom augmentation.</li> <li>reward (Optional[IntrinsicRewardModule]) : A reward of <code>rllte.xplore.reward</code> or a custom reward.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/prototype/base_agent/#mode","title":".mode","text":"<p>source <pre><code>.mode(\n   training: bool = True\n)\n</code></pre></p> <p>Set the training mode.</p> <p>Args</p> <ul> <li>training (bool) : True (training) or False (evaluation).</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/prototype/base_agent/#save","title":".save","text":"<p>source <pre><code>.save()\n</code></pre></p> <p>Save the agent.</p>"},{"location":"api_docs/common/prototype/base_agent/#update","title":".update","text":"<p>source <pre><code>.update(\n   *args, **kwargs\n)\n</code></pre></p> <p>Update function of the agent.</p>"},{"location":"api_docs/common/prototype/base_agent/#train","title":".train","text":"<p>source <pre><code>.train(\n   num_train_steps: int, init_model_path: Optional[str], log_interval: int,\n   eval_interval: int, save_interval: int, num_eval_episodes: int, th_compile: bool\n)\n</code></pre></p> <p>Training function.</p> <p>Args</p> <ul> <li>num_train_steps (int) : The number of training steps.</li> <li>init_model_path (Optional[str]) : The path of the initial model.</li> <li>log_interval (int) : The interval of logging.</li> <li>eval_interval (int) : The interval of evaluation.</li> <li>save_interval (int) : The interval of saving model.</li> <li>num_eval_episodes (int) : The number of evaluation episodes.</li> <li>th_compile (bool) : Whether to use <code>th.compile</code> or not.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/prototype/base_agent/#eval","title":".eval","text":"<p>source <pre><code>.eval(\n   num_eval_episodes: int\n)\n</code></pre></p> <p>Evaluation function.</p> <p>Args</p> <ul> <li>num_eval_episodes (int) : The number of evaluation episodes.</li> </ul> <p>Returns</p> <p>The evaluation results.</p>"},{"location":"api_docs/common/prototype/base_augmentation/","title":"BaseAugmentation","text":""},{"location":"api_docs/common/prototype/base_augmentation/#baseaugmentation","title":"BaseAugmentation","text":"<p>source </p> <p>Base class of augmentation.</p>"},{"location":"api_docs/common/prototype/base_distribution/","title":"BaseDistribution","text":""},{"location":"api_docs/common/prototype/base_distribution/#basedistribution","title":"BaseDistribution","text":"<p>source <pre><code>BaseDistribution(\n   *args, **kwargs\n)\n</code></pre></p> <p>Abstract base class of distributions. In rllte, the action noise is implemented as a distribution.</p> <p>Methods:</p>"},{"location":"api_docs/common/prototype/base_distribution/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   *args, **kwargs\n)\n</code></pre></p> <p>Generate samples.</p>"},{"location":"api_docs/common/prototype/base_encoder/","title":"BaseEncoder","text":""},{"location":"api_docs/common/prototype/base_encoder/#baseencoder","title":"BaseEncoder","text":"<p>source <pre><code>BaseEncoder(\n   observation_space: gym.Space, feature_dim: int = 0\n)\n</code></pre></p> <p>Base class that represents a features extractor.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : The observation space.</li> <li>feature_dim (int) : Number of features extracted.</li> </ul> <p>Returns</p> <p>The base encoder instance.</p>"},{"location":"api_docs/common/prototype/base_policy/","title":"BasePolicy","text":""},{"location":"api_docs/common/prototype/base_policy/#basepolicy","title":"BasePolicy","text":"<p>source <pre><code>BasePolicy(\n   observation_space: gym.Space, action_space: gym.Space, feature_dim: int,\n   hidden_dim: int, opt_class: Type[th.optim.Optimizer] = th.optim.Adam,\n   opt_kwargs: Optional[Dict[str, Any]] = None, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Base class for all policies.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>feature_dim (int) : Number of features accepted.</li> <li>hidden_dim (int) : Number of units per hidden layer.</li> <li>opt_class (Type[th.optim.Optimizer]) : Optimizer class.</li> <li>opt_kwargs (Dict[str, Any]) : Optimizer keyword arguments.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>Base policy instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/prototype/base_policy/#optimizers","title":".optimizers","text":"<p>source <pre><code>.optimizers()\n</code></pre></p> <p>Get optimizers.</p>"},{"location":"api_docs/common/prototype/base_policy/#describe","title":".describe","text":"<p>source <pre><code>.describe()\n</code></pre></p> <p>Describe the policy.</p>"},{"location":"api_docs/common/prototype/base_policy/#describe_1","title":".describe","text":"<p>source <pre><code>.describe()\n</code></pre></p> <p>Describe the policy.</p>"},{"location":"api_docs/common/prototype/base_policy/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor, training: bool = True\n)\n</code></pre></p> <p>Forward method.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observation from the environment.</li> <li>training (bool) : Whether the agent is being trained or not.</li> </ul> <p>Returns</p> <p>Sampled actions, estimated values, ..., depends on specific algorithms.</p>"},{"location":"api_docs/common/prototype/base_policy/#freeze","title":".freeze","text":"<p>source <pre><code>.freeze(\n   *args, **kwargs\n)\n</code></pre></p> <p>Freeze the policy and start training.</p>"},{"location":"api_docs/common/prototype/base_policy/#save","title":".save","text":"<p>source <pre><code>.save(\n   *args, **kwargs\n)\n</code></pre></p> <p>Save models.</p>"},{"location":"api_docs/common/prototype/base_policy/#load","title":".load","text":"<p>source <pre><code>.load(\n   path: str, device: th.device\n)\n</code></pre></p> <p>Load initial parameters.</p> <p>Args</p> <ul> <li>path (str) : Import path.</li> <li>device (th.device) : Device to use.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/prototype/base_reward/","title":"BaseIntrinsicRewardModule","text":""},{"location":"api_docs/common/prototype/base_reward/#basereward","title":"BaseReward","text":"<p>source <pre><code>BaseReward(\n   envs: VectorEnv, device: str = 'cpu', beta: float = 1.0, kappa: float = 0.0,\n   gamma: Optional[float] = None, rwd_norm_type: str = 'rms', obs_norm_type: str = 'rms'\n)\n</code></pre></p> <p>Base class of reward module.</p> <p>Args</p> <ul> <li>envs (VectorEnv) : The vectorized environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate of the weighting coefficient.</li> <li>gamma (Optional[float]) : Intrinsic reward discount rate, default is <code>None</code>.</li> <li>rwd_norm_type (str) : Normalization type for intrinsic rewards from ['rms', 'minmax', 'none'].</li> <li>obs_norm_type (str) : Normalization type for observations data from ['rms', 'none'].</li> </ul> <p>Returns</p> <p>Instance of the base reward module.</p> <p>Methods:</p>"},{"location":"api_docs/common/prototype/base_reward/#weight","title":".weight","text":"<p>source <pre><code>.weight()\n</code></pre></p> <p>Get the weighting coefficient of the intrinsic rewards.</p>"},{"location":"api_docs/common/prototype/base_reward/#scale","title":".scale","text":"<p>source <pre><code>.scale(\n   rewards: th.Tensor\n)\n</code></pre></p> <p>Scale the intrinsic rewards.</p> <p>Args</p> <ul> <li>rewards (th.Tensor) : The intrinsic rewards with shape (n_steps, n_envs).</li> </ul> <p>Returns</p> <p>The scaled intrinsic rewards.</p>"},{"location":"api_docs/common/prototype/base_reward/#normalize","title":".normalize","text":"<p>source <pre><code>.normalize(\n   x: th.Tensor\n)\n</code></pre></p> <p>Normalize the observations data, especially useful for images-based observations.</p>"},{"location":"api_docs/common/prototype/base_reward/#init_normalization","title":".init_normalization","text":"<p>source <pre><code>.init_normalization()\n</code></pre></p> <p>Initialize the normalization parameters for observations if the RMS is used.</p>"},{"location":"api_docs/common/prototype/base_reward/#watch","title":".watch","text":"<p>source <pre><code>.watch(\n   observations: th.Tensor, actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, next_observations: th.Tensor\n)\n</code></pre></p> <p>Watch the interaction processes and obtain necessary elements for reward computation.</p> <p>Args</p> <ul> <li>observations (th.Tensor) : Observations data with shape (n_envs, *obs_shape).</li> <li>actions (th.Tensor) : Actions data with shape (n_envs, *action_shape).</li> <li>rewards (th.Tensor) : Extrinsic rewards data with shape (n_envs).</li> <li>terminateds (th.Tensor) : Termination signals with shape (n_envs).</li> <li>truncateds (th.Tensor) : Truncation signals with shape (n_envs).</li> <li>next_observations (th.Tensor) : Next observations data with shape (n_envs, *obs_shape).</li> </ul> <p>Returns</p> <p>Feedbacks for the current samples.</p>"},{"location":"api_docs/common/prototype/base_reward/#compute","title":".compute","text":"<p>source <pre><code>.compute(\n   samples: Dict[str, th.Tensor], sync: bool = True\n)\n</code></pre></p> <p>Compute the rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples. A python dict consists of multiple tensors,     whose keys are ['observations', 'actions', 'rewards', 'terminateds', 'truncateds', 'next_observations'].     For example, the data shape of 'observations' is (n_steps, n_envs, *obs_shape).</li> <li>sync (bool) : Whether to update the reward module after the <code>compute</code> function, default is <code>True</code>.</li> </ul> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/common/prototype/base_reward/#update","title":".update","text":"<p>source <pre><code>.update(\n   samples: Dict[str, th.Tensor]\n)\n</code></pre></p> <p>Update the reward module if necessary.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples same as the <code>compute</code> function.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/prototype/base_storage/","title":"BaseStorage","text":""},{"location":"api_docs/common/prototype/base_storage/#basestorage","title":"BaseStorage","text":"<p>source <pre><code>BaseStorage(\n   observation_space: gym.Space, action_space: gym.Space, device: str,\n   storage_size: int, batch_size: int, num_envs: int\n)\n</code></pre></p> <p>Base class of the storage module.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : The observation space of environment.</li> <li>action_space (gym.Space) : The action space of environment.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>storage_size (int) : The size of the storage.</li> <li>batch_size (int) : Batch size of samples.</li> <li>num_envs (int) : The number of parallel environments.</li> </ul> <p>Returns</p> <p>Instance of the base storage.</p> <p>Methods:</p>"},{"location":"api_docs/common/prototype/base_storage/#to_torch","title":".to_torch","text":"<p>source <pre><code>.to_torch(\n   x: np.ndarray\n)\n</code></pre></p> <p>Convert numpy array to torch tensor.</p> <p>Args</p> <ul> <li>x (np.ndarray) : Numpy array.</li> </ul> <p>Returns</p> <p>Torch tensor.</p>"},{"location":"api_docs/common/prototype/base_storage/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the storage.</p>"},{"location":"api_docs/common/prototype/base_storage/#add","title":".add","text":"<p>source <pre><code>.add(\n   *args, **kwargs\n)\n</code></pre></p> <p>Add samples to the storage.</p>"},{"location":"api_docs/common/prototype/base_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   *args, **kwargs\n)\n</code></pre></p> <p>Sample from the storage.</p>"},{"location":"api_docs/common/prototype/base_storage/#update","title":".update","text":"<p>source <pre><code>.update(\n   *args, **kwargs\n)\n</code></pre></p> <p>Update the storage if necessary.</p>"},{"location":"api_docs/common/prototype/distributed_agent/","title":"DistributedAgent","text":""},{"location":"api_docs/common/prototype/distributed_agent/#distributedagent","title":"DistributedAgent","text":"<p>source <pre><code>DistributedAgent(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', num_steps: int = 80, num_actors: int = 45, num_learners: int = 4,\n   num_storages: int = 60, **kwargs\n)\n</code></pre></p> <p>Trainer for distributed algorithms.</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on pre-training model or not.</li> <li>num_steps (int) : The sample length of per rollout.</li> <li>num_actors (int) : Number of actors.</li> <li>num_learners (int) : Number of learners.</li> <li>num_storages (int) : Number of storages.</li> <li>kwargs  : Arbitrary arguments such as <code>batch_size</code> and <code>hidden_dim</code>.</li> </ul> <p>Returns</p> <p>Distributed agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/prototype/distributed_agent/#run","title":".run","text":"<p>source <pre><code>.run(\n   env: DistributedWrapper, actor_idx: int\n)\n</code></pre></p> <p>Sample function of each actor. Implemented by individual algorithms.</p> <p>Args</p> <ul> <li>env (DistributedWrapper) : A Gym-like environment wrapped by <code>DistributedWrapper</code>.</li> <li>actor_idx (int) : The index of actor.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/prototype/distributed_agent/#update","title":".update","text":"<p>source <pre><code>.update(\n   *args, **kwargs\n)\n</code></pre></p> <p>Update the agent. Implemented by individual algorithms.</p>"},{"location":"api_docs/common/prototype/distributed_agent/#train","title":".train","text":"<p>source <pre><code>.train(\n   num_train_steps: int, init_model_path: Optional[str] = None, log_interval: int = 1,\n   eval_interval: int = 5000, save_interval: int = 5000, num_eval_episodes: int = 10,\n   th_compile: bool = False\n)\n</code></pre></p> <p>Training function.</p> <p>Args</p> <ul> <li>num_train_steps (int) : The number of training steps.</li> <li>init_model_path (Optional[str]) : The path of the initial model.</li> <li>log_interval (int) : The interval of logging.</li> <li>eval_interval (int) : The interval of evaluation.</li> <li>save_interval (int) : The interval of saving model.</li> <li>num_eval_episodes (int) : The number of evaluation episodes.</li> <li>th_compile (bool) : Whether to use <code>th.compile</code> or not.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/prototype/distributed_agent/#eval","title":".eval","text":"<p>source <pre><code>.eval(\n   num_eval_episodes: int\n)\n</code></pre></p> <p>Evaluation function.</p> <p>Args</p> <ul> <li>num_eval_episodes (int) : The number of evaluation episodes.</li> </ul> <p>Returns</p> <p>The evaluation results.</p>"},{"location":"api_docs/common/prototype/off_policy_agent/","title":"OffPolicyAgent","text":""},{"location":"api_docs/common/prototype/off_policy_agent/#offpolicyagent","title":"OffPolicyAgent","text":"<p>source <pre><code>OffPolicyAgent(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_init_steps: int = 2000, **kwargs\n)\n</code></pre></p> <p>Trainer for off-policy algorithms.</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (Optional[VecEnv]) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on pre-training model or not.</li> <li>num_init_steps (int) : Number of initial exploration steps.</li> <li>kwargs  : Arbitrary arguments such as <code>batch_size</code> and <code>hidden_dim</code>.</li> </ul> <p>Returns</p> <p>Off-policy agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/prototype/off_policy_agent/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update the agent. Implemented by individual algorithms.</p>"},{"location":"api_docs/common/prototype/off_policy_agent/#train","title":".train","text":"<p>source <pre><code>.train(\n   num_train_steps: int, init_model_path: Optional[str] = None, log_interval: int = 1,\n   eval_interval: int = 5000, save_interval: int = 5000, num_eval_episodes: int = 10,\n   th_compile: bool = False, anneal_lr: bool = False\n)\n</code></pre></p> <p>Training function.</p> <p>Args</p> <ul> <li>num_train_steps (int) : The number of training steps.</li> <li>init_model_path (Optional[str]) : The path of the initial model.</li> <li>log_interval (int) : The interval of logging.</li> <li>eval_interval (int) : The interval of evaluation.</li> <li>save_interval (int) : The interval of saving model.</li> <li>num_eval_episodes (int) : The number of evaluation episodes.</li> <li>th_compile (bool) : Whether to use <code>th.compile</code> or not.</li> <li>anneal_lr (bool) : Whether to anneal the learning rate or not.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/prototype/off_policy_agent/#eval","title":".eval","text":"<p>source <pre><code>.eval(\n   num_eval_episodes: int\n)\n</code></pre></p> <p>Evaluation function.</p> <p>Args</p> <ul> <li>num_eval_episodes (int) : The number of evaluation episodes.</li> </ul> <p>Returns</p> <p>The evaluation results.</p>"},{"location":"api_docs/common/prototype/on_policy_agent/","title":"OnPolicyAgent","text":""},{"location":"api_docs/common/prototype/on_policy_agent/#onpolicyagent","title":"OnPolicyAgent","text":"<p>source <pre><code>OnPolicyAgent(\n   env: VecEnv, eval_env: Optional[VecEnv] = None, tag: str = 'default', seed: int = 1,\n   device: str = 'cpu', pretraining: bool = False, num_steps: int = 128\n)\n</code></pre></p> <p>Trainer for on-policy algorithms.</p> <p>Args</p> <ul> <li>env (VecEnv) : Vectorized environments for training.</li> <li>eval_env (VecEnv) : Vectorized environments for evaluation.</li> <li>tag (str) : An experiment tag.</li> <li>seed (int) : Random seed for reproduction.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>pretraining (bool) : Turn on pre-training model or not.</li> <li>num_steps (int) : The sample length of per rollout.</li> </ul> <p>Returns</p> <p>On-policy agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/prototype/on_policy_agent/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update the agent. Implemented by individual algorithms.</p>"},{"location":"api_docs/common/prototype/on_policy_agent/#train","title":".train","text":"<p>source <pre><code>.train(\n   num_train_steps: int, init_model_path: Optional[str] = None, log_interval: int = 1,\n   eval_interval: int = 100, save_interval: int = 100, num_eval_episodes: int = 10,\n   th_compile: bool = False, anneal_lr: bool = False\n)\n</code></pre></p> <p>Training function.</p> <p>Args</p> <ul> <li>num_train_steps (int) : The number of training steps.</li> <li>init_model_path (Optional[str]) : The path of the initial model.</li> <li>log_interval (int) : The interval of logging.</li> <li>eval_interval (int) : The interval of evaluation.</li> <li>save_interval (int) : The interval of saving model.</li> <li>num_eval_episodes (int) : The number of evaluation episodes.</li> <li>th_compile (bool) : Whether to use <code>th.compile</code> or not.</li> <li>anneal_lr (bool) : Whether to anneal the learning rate or not.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/prototype/on_policy_agent/#eval","title":".eval","text":"<p>source <pre><code>.eval(\n   num_eval_episodes: int\n)\n</code></pre></p> <p>Evaluation function.</p> <p>Args</p> <ul> <li>num_eval_episodes (int) : The number of evaluation episodes.</li> </ul> <p>Returns</p> <p>The evaluation results.</p>"},{"location":"api_docs/env/utils/","title":"make_rllte_env","text":""},{"location":"api_docs/env/utils/#make_rllte_env","title":"make_rllte_env","text":"<p>source <pre><code>.make_rllte_env(\n   env_id: Union[str, Callable[..., gym.Env]], num_envs: int = 1, seed: int = 1,\n   device: str = 'cpu', asynchronous: bool = True, env_kwargs: Optional[Dict[str,\n   Any]] = None\n)\n</code></pre></p> <p>Create environments that adapt to rllte engine.</p> <p>Args</p> <ul> <li>env_id (Union[str, Callable[..., gym.Env]]) : either the env ID, the env class or a callable returning an env</li> <li>num_envs (int) : Number of environments.</li> <li>seed (int) : Random seed.</li> <li>device (str) : Device to convert data.</li> <li>asynchronous (bool) : <code>True</code> for <code>AsyncVectorEnv</code> and <code>False</code> for <code>SyncVectorEnv</code>.</li> <li>env_kwargs  : Optional keyword argument to pass to the env constructor.</li> </ul> <p>Returns</p> <p>Environment wrapped by <code>TorchVecEnvWrapper</code>.</p>"},{"location":"api_docs/env/atari/__init__/","title":"make_atari_env","text":""},{"location":"api_docs/env/atari/__init__/#make_atari_env","title":"make_atari_env","text":"<p>source <pre><code>.make_atari_env(\n   env_id: str = 'Alien-v5', num_envs: int = 8, device: str = 'cpu', seed: int = 1,\n   frame_stack: int = 4, asynchronous: bool = True\n)\n</code></pre></p> <p>Create Atari environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of environments.</li> <li>device (str) : Device to convert the data.</li> <li>seed (int) : Random seed.</li> <li>frame_stack (int) : Number of stacked frames.</li> <li>asynchronous (bool) : <code>True</code> for creating asynchronous environments,     and <code>False</code> for creating synchronous environments.</li> </ul> <p>Returns</p> <p>The vectorized environments.</p>"},{"location":"api_docs/env/atari/__init__/#make_envpool_atari_env","title":"make_envpool_atari_env","text":"<p>source <pre><code>.make_envpool_atari_env(\n   env_id: str = 'Alien-v5', num_envs: int = 8, device: str = 'cpu', seed: int = 1,\n   asynchronous: bool = True\n)\n</code></pre></p> <p>Create Atari environments with <code>envpool</code>.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of environments.</li> <li>device (str) : Device to convert the data.</li> <li>seed (int) : Random seed.</li> <li>asynchronous (bool) : <code>True</code> for creating asynchronous environments,     and <code>False</code> for creating synchronous environments.</li> </ul> <p>Returns</p> <p>The vectorized environments.</p>"},{"location":"api_docs/env/bullet/__init__/","title":"make_bullet_env","text":""},{"location":"api_docs/env/bullet/__init__/#make_bullet_env","title":"make_bullet_env","text":"<p>source <pre><code>.make_bullet_env(\n   env_id: str = 'AntBulletEnv-v0', num_envs: int = 1, device: str = 'cpu', seed: int = 0,\n   parallel: bool = True\n)\n</code></pre></p> <p>Create PyBullet robotics environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of environments.</li> <li>device (str) : Device to convert the data.</li> <li>seed (int) : Random seed.</li> <li>parallel (bool) : <code>True</code> for creating asynchronous environments, and <code>False</code>     for creating synchronous environments.</li> </ul> <p>Returns</p> <p>The vectorized environments.</p>"},{"location":"api_docs/env/dmc/__init__/","title":"make_dmc_env","text":""},{"location":"api_docs/env/dmc/__init__/#make_dmc_env","title":"make_dmc_env","text":"<p>source <pre><code>.make_dmc_env(\n   env_id: str = 'humanoid_run', num_envs: int = 1, device: str = 'cpu', seed: int = 1,\n   visualize_reward: bool = True, from_pixels: bool = False, height: int = 84,\n   width: int = 84, frame_stack: int = 3, action_repeat: int = 1, asynchronous: bool = True\n)\n</code></pre></p> <p>Create DeepMind Control Suite environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of environments.</li> <li>device (str) : Device to convert the data.</li> <li>seed (int) : Random seed.</li> <li>visualize_reward (bool) : Opposite to <code>from_pixels</code>.</li> <li>from_pixels (bool) : Provide image-based observations or not.</li> <li>height (int) : Image observation height.</li> <li>width (int) : Image observation width.</li> <li>frame_stack (int) : Number of stacked frames.</li> <li>action_repeat (int) : Number of action repeats.</li> <li>asynchronous (bool) : <code>True</code> for creating asynchronous environments,     and <code>False</code> for creating synchronous environments.</li> </ul> <p>Returns</p> <p>The vectorized environments.</p>"},{"location":"api_docs/env/minigrid/__init__/","title":"init","text":""},{"location":"api_docs/env/minigrid/__init__/#make_minigrid_env","title":"make_minigrid_env","text":"<p>source <pre><code>.make_minigrid_env(\n   env_id: str = 'MiniGrid-DoorKey-5x5-v0', num_envs: int = 8,\n   fully_observable: bool = True, fully_numerical: bool = False, seed: int = 0,\n   frame_stack: int = 1, device: str = 'cpu', asynchronous: bool = True\n)\n</code></pre></p> <p>Create MiniGrid environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of environments.</li> <li>fully_observable (bool) : Fully observable gridworld using a compact grid encoding instead of the agent view.</li> <li>fully_numerical (bool) : Transforms the observation space (that has a textual component) to a fully numerical     observation space, where the textual instructions are replaced by arrays representing the indices of each     word in a fixed vocabulary.</li> <li>seed (int) : Random seed.</li> <li>frame_stack (int) : Number of stacked frames.</li> <li>device (str) : Device to convert the data.</li> <li>asynchronous (bool) : <code>True</code> for creating asynchronous environments,     and <code>False</code> for creating synchronous environments.</li> </ul> <p>Returns</p> <p>The vectorized environments.</p>"},{"location":"api_docs/env/procgen/__init__/","title":"make_procgen_env","text":""},{"location":"api_docs/env/procgen/__init__/#make_procgen_env","title":"make_procgen_env","text":"<p>source <pre><code>.make_procgen_env(\n   env_id: str = 'bigfish', num_envs: int = 64, device: str = 'cpu', seed: int = 1,\n   gamma: float = 0.99, num_levels: int = 200, start_level: int = 0,\n   distribution_mode: str = 'easy'\n)\n</code></pre></p> <p>Create Procgen environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of environments.</li> <li>device (str) : Device to convert the data.</li> <li>seed (int) : Random seed.</li> <li>gamma (float) : A discount factor.</li> <li>num_levels (int) : The number of unique levels that can be generated.     Set to 0 to use unlimited levels.</li> <li>start_level (int) : The lowest seed that will be used to generated levels.     'start_level' and 'num_levels' fully specify the set of possible levels.</li> <li>distribution_mode (str) : What variant of the levels to use, the options are \"easy\",     \"hard\", \"extreme\", \"memory\", \"exploration\".</li> </ul> <p>Returns</p> <p>The vectorized environment.</p>"},{"location":"api_docs/env/procgen/__init__/#make_envpool_procgen_env","title":"make_envpool_procgen_env","text":"<p>source <pre><code>.make_envpool_procgen_env(\n   env_id: str = 'bigfish', num_envs: int = 64, device: str = 'cpu', seed: int = 1,\n   gamma: float = 0.99, num_levels: int = 200, start_level: int = 0,\n   distribution_mode: str = 'easy', asynchronous: bool = True\n)\n</code></pre></p> <p>Create Procgen environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of environments.</li> <li>device (str) : Device to convert the data.</li> <li>seed (int) : Random seed.</li> <li>gamma (float) : A discount factor.</li> <li>num_levels (int) : The number of unique levels that can be generated.     Set to 0 to use unlimited levels.</li> <li>start_level (int) : The lowest seed that will be used to generated levels.     'start_level' and 'num_levels' fully specify the set of possible levels.</li> <li>distribution_mode (str) : What variant of the levels to use, the options are \"easy\",     \"hard\", \"extreme\", \"memory\", \"exploration\".</li> <li>asynchronous (bool) : <code>True</code> for creating asynchronous environments,     and <code>False</code> for creating synchronous environments.</li> </ul> <p>Returns</p> <p>The vectorized environments.</p>"},{"location":"api_docs/evaluation/comparison/","title":"Comparison","text":""},{"location":"api_docs/evaluation/comparison/#comparison","title":"Comparison","text":"<p>source <pre><code>Comparison(\n   scores_x: np.ndarray, scores_y: np.ndarray, get_ci: bool = False,\n   method: str = 'percentile', reps: int = 2000, confidence_interval_size: float = 0.95,\n   random_state: Optional[random.RandomState] = None\n)\n</code></pre></p> <p>Compare the performance between algorithms. Based on: https://github.com/google-research/rliable/blob/master/rliable/metrics.py</p> <p>Args</p> <ul> <li>scores_x (np.ndarray) : A matrix of size (<code>num_runs_x</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code> for algorithm <code>X</code>.</li> <li>scores_y (np.ndarray) : A matrix of size (<code>num_runs_y</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code> for algorithm <code>Y</code>.</li> <li>get_ci (bool) : Compute CIs or not.</li> <li>method (str) : One of <code>basic</code>, <code>percentile</code>, <code>bc</code> (identical to <code>debiased</code>,     <code>bias-corrected</code>), or <code>bca</code>.</li> <li>reps (int) : Number of bootstrap replications.</li> <li>confidence_interval_size (float) : Coverage of confidence interval.</li> <li>random_state (int) : If specified, ensures reproducibility in uncertainty estimates.</li> </ul> <p>Returns</p> <p>Comparer instance.</p> <p>Methods:</p>"},{"location":"api_docs/evaluation/comparison/#compute_poi","title":".compute_poi","text":"<p>source <pre><code>.compute_poi()\n</code></pre></p> <p>Compute the overall probability of imporvement of algorithm <code>X</code> over <code>Y</code>.</p>"},{"location":"api_docs/evaluation/comparison/#get_interval_estimates","title":".get_interval_estimates","text":"<p>source <pre><code>.get_interval_estimates(\n   scores_x: np.ndarray, scores_y: np.ndarray, metric: Callable\n)\n</code></pre></p> <p>Computes interval estimation of the above performance evaluators.</p> <p>Args</p> <ul> <li>scores_x (np.ndarray) : A matrix of size (<code>num_runs_x</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code> for algorithm <code>X</code>.</li> <li>scores_y (np.ndarray) : A matrix of size (<code>num_runs_y</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code> for algorithm <code>Y</code>.</li> <li>metric (Callable) : One of the above performance evaluators used for estimation.</li> </ul> <p>Returns</p> <p>Confidence intervals.</p>"},{"location":"api_docs/evaluation/performance/","title":"Performance","text":""},{"location":"api_docs/evaluation/performance/#performance","title":"Performance","text":"<p>source <pre><code>Performance(\n   scores: np.ndarray, get_ci: bool = False, method: str = 'percentile',\n   task_bootstrap: bool = False, reps: int = 50000,\n   confidence_interval_size: float = 0.95,\n   random_state: Optional[random.RandomState] = None\n)\n</code></pre></p> <p>Evaluate the performance of an algorithm. Based on: https://github.com/google-research/rliable/blob/master/rliable/metrics.py</p> <p>Args</p> <ul> <li>scores (np.ndarray) : A matrix of size (<code>num_runs</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code>.</li> <li>get_ci (bool) : Compute CIs or not.</li> <li>method (str) : One of <code>basic</code>, <code>percentile</code>, <code>bc</code> (identical to <code>debiased</code>,     <code>bias-corrected</code>), or <code>bca</code>.</li> <li>task_bootstrap (bool) :  Whether to perform bootstrapping over tasks in addition to     runs. Defaults to False. See <code>StratifiedBoostrap</code> for more details.</li> <li>reps (int) : Number of bootstrap replications.</li> <li>confidence_interval_size (float) : Coverage of confidence interval.</li> <li>random_state (int) : If specified, ensures reproducibility in uncertainty estimates.</li> </ul> <p>Returns</p> <p>Performance evaluator.</p> <p>Methods:</p>"},{"location":"api_docs/evaluation/performance/#aggregate_mean","title":".aggregate_mean","text":"<p>source <pre><code>.aggregate_mean()\n</code></pre></p> <p>Computes mean of sample mean scores per task.</p>"},{"location":"api_docs/evaluation/performance/#aggregate_median","title":".aggregate_median","text":"<p>source <pre><code>.aggregate_median()\n</code></pre></p> <p>Computes median of sample mean scores per task.</p>"},{"location":"api_docs/evaluation/performance/#aggregate_og","title":".aggregate_og","text":"<p>source <pre><code>.aggregate_og(\n   gamma: float = 1.0\n)\n</code></pre></p> <p>Computes optimality gap across all runs and tasks.</p> <p>Args</p> <ul> <li>gamma (float) : Threshold for optimality gap. All scores above <code>gamma</code> are clipped to <code>gamma</code>.</li> </ul> <p>Returns</p> <p>Optimality gap at threshold <code>gamma</code>.</p>"},{"location":"api_docs/evaluation/performance/#aggregate_iqm","title":".aggregate_iqm","text":"<p>source <pre><code>.aggregate_iqm()\n</code></pre></p> <p>Computes the interquartile mean across runs and tasks.</p>"},{"location":"api_docs/evaluation/performance/#get_interval_estimates","title":".get_interval_estimates","text":"<p>source <pre><code>.get_interval_estimates(\n   scores: np.ndarray, metric: Callable\n)\n</code></pre></p> <p>Computes interval estimation of the above performance evaluators.</p> <p>Args</p> <ul> <li>scores (np.ndarray) : A matrix of size (<code>num_runs</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code>.</li> <li>metric (Callable) : One of the above performance evaluators used for estimation.</li> </ul> <p>Returns</p> <p>Confidence intervals.</p>"},{"location":"api_docs/evaluation/performance/#create_performance_profile","title":".create_performance_profile","text":"<p>source <pre><code>.create_performance_profile(\n   tau_list: Union[List[float], np.ndarray], use_score_distribution: bool = True\n)\n</code></pre></p> <p>Method for calculating performance profilies.</p> <p>Args</p> <ul> <li>tau_list (Union[List[float], np.ndarray]) : List of 1D numpy array of threshold     values on which the profile is evaluated.</li> <li>use_score_distribution (bool) : Whether to report score distributions or average     score distributions.</li> </ul> <p>Returns</p> <p>Point and interval estimates of profiles evaluated at all thresholds in 'tau_list'.</p>"},{"location":"api_docs/evaluation/utils/","title":"Utils","text":""},{"location":"api_docs/evaluation/utils/#min_max_normalize","title":"min_max_normalize","text":"<p>source <pre><code>.min_max_normalize(\n   value: np.ndarray, min_scores: np.ndarray, max_scores: np.ndarray\n)\n</code></pre></p> <p>Perform <code>Max-Min</code> normalization.</p>"},{"location":"api_docs/evaluation/visualization/","title":"Visualization","text":""},{"location":"api_docs/evaluation/visualization/#plot_interval_estimates","title":"plot_interval_estimates","text":"<p>source <pre><code>.plot_interval_estimates(\n   metrics_dict: Dict[str, Dict], metric_names: List[str], algorithms: List[str],\n   colors: Optional[List[str]] = None, color_palette: str = 'colorblind',\n   max_ticks: float = 4, subfigure_width: float = 3.4, row_height: float = 0.37,\n   interval_height: float = 0.6, xlabel_y_coordinate: float = -0.16,\n   xlabel: str = 'NormalizedScore', **kwargs\n)\n</code></pre></p> <p>Plots verious metrics of algorithms with stratified confidence intervals. Based on: https://github.com/google-research/rliable/blob/master/rliable/plot_utils.py See https://docs.rllte.dev/tutorials/evaluation/ for usage tutorials.</p> <p>Args</p> <ul> <li>metrics_dict (Dict[str, Dict]) : The dictionary of various metrics of algorithms.</li> <li>metric_names (List[str]) : Names of the metrics corresponding to <code>metrics_dict</code>.</li> <li>algorithms (List[str]) : List of methods used for plotting.</li> <li>colors (Optional[List[str]]) : Maps each method to a color.     If None, then this mapping is created based on <code>color_palette</code>.</li> <li>color_palette (str) : <code>seaborn.color_palette</code> object for mapping each method to a color.</li> <li>max_ticks (float) : Find nice tick locations with no more than <code>max_ticks</code>. Passed to <code>plt.MaxNLocator</code>.</li> <li>subfigure_width (float) : Width of each subfigure.</li> <li>row_height (float) : Height of each row in a subfigure.</li> <li>interval_height (float) : Height of confidence intervals.</li> <li>xlabel_y_coordinate (float) : y-coordinate of the x-axis label.</li> <li>xlabel (str) : Label for the x-axis.</li> <li>kwargs  : Arbitrary keyword arguments.</li> </ul> <p>Returns</p> <p>A matplotlib figure and an array of Axes.</p>"},{"location":"api_docs/evaluation/visualization/#plot_performance_profile","title":"plot_performance_profile","text":"<p>source <pre><code>.plot_performance_profile(\n   profile_dict: Dict[str, List], tau_list: np.ndarray,\n   use_non_linear_scaling: bool = False, figsize: Tuple[float, float] = (10.0, 5.0),\n   colors: Optional[List[str]] = None, color_palette: str = 'colorblind',\n   alpha: float = 0.15, xticks: Optional[Iterable] = None,\n   yticks: Optional[Iterable] = None,\n   xlabel: Optional[str] = 'NormalizedScore($\\\\tau$)',\n   ylabel: Optional[str] = 'Fractionofrunswithscore$&gt;\\\\tau$',\n   linestyles: Optional[str] = None, **kwargs\n)\n</code></pre></p> <p>Plots performance profiles with stratified confidence intervals. Based on: https://github.com/google-research/rliable/blob/master/rliable/plot_utils.py See https://docs.rllte.dev/tutorials/evaluation/ for usage tutorials.</p> <p>Args</p> <ul> <li>profile_dict (Dict[str, List]) : A dictionary mapping a method to its performance.</li> <li>tau_list (np.ndarray) : 1D numpy array of threshold values on which the profile is evaluated.</li> <li>use_non_linear_scaling (bool) : Whether to scale the x-axis in proportion to the     number of runs within any specified range.</li> <li>figsize (Tuple[float]) : Size of the figure passed to <code>matplotlib.subplots</code>.</li> <li>colors (Optional[List[str]]) : Maps each method to a color. If None, then     this mapping is created based on <code>color_palette</code>.</li> <li>color_palette (str) : <code>seaborn.color_palette</code> object for mapping each method to a color.</li> <li>alpha (float) : Changes the transparency of the shaded regions corresponding to the confidence intervals.</li> <li>xticks (Optional[Iterable]) : The list of x-axis tick locations. Passing an empty list removes all xticks.</li> <li>yticks (Optional[Iterable]) : The list of y-axis tick locations between 0 and 1.     If None, defaults to <code>[0, 0.25, 0.5, 0.75, 1.0]</code>.</li> <li>xlabel (str) : Label for the x-axis.</li> <li>ylabel (str) : Label for the y-axis.</li> <li>linestyles (str) : Maps each method to a linestyle. If None, then the 'solid' linestyle is used for all methods.</li> <li>kwargs  : Arbitrary keyword arguments for annotating and decorating the     figure. For valid arguments, refer to <code>_annotate_and_decorate_axis</code>.</li> </ul> <p>Returns</p> <p>A matplotlib figure and <code>axes.Axes</code> which contains the plot for performance profiles.</p>"},{"location":"api_docs/evaluation/visualization/#plot_probability_improvement","title":"plot_probability_improvement","text":"<p>source <pre><code>.plot_probability_improvement(\n   poi_dict: Dict[str, List], pair_separator: str = '_', figsize: Tuple[float,\n   float] = (3.7, 2.1), colors: Optional[List[str]] = None,\n   color_palette: str = 'colorblind', alpha: float = 0.75, interval_height: float = 0.6,\n   xticks: Optional[Iterable] = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n   xlabel: str = 'P(X&gt;Y)', left_ylabel: str = 'AlgorithmX',\n   right_ylabel: str = 'AlgorithmY', **kwargs\n)\n</code></pre></p> <p>Plots probability of improvement with stratified confidence intervals. Based on: https://github.com/google-research/rliable/blob/master/rliable/plot_utils.py See https://docs.rllte.dev/tutorials/evaluation/ for usage tutorials.</p> <p>Args</p> <ul> <li>poi_dict (Dict[str, List]) : The dictionary of probability of improvements of different algorithms pairs.</li> <li>pair_separator (str) : Each algorithm pair name in dictionaries above is joined by a string separator.     For example, if the pairs are specified as 'X;Y', then the separator corresponds to ';'. Defaults to ','.</li> <li>figsize (Tuple[float]) : Size of the figure passed to <code>matplotlib.subplots</code>.</li> <li>colors (Optional[List[str]]) : Maps each method to a color. If None, then this mapping     is created based on <code>color_palette</code>.</li> <li>color_palette (str) : <code>seaborn.color_palette</code> object for mapping each method to a color.</li> <li>interval_height (float) : Height of confidence intervals.</li> <li>alpha (float) : Changes the transparency of the shaded regions corresponding to the confidence intervals.</li> <li>xticks (Optional[Iterable]) : The list of x-axis tick locations. Passing an empty list removes all xticks.</li> <li>xlabel (str) : Label for the x-axis.</li> <li>left_ylabel (str) : Label for the left y-axis. Defaults to 'Algorithm X'.</li> <li>right_ylabel (str) : Label for the left y-axis. Defaults to 'Algorithm Y'.</li> <li>kwargs  : Arbitrary keyword arguments for annotating and decorating the     figure. For valid arguments, refer to <code>_annotate_and_decorate_axis</code>.</li> </ul> <p>Returns</p> <p>A matplotlib figure and <code>axes.Axes</code> which contains the plot for probability of improvement.</p>"},{"location":"api_docs/evaluation/visualization/#plot_sample_efficiency_curve","title":"plot_sample_efficiency_curve","text":"<p>source <pre><code>.plot_sample_efficiency_curve(\n   sampling_dict: Dict[str, Dict], frames: np.ndarray, algorithms: List[str],\n   colors: Optional[List[str]] = None, color_palette: str = 'colorblind',\n   figsize: Tuple[float, float] = (3.7, 2.1),\n   xlabel: Optional[str] = 'NumberofFrames(inmillions)',\n   ylabel: Optional[str] = 'AggregateHumanNormalizedScore',\n   labelsize: str = 'xx-large', ticklabelsize: str = 'xx-large', **kwargs\n)\n</code></pre></p> <p>Plots an aggregate metric with CIs as a function of environment frames. Based on: https://github.com/google-research/rliable/blob/master/rliable/plot_utils.py See https://docs.rllte.dev/tutorials/evaluation/ for usage tutorials.</p> <p>Args</p> <ul> <li>sampling_dict (Dict[str, Dict]) : A dictionary of values with stratified confidence intervals in different frames.</li> <li>frames (np.ndarray) : Array containing environment frames to mark on the x-axis.</li> <li>algorithms (List[str]) : List of methods used for plotting.</li> <li>colors (Optional[List[str]]) : Maps each method to a color. If None, then this mapping     is created based on <code>color_palette</code>.</li> <li>color_palette (str) : <code>seaborn.color_palette</code> object for mapping each method to a color.</li> <li>max_ticks (float) : Find nice tick locations with no more than <code>max_ticks</code>. Passed to <code>plt.MaxNLocator</code>.</li> <li>subfigure_width (float) : Width of each subfigure.</li> <li>row_height (float) : Height of each row in a subfigure.</li> <li>interval_height (float) : Height of confidence intervals.</li> <li>xlabel_y_coordinate (float) : y-coordinate of the x-axis label.</li> <li>xlabel (str) : Label for the x-axis.</li> <li>kwargs  : Arbitrary keyword arguments.</li> </ul> <p>Returns</p> <p>A matplotlib figure and an array of Axes.</p>"},{"location":"api_docs/hub/atari/","title":"Atari","text":""},{"location":"api_docs/hub/atari/#atari","title":"Atari","text":"<p>source </p> <p>Scores and learning cures of various RL algorithms on the full Atari benchmark. Environment link: https://github.com/Farama-Foundation/Arcade-Learning-Environment Number of environments: 57 Number of training steps: 10,000,000 Number of seeds: 10 Added algorithms: [PPO]</p> <p>Methods:</p>"},{"location":"api_docs/hub/atari/#load_scores","title":".load_scores","text":"<p>source <pre><code>.load_scores(\n   env_id: str, agent: str\n)\n</code></pre></p> <p>Returns final performance.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent_id (str) : Agent name.</li> </ul> <p>Returns</p> <p>Test scores data array with shape (N_SEEDS, N_POINTS).</p>"},{"location":"api_docs/hub/atari/#load_curves","title":".load_curves","text":"<p>source <pre><code>.load_curves(\n   env_id: str, agent: str\n)\n</code></pre></p> <p>Returns learning curves using a <code>Dict</code> of NumPy arrays.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent_id (str) : Agent name.</li> </ul> <p>Returns</p> <ul> <li>train  : np.ndarray(shape=(N_SEEDS, N_POINTS))</li> <li>eval  :  np.ndarray(shape=(N_SEEDS, N_POINTS)) Learning curves data with structure: curves</li> </ul>"},{"location":"api_docs/hub/atari/#load_models","title":".load_models","text":"<p>source <pre><code>.load_models(\n   env_id: str, agent: str, seed: int, device: str = 'cpu'\n)\n</code></pre></p> <p>Load the model from the hub.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent (str) : Agent name.</li> <li>seed (int) : The seed to load.</li> <li>device (str) : The device to load the model on.</li> </ul> <p>Returns</p> <p>The loaded model.</p>"},{"location":"api_docs/hub/atari/#load_apis","title":".load_apis","text":"<p>source <pre><code>.load_apis(\n   env_id: str, agent: str, seed: int, device: str = 'cpu'\n)\n</code></pre></p> <p>Load the a training API.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent (str) : Agent name.</li> <li>seed (int) : The seed to load.</li> <li>device (str) : The device to load the model on.</li> </ul> <p>Returns</p> <p>The loaded API.</p>"},{"location":"api_docs/hub/dmc/","title":"DMControl","text":""},{"location":"api_docs/hub/dmc/#dmcontrol","title":"DMControl","text":"<p>source </p> <p>Scores and learning cures of various RL algorithms on the full DeepMind Control Suite benchmark.</p> <p>Environment link: https://github.com/google-deepmind/dm_control Number of environments: 27 Number of training steps: 10,000,000 for humanoid, 2,000,000 for others Number of seeds: 10 Added algorithms: [SAC, DrQ-v2]</p> <p>Methods:</p>"},{"location":"api_docs/hub/dmc/#get_obs_type","title":".get_obs_type","text":"<p>source <pre><code>.get_obs_type(\n   agent: str\n)\n</code></pre></p> <p>Returns the observation type of the agent.</p> <p>Args</p> <ul> <li>agent (str) : Agent name.</li> </ul> <p>Returns</p> <p>Observation type.</p>"},{"location":"api_docs/hub/dmc/#load_scores","title":".load_scores","text":"<p>source <pre><code>.load_scores(\n   env_id: str, agent: str\n)\n</code></pre></p> <p>Returns final performance.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent_id (str) : Agent name.</li> </ul> <p>Returns</p> <p>Test scores data array with shape (N_SEEDS, N_POINTS).</p>"},{"location":"api_docs/hub/dmc/#load_curves","title":".load_curves","text":"<p>source <pre><code>.load_curves(\n   env_id: str, agent: str\n)\n</code></pre></p> <p>Returns learning curves using a <code>Dict</code> of NumPy arrays.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent_id (str) : Agent name.</li> <li>obs_type (str) : A type from ['state', 'pixel'].</li> </ul> <p>Returns</p> <ul> <li>train  : np.ndarray(shape=(N_SEEDS, N_POINTS))</li> <li>eval  :  np.ndarray(shape=(N_SEEDS, N_POINTS)) Learning curves data with structure: curves</li> </ul>"},{"location":"api_docs/hub/dmc/#load_models","title":".load_models","text":"<p>source <pre><code>.load_models(\n   env_id: str, agent: str, seed: int, device: str = 'cpu'\n)\n</code></pre></p> <p>Load the model from the hub.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent (str) : Agent name.</li> <li>seed (int) : The seed to load.</li> <li>device (str) : The device to load the model on.</li> </ul> <p>Returns</p> <p>The loaded model.</p>"},{"location":"api_docs/hub/dmc/#load_apis","title":".load_apis","text":"<p>source <pre><code>.load_apis(\n   env_id: str, agent: str, seed: int, device: str = 'cpu'\n)\n</code></pre></p> <p>Load the a training API.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent (str) : Agent name.</li> <li>seed (int) : The seed to load.</li> <li>device (str) : The device to load the model on.</li> </ul> <p>Returns</p> <p>The loaded API.</p>"},{"location":"api_docs/hub/minigrid/","title":"MiniGrid","text":""},{"location":"api_docs/hub/minigrid/#minigrid","title":"MiniGrid","text":"<p>source </p> <p>Scores and learning cures of various RL algorithms on the MiniGrid benchmark. Environment link: https://github.com/Farama-Foundation/Minigrid Number of environments: 16 Number of training steps: 1,000,000 Number of seeds: 10 Added algorithms: [A2C]</p> <p>Methods:</p>"},{"location":"api_docs/hub/minigrid/#load_scores","title":".load_scores","text":"<p>source <pre><code>.load_scores(\n   env_id: str, agent: str\n)\n</code></pre></p> <p>Returns final performance.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent_id (str) : Agent name.</li> </ul> <p>Returns</p> <p>Test scores data array with shape (N_SEEDS, N_POINTS).</p>"},{"location":"api_docs/hub/minigrid/#load_curves","title":".load_curves","text":"<p>source <pre><code>.load_curves(\n   env_id: str, agent: str\n)\n</code></pre></p> <p>Returns learning curves using a <code>Dict</code> of NumPy arrays.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent_id (str) : Agent name.</li> </ul> <p>Returns</p> <ul> <li>train  : np.ndarray(shape=(N_SEEDS, N_POINTS))</li> <li>eval  :  np.ndarray(shape=(N_SEEDS, N_POINTS)) Learning curves data with structure: curves</li> </ul>"},{"location":"api_docs/hub/minigrid/#load_models","title":".load_models","text":"<p>source <pre><code>.load_models(\n   env_id: str, agent: str, seed: int, device: str = 'cpu'\n)\n</code></pre></p> <p>Load the model from the hub.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent (str) : Agent name.</li> <li>seed (int) : The seed to load.</li> <li>device (str) : The device to load the model on.</li> </ul> <p>Returns</p> <p>The loaded model.</p>"},{"location":"api_docs/hub/minigrid/#load_apis","title":".load_apis","text":"<p>source <pre><code>.load_apis(\n   env_id: str, agent: str, seed: int, device: str = 'cpu'\n)\n</code></pre></p> <p>Load the a training API.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent (str) : Agent name.</li> <li>seed (int) : The seed to load.</li> <li>device (str) : The device to load the model on.</li> </ul> <p>Returns</p> <p>The loaded API.</p>"},{"location":"api_docs/hub/procgen/","title":"Procgen","text":""},{"location":"api_docs/hub/procgen/#procgen","title":"Procgen","text":"<p>source </p> <p>Scores and learning cures of various RL algorithms on the full Procgen benchmark. Environment link: https://github.com/openai/procgen Number of environments: 16 Number of training steps: 25,000,000 Number of seeds: 10 Added algorithms: [PPO]</p> <p>Methods:</p>"},{"location":"api_docs/hub/procgen/#load_scores","title":".load_scores","text":"<p>source <pre><code>.load_scores(\n   env_id: str, agent: str\n)\n</code></pre></p> <p>Returns final performance.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent_id (str) : Agent name.</li> </ul> <p>Returns</p> <p>Test scores data array with shape (N_SEEDS, N_POINTS).</p>"},{"location":"api_docs/hub/procgen/#load_curves","title":".load_curves","text":"<p>source <pre><code>.load_curves(\n   env_id: str, agent: str\n)\n</code></pre></p> <p>Returns learning curves using a <code>Dict</code> of NumPy arrays.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent_id (str) : Agent name.</li> </ul> <p>Returns</p> <ul> <li>train  : np.ndarray(shape=(N_SEEDS, N_POINTS))</li> <li>eval  :  np.ndarray(shape=(N_SEEDS, N_POINTS)) Learning curves data with structure: curves</li> </ul>"},{"location":"api_docs/hub/procgen/#load_models","title":".load_models","text":"<p>source <pre><code>.load_models(\n   env_id: str, agent: str, seed: int, device: str = 'cpu'\n)\n</code></pre></p> <p>Load the model from the hub.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent (str) : Agent name.</li> <li>seed (int) : The seed to load.</li> <li>device (str) : The device to load the model on.</li> </ul> <p>Returns</p> <p>The loaded model.</p>"},{"location":"api_docs/hub/procgen/#load_apis","title":".load_apis","text":"<p>source <pre><code>.load_apis(\n   env_id: str, agent: str, seed: int, device: str = 'cpu'\n)\n</code></pre></p> <p>Load the a training API.</p> <p>Args</p> <ul> <li>env_id (str) : Environment ID.</li> <li>agent (str) : Agent name.</li> <li>seed (int) : The seed to load.</li> <li>device (str) : The device to load the model on.</li> </ul> <p>Returns</p> <p>The loaded API.</p>"},{"location":"api_docs/xploit/encoder/espeholt_residual_encoder/","title":"EspeholtResidualEncoder","text":""},{"location":"api_docs/xploit/encoder/espeholt_residual_encoder/#espeholtresidualencoder","title":"EspeholtResidualEncoder","text":"<p>source <pre><code>EspeholtResidualEncoder(\n   observation_space: gym.Space, feature_dim: int = 0, net_arch: List[int] = [16, 32,\n   32]\n)\n</code></pre></p> <p>ResNet-like encoder for processing image-based observations. Proposed by Espeholt L, Soyer H, Munos R, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures[C]//International conference on machine learning. PMLR, 2018: 1407-1416. Target task: Atari games and Procgen games.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>feature_dim (int) : Number of features extracted.</li> <li>net_arch (List) : Architecture of the network.     It represents the out channels of each residual layer.     The length of this list is the number of residual layers.</li> </ul> <p>Returns</p> <p>ResNet-like encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/espeholt_residual_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Forward method implementation.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observation tensor.</li> </ul> <p>Returns</p> <p>Encoded observation tensor.</p>"},{"location":"api_docs/xploit/encoder/identity_encoder/","title":"IdentityEncoder","text":""},{"location":"api_docs/xploit/encoder/identity_encoder/#identityencoder","title":"IdentityEncoder","text":"<p>source <pre><code>IdentityEncoder(\n   observation_space: gym.Space, feature_dim: int = 64\n)\n</code></pre></p> <p>Identity encoder for state-based observations.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>feature_dim (int) : Number of features extracted.</li> </ul> <p>Returns</p> <p>Identity encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/identity_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Forward method implementation.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observation tensor.</li> </ul> <p>Returns</p> <p>Encoded observation tensor.</p>"},{"location":"api_docs/xploit/encoder/mnih_cnn_encoder/","title":"MnihCnnEncoder","text":""},{"location":"api_docs/xploit/encoder/mnih_cnn_encoder/#mnihcnnencoder","title":"MnihCnnEncoder","text":"<p>source <pre><code>MnihCnnEncoder(\n   observation_space: gym.Space, feature_dim: int = 0\n)\n</code></pre></p> <p>Convolutional neural network (CNN)-based encoder for processing image-based observations. Proposed by Mnih V, Kavukcuoglu K, Silver D, et al. Playing atari with deep reinforcement learning[J]. arXiv preprint arXiv:1312.5602, 2013. Target task: Atari games.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>feature_dim (int) : Number of features extracted.</li> </ul> <p>Returns</p> <p>CNN-based encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/mnih_cnn_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Forward method implementation.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observation tensor.</li> </ul> <p>Returns</p> <p>Encoded observation tensor.</p>"},{"location":"api_docs/xploit/encoder/pathak_cnn_encoder/","title":"PathakCnnEncoder","text":""},{"location":"api_docs/xploit/encoder/pathak_cnn_encoder/#pathakcnnencoder","title":"PathakCnnEncoder","text":"<p>source <pre><code>PathakCnnEncoder(\n   observation_space: gym.Space, feature_dim: int = 0\n)\n</code></pre></p> <p>Convolutional neural network (CNN)-based encoder for processing image-based observations. Proposed by Pathak D, Agrawal P, Efros A A, et al. Curiosity-driven exploration by self-supervised prediction[C]// International conference on machine learning. PMLR, 2017: 2778-2787. Target task: Atari and MiniGrid games.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>feature_dim (int) : Number of features extracted.</li> </ul> <p>Returns</p> <p>CNN-based encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/pathak_cnn_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Forward method implementation.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observation tensor.</li> </ul> <p>Returns</p> <p>Encoded observation tensor.</p>"},{"location":"api_docs/xploit/encoder/raffin_combined_encoder/","title":"RaffinCombinedEncoder","text":""},{"location":"api_docs/xploit/encoder/raffin_combined_encoder/#raffincombinedencoder","title":"RaffinCombinedEncoder","text":"<p>source <pre><code>RaffinCombinedEncoder(\n   observation_space: gym.Space, feature_dim: int = 256, cnn_output_dim: int = 256\n)\n</code></pre></p> <p>Combined features extractor for Dict observation spaces. Based on: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/torch_layers.py#L231</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>feature_dim (int) : Number of features extracted.</li> <li>cnn_output_dim (int) : Number of features extracted by the CNN.</li> </ul> <p>Returns</p> <p>Identity encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/raffin_combined_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: Dict[str, th.Tensor]\n)\n</code></pre></p> <p>Forward method implementation.</p> <p>Args</p> <ul> <li>obs (Dict[str, th.Tensor]) : Observation tensor.</li> </ul> <p>Returns</p> <p>Encoded observation tensor.</p>"},{"location":"api_docs/xploit/encoder/tassa_cnn_encoder/","title":"TassaCnnEncoder","text":""},{"location":"api_docs/xploit/encoder/tassa_cnn_encoder/#tassacnnencoder","title":"TassaCnnEncoder","text":"<p>source <pre><code>TassaCnnEncoder(\n   observation_space: gym.Space, feature_dim: int = 50\n)\n</code></pre></p> <p>Convolutional neural network (CNN)-based encoder for processing image-based observations. Proposed by Tassa Y, Doron Y, Muldal A, et al. Deepmind control suite[J]. arXiv preprint arXiv:1801.00690, 2018. Target task: DeepMind Control Suite.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> </ul> <p>Returns</p> <p>CNN-based encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/tassa_cnn_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Forward method implementation.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observation tensor.</li> </ul> <p>Returns</p> <p>Encoded observation tensor.</p>"},{"location":"api_docs/xploit/encoder/vanilla_mlp_encoder/","title":"VanillaMlpEncoder","text":""},{"location":"api_docs/xploit/encoder/vanilla_mlp_encoder/#vanillamlpencoder","title":"VanillaMlpEncoder","text":"<p>source <pre><code>VanillaMlpEncoder(\n   observation_space: gym.Space, feature_dim: int = 64, hidden_dim: int = 64\n)\n</code></pre></p> <p>Multi layer perceptron (MLP) for processing state-based inputs.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>feature_dim (int) : Number of features extracted.</li> <li>hidden_dim (int) : Number of hidden units in the hidden layer.</li> </ul> <p>Returns</p> <p>Mlp-based encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/vanilla_mlp_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Forward method implementation.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observation tensor.</li> </ul> <p>Returns</p> <p>Encoded observation tensor.</p>"},{"location":"api_docs/xploit/policy/distributed_actor_learner/","title":"DistributedActorLearner","text":""},{"location":"api_docs/xploit/policy/distributed_actor_learner/#distributedactorlearner","title":"DistributedActorLearner","text":"<p>source <pre><code>DistributedActorLearner(\n   observation_space: gym.Space, action_space: gym.Space, feature_dim: int,\n   hidden_dim: int = 512, opt_class: Type[th.optim.Optimizer] = th.optim.Adam,\n   opt_kwargs: Optional[Dict[str, Any]] = None, init_fn: str = 'orthogonal',\n   use_lstm: bool = False\n)\n</code></pre></p> <p>Actor-Learner network for IMPALA.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>feature_dim (int) : Number of features accepted.</li> <li>hidden_dim (int) : Number of units per hidden layer.</li> <li>opt_class (Type[th.optim.Optimizer]) : Optimizer class.</li> <li>opt_kwargs (Dict[str, Any]) : Optimizer keyword arguments.</li> <li>init_fn (str) : Parameters initialization method.</li> <li>use_lstm (bool) : Whether to use LSTM module.</li> </ul> <p>Returns</p> <p>Actor-Critic network.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/policy/distributed_actor_learner/#describe","title":".describe","text":"<p>source <pre><code>.describe()\n</code></pre></p> <p>Describe the policy.</p>"},{"location":"api_docs/xploit/policy/distributed_actor_learner/#explore","title":".explore","text":"<p>source <pre><code>.explore(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Explore the environment and randomly generate actions.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observation from the environment.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/policy/distributed_actor_learner/#freeze","title":".freeze","text":"<p>source <pre><code>.freeze(\n   encoder: nn.Module, dist: Distribution\n)\n</code></pre></p> <p>Freeze all the elements like <code>encoder</code> and <code>dist</code>.</p> <p>Args</p> <ul> <li>encoder (nn.Module) : Encoder network.</li> <li>dist (Distribution) : Distribution.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/distributed_actor_learner/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   *args\n)\n</code></pre></p> <p>Only for inference.</p>"},{"location":"api_docs/xploit/policy/distributed_actor_learner/#to","title":".to","text":"<p>source <pre><code>.to(\n   device: th.device\n)\n</code></pre></p> <p>Only move the learner to device, and keep actor in CPU.</p> <p>Args</p> <ul> <li>device (th.device) : Device to use.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/distributed_actor_learner/#save","title":".save","text":"<p>source <pre><code>.save(\n   path: Path, pretraining: bool, global_step: int\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Save path.</li> <li>pretraining (bool) : Pre-training mode.</li> <li>global_step (int) : Global training step.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/distributed_actor_learner/#load","title":".load","text":"<p>source <pre><code>.load(\n   path: str, device: th.device\n)\n</code></pre></p> <p>Load initial parameters.</p> <p>Args</p> <ul> <li>path (str) : Import path.</li> <li>device (th.device) : Device to use.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/off_policy_det_actor_double_critic/","title":"OffPolicyDetActorDoubleCritic","text":""},{"location":"api_docs/xploit/policy/off_policy_det_actor_double_critic/#offpolicydetactordoublecritic","title":"OffPolicyDetActorDoubleCritic","text":"<p>source <pre><code>OffPolicyDetActorDoubleCritic(\n   observation_space: gym.Space, action_space: gym.Space, feature_dim: int = 64,\n   hidden_dim: int = 1024, opt_class: Type[th.optim.Optimizer] = th.optim.Adam,\n   opt_kwargs: Optional[Dict[str, Any]] = None, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Deterministic actor network and double critic network for off-policy algortithms like <code>DrQv2</code>, <code>DDPG</code>. Here the 'self.dist' refers to an action noise.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>feature_dim (int) : Number of features accepted.</li> <li>hidden_dim (int) : Number of units per hidden layer.</li> <li>opt_class (Type[th.optim.Optimizer]) : Optimizer class.</li> <li>opt_kwargs (Dict[str, Any]) : Optimizer keyword arguments.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>Actor-Critic network.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/policy/off_policy_det_actor_double_critic/#describe","title":".describe","text":"<p>source <pre><code>.describe()\n</code></pre></p> <p>Describe the policy.</p>"},{"location":"api_docs/xploit/policy/off_policy_det_actor_double_critic/#freeze","title":".freeze","text":"<p>source <pre><code>.freeze(\n   encoder: nn.Module, dist: Distribution\n)\n</code></pre></p> <p>Freeze all the elements like <code>encoder</code> and <code>dist</code>.</p> <p>Args</p> <ul> <li>encoder (nn.Module) : Encoder network.</li> <li>dist (Distribution) : Distribution.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/off_policy_det_actor_double_critic/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor, training: bool = True\n)\n</code></pre></p> <p>Sample actions based on observations.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>training (bool) : Training mode, True or False.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/policy/off_policy_det_actor_double_critic/#get_dist","title":".get_dist","text":"<p>source <pre><code>.get_dist(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Get sample distribution.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> </ul> <p>Returns</p> <p>RLLTE distribution.</p>"},{"location":"api_docs/xploit/policy/off_policy_det_actor_double_critic/#save","title":".save","text":"<p>source <pre><code>.save(\n   path: Path, pretraining: bool, global_step: int\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Save path.</li> <li>pretraining (bool) : Pre-training mode.</li> <li>global_step (int) : Global training step.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/off_policy_double_actor_double_critic/","title":"OffPolicyDoubleActorDoubleCritic","text":""},{"location":"api_docs/xploit/policy/off_policy_double_actor_double_critic/#offpolicydoubleactordoublecritic","title":"OffPolicyDoubleActorDoubleCritic","text":"<p>source <pre><code>OffPolicyDoubleActorDoubleCritic(\n   observation_space: gym.Space, action_space: gym.Space, feature_dim: int = 64,\n   hidden_dim: int = 1024, opt_class: Type[th.optim.Optimizer] = th.optim.Adam,\n   opt_kwargs: Optional[Dict[str, Any]] = None, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Double deterministic actor network and double critic network for off-policy algortithms like <code>DDPG</code>, <code>TD3</code>. Here the 'self.dist' refers to an action noise.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>feature_dim (int) : Number of features accepted.</li> <li>hidden_dim (int) : Number of units per hidden layer.</li> <li>opt_class (Type[th.optim.Optimizer]) : Optimizer class.</li> <li>opt_kwargs (Dict[str, Any]) : Optimizer keyword arguments.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>Actor-Critic network.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/policy/off_policy_double_actor_double_critic/#describe","title":".describe","text":"<p>source <pre><code>.describe()\n</code></pre></p> <p>Describe the policy.</p>"},{"location":"api_docs/xploit/policy/off_policy_double_actor_double_critic/#freeze","title":".freeze","text":"<p>source <pre><code>.freeze(\n   encoder: nn.Module, dist: Distribution\n)\n</code></pre></p> <p>Freeze all the elements like <code>encoder</code> and <code>dist</code>.</p> <p>Args</p> <ul> <li>encoder (nn.Module) : Encoder network.</li> <li>dist (Distribution) : Distribution.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/off_policy_double_actor_double_critic/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor, training: bool = True\n)\n</code></pre></p> <p>Sample actions based on observations.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>training (bool) : Training mode, True or False.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/policy/off_policy_double_actor_double_critic/#get_dist","title":".get_dist","text":"<p>source <pre><code>.get_dist(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Get sample distribution.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> </ul> <p>Returns</p> <p>RLLTE distribution.</p>"},{"location":"api_docs/xploit/policy/off_policy_double_actor_double_critic/#save","title":".save","text":"<p>source <pre><code>.save(\n   path: Path, pretraining: bool, global_step: int\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Save path.</li> <li>pretraining (bool) : Pre-training mode.</li> <li>global_step (int) : Global training step.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/off_policy_double_qnetwork/","title":"OffPolicyDoubleQNetwork","text":""},{"location":"api_docs/xploit/policy/off_policy_double_qnetwork/#offpolicydoubleqnetwork","title":"OffPolicyDoubleQNetwork","text":"<p>source <pre><code>OffPolicyDoubleQNetwork(\n   observation_space: gym.Space, action_space: gym.Space, feature_dim: int = 64,\n   hidden_dim: int = 1024, opt_class: Type[th.optim.Optimizer] = th.optim.Adam,\n   opt_kwargs: Optional[Dict[str, Any]] = None, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Q-network for off-policy algortithms like <code>DQN</code>.</p> <p>Structure: self.encoder (shared by actor and critic), self.qnet, self.qnet_target Optimizers: self.opt -&gt; (self.qnet, self.qnet_target)</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>feature_dim (int) : Number of features accepted.</li> <li>hidden_dim (int) : Number of units per hidden layer.</li> <li>opt_class (Type[th.optim.Optimizer]) : Optimizer class.</li> <li>opt_kwargs (Dict[str, Any]) : Optimizer keyword arguments.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>Actor network instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/policy/off_policy_double_qnetwork/#describe","title":".describe","text":"<p>source <pre><code>.describe()\n</code></pre></p> <p>Describe the policy.</p>"},{"location":"api_docs/xploit/policy/off_policy_double_qnetwork/#freeze","title":".freeze","text":"<p>source <pre><code>.freeze(\n   encoder: nn.Module, dist: Distribution\n)\n</code></pre></p> <p>Freeze all the elements like <code>encoder</code> and <code>dist</code>.</p> <p>Args</p> <ul> <li>encoder (nn.Module) : Encoder network.</li> <li>dist (Distribution) : Distribution class.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/off_policy_double_qnetwork/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor, training: bool = True\n)\n</code></pre></p> <p>Sample actions based on observations.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>training (bool) : Training mode, True or False.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/policy/off_policy_double_qnetwork/#save","title":".save","text":"<p>source <pre><code>.save(\n   path: Path, pretraining: bool, global_step: int\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Save path.</li> <li>pretraining (bool) : Pre-training mode.</li> <li>global_step (int) : Global training step.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/off_policy_stoch_actor_double_critic/","title":"OffPolicyStochActorDoubleCritic","text":""},{"location":"api_docs/xploit/policy/off_policy_stoch_actor_double_critic/#offpolicystochactordoublecritic","title":"OffPolicyStochActorDoubleCritic","text":"<p>source <pre><code>OffPolicyStochActorDoubleCritic(\n   observation_space: gym.Space, action_space: gym.Space, feature_dim: int = 64,\n   hidden_dim: int = 1024, opt_class: Type[th.optim.Optimizer] = th.optim.Adam,\n   opt_kwargs: Optional[Dict[str, Any]] = None, log_std_range: Tuple = (-5, 2),\n   init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Stochastic actor network and double critic network for off-policy algortithms like <code>SAC</code>. Here the 'self.dist' refers to an sampling distribution instance.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>feature_dim (int) : Number of features accepted.</li> <li>hidden_dim (int) : Number of units per hidden layer.</li> <li>opt_class (Type[th.optim.Optimizer]) : Optimizer class.</li> <li>opt_kwargs (Dict[str, Any]) : Optimizer keyword arguments.</li> <li>log_std_range (Tuple) : Range of log standard deviation.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>Actor-Critic network.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/policy/off_policy_stoch_actor_double_critic/#describe","title":".describe","text":"<p>source <pre><code>.describe()\n</code></pre></p> <p>Describe the policy.</p>"},{"location":"api_docs/xploit/policy/off_policy_stoch_actor_double_critic/#freeze","title":".freeze","text":"<p>source <pre><code>.freeze(\n   encoder: nn.Module, dist: Distribution\n)\n</code></pre></p> <p>Freeze all the elements like <code>encoder</code> and <code>dist</code>.</p> <p>Args</p> <ul> <li>encoder (nn.Module) : Encoder network.</li> <li>dist (Distribution) : Distribution class.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/off_policy_stoch_actor_double_critic/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor, training: bool = True\n)\n</code></pre></p> <p>Sample actions based on observations.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>training (bool) : Training mode, True or False.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/policy/off_policy_stoch_actor_double_critic/#get_dist","title":".get_dist","text":"<p>source <pre><code>.get_dist(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Get sample distribution.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Action distribution.</p>"},{"location":"api_docs/xploit/policy/off_policy_stoch_actor_double_critic/#save","title":".save","text":"<p>source <pre><code>.save(\n   path: Path, pretraining: bool, global_step: int\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Save path.</li> <li>pretraining (bool) : Pre-training mode.</li> <li>global_step (int) : Global training step.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/on_policy_decoupled_actor_critic/","title":"OnPolicyDecoupledActorCritic","text":""},{"location":"api_docs/xploit/policy/on_policy_decoupled_actor_critic/#onpolicydecoupledactorcritic","title":"OnPolicyDecoupledActorCritic","text":"<p>source <pre><code>OnPolicyDecoupledActorCritic(\n   observation_space: gym.Space, action_space: gym.Space, feature_dim: int,\n   hidden_dim: int = 512, opt_class: Type[th.optim.Optimizer] = th.optim.Adam,\n   opt_kwargs: Optional[Dict[str, Any]] = None, init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Actor-Critic network for on-policy algorithms like <code>DAAC</code>.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>feature_dim (int) : Number of features accepted.</li> <li>hidden_dim (int) : Number of units per hidden layer.</li> <li>opt_class (Type[th.optim.Optimizer]) : Optimizer class.</li> <li>opt_kwargs (Dict[str, Any]) : Optimizer keyword arguments.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>Actor-Critic network instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/policy/on_policy_decoupled_actor_critic/#describe","title":".describe","text":"<p>source <pre><code>.describe()\n</code></pre></p> <p>Describe the policy.</p>"},{"location":"api_docs/xploit/policy/on_policy_decoupled_actor_critic/#freeze","title":".freeze","text":"<p>source <pre><code>.freeze(\n   encoder: nn.Module, dist: Distribution\n)\n</code></pre></p> <p>Freeze all the elements like <code>encoder</code> and <code>dist</code>.</p> <p>Args</p> <ul> <li>encoder (nn.Module) : Encoder network.</li> <li>dist (Distribution) : Distribution class.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/on_policy_decoupled_actor_critic/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor, training: bool = True\n)\n</code></pre></p> <p>Get actions and estimated values for observations.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>training (bool) : training mode, <code>True</code> or <code>False</code>.</li> </ul> <p>Returns</p> <p>Sampled actions, estimated values, and log of probabilities for observations when <code>training</code> is <code>True</code>, else only deterministic actions.</p>"},{"location":"api_docs/xploit/policy/on_policy_decoupled_actor_critic/#get_value","title":".get_value","text":"<p>source <pre><code>.get_value(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Get estimated values for observations.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> </ul> <p>Returns</p> <p>Estimated values.</p>"},{"location":"api_docs/xploit/policy/on_policy_decoupled_actor_critic/#evaluate_actions","title":".evaluate_actions","text":"<p>source <pre><code>.evaluate_actions(\n   obs: th.Tensor, actions: th.Tensor\n)\n</code></pre></p> <p>Evaluate actions according to the current policy given the observations.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Sampled observations.</li> <li>actions (th.Tensor) : Sampled actions.</li> </ul> <p>Returns</p> <p>Estimated values, log of the probability evaluated at <code>actions</code>, entropy of distribution.</p>"},{"location":"api_docs/xploit/policy/on_policy_decoupled_actor_critic/#save","title":".save","text":"<p>source <pre><code>.save(\n   path: Path, pretraining: bool, global_step: int\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Save path.</li> <li>pretraining (bool) : Pre-training mode.</li> <li>global_step (int) : Global training step.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/on_policy_shared_actor_critic/","title":"OnPolicySharedActorCritic","text":""},{"location":"api_docs/xploit/policy/on_policy_shared_actor_critic/#onpolicysharedactorcritic","title":"OnPolicySharedActorCritic","text":"<p>source <pre><code>OnPolicySharedActorCritic(\n   observation_space: gym.Space, action_space: gym.Space, feature_dim: int,\n   hidden_dim: int = 512, opt_class: Type[th.optim.Optimizer] = th.optim.Adam,\n   opt_kwargs: Optional[Dict[str, Any]] = None, aux_critic: bool = False,\n   init_fn: str = 'orthogonal'\n)\n</code></pre></p> <p>Actor-Critic network for on-policy algorithms like <code>PPO</code> and <code>A2C</code>.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>feature_dim (int) : Number of features accepted.</li> <li>hidden_dim (int) : Number of units per hidden layer.</li> <li>opt_class (Type[th.optim.Optimizer]) : Optimizer class.</li> <li>opt_kwargs (Dict[str, Any]) : Optimizer keyword arguments.</li> <li>aux_critic (bool) : Use auxiliary critic or not, for <code>PPG</code> agent.</li> <li>init_fn (str) : Parameters initialization method.</li> </ul> <p>Returns</p> <p>Actor-Critic network instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/policy/on_policy_shared_actor_critic/#describe","title":".describe","text":"<p>source <pre><code>.describe()\n</code></pre></p> <p>Describe the policy.</p>"},{"location":"api_docs/xploit/policy/on_policy_shared_actor_critic/#freeze","title":".freeze","text":"<p>source <pre><code>.freeze(\n   encoder: nn.Module, dist: Distribution\n)\n</code></pre></p> <p>Freeze all the elements like <code>encoder</code> and <code>dist</code>.</p> <p>Args</p> <ul> <li>encoder (nn.Module) : Encoder network.</li> <li>dist (Distribution) : Distribution class.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/policy/on_policy_shared_actor_critic/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   obs: th.Tensor, training: bool = True\n)\n</code></pre></p> <p>Get actions and estimated values for observations.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> <li>training (bool) : training mode, <code>True</code> or <code>False</code>.</li> </ul> <p>Returns</p> <p>Sampled actions, estimated values, and log of probabilities for observations when <code>training</code> is <code>True</code>, else only deterministic actions.</p>"},{"location":"api_docs/xploit/policy/on_policy_shared_actor_critic/#get_value","title":".get_value","text":"<p>source <pre><code>.get_value(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Get estimated values for observations.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Observations.</li> </ul> <p>Returns</p> <p>Estimated values.</p>"},{"location":"api_docs/xploit/policy/on_policy_shared_actor_critic/#evaluate_actions","title":".evaluate_actions","text":"<p>source <pre><code>.evaluate_actions(\n   obs: th.Tensor, actions: th.Tensor\n)\n</code></pre></p> <p>Evaluate actions according to the current policy given the observations.</p> <p>Args</p> <ul> <li>obs (th.Tensor) : Sampled observations.</li> <li>actions (th.Tensor) : Sampled actions.</li> </ul> <p>Returns</p> <p>Estimated values, log of the probability evaluated at <code>actions</code>, entropy of distribution.</p>"},{"location":"api_docs/xploit/policy/on_policy_shared_actor_critic/#get_policy_outputs","title":".get_policy_outputs","text":"<p>source <pre><code>.get_policy_outputs(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Get policy outputs for training.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> </ul> <p>Returns</p> <p>Policy outputs like unnormalized probabilities for <code>Discrete</code> tasks.</p>"},{"location":"api_docs/xploit/policy/on_policy_shared_actor_critic/#get_dist_and_aux_value","title":".get_dist_and_aux_value","text":"<p>source <pre><code>.get_dist_and_aux_value(\n   obs: th.Tensor\n)\n</code></pre></p> <p>Get probs and auxiliary estimated values for auxiliary phase update.</p> <p>Args</p> <ul> <li>obs  : Sampled observations.</li> </ul> <p>Returns</p> <p>Sample distribution, estimated values, auxiliary estimated values.</p>"},{"location":"api_docs/xploit/policy/on_policy_shared_actor_critic/#save","title":".save","text":"<p>source <pre><code>.save(\n   path: Path, pretraining: bool, global_step: int\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Save path.</li> <li>pretraining (bool) : Pre-training mode.</li> <li>global_step (int) : Global training step.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/dict_replay_storage/","title":"DictReplayStorage","text":""},{"location":"api_docs/xploit/storage/dict_replay_storage/#dictreplaystorage","title":"DictReplayStorage","text":"<p>source <pre><code>DictReplayStorage(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   storage_size: int = 1000000, batch_size: int = 1024, num_envs: int = 1\n)\n</code></pre></p> <p>Dict replay storage for off-policy algorithms and dictionary observations.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>device (str) : Device to convert the data.</li> <li>storage_size (int) : The capacity of the storage.</li> <li>batch_size (int) : Batch size of samples.</li> <li>num_envs (int) : The number of parallel environments.</li> </ul> <p>Returns</p> <p>Dict replay storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/dict_replay_storage/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the storage.</p>"},{"location":"api_docs/xploit/storage/dict_replay_storage/#add","title":".add","text":"<p>source <pre><code>.add(\n   observations: Dict[str, th.Tensor], actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, infos: Dict[str, Any],\n   next_observations: Dict[str, th.Tensor]\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>observations (Dict[str, th.Tensor]) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>terminateds (th.Tensor) : Termination flag.</li> <li>truncateds (th.Tensor) : Truncation flag.</li> <li>infos (Dict[str, Any]) : Additional information.</li> <li>next_observations (Dict[str, th.Tensor]) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/dict_replay_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample()\n</code></pre></p> <p>Sample from the storage.</p>"},{"location":"api_docs/xploit/storage/dict_replay_storage/#update","title":".update","text":"<p>source <pre><code>.update(\n   *args, **kwargs\n)\n</code></pre></p> <p>Update the storage if necessary.</p>"},{"location":"api_docs/xploit/storage/dict_rollout_storage/","title":"DictRolloutStorage","text":""},{"location":"api_docs/xploit/storage/dict_rollout_storage/#dictrolloutstorage","title":"DictRolloutStorage","text":"<p>source <pre><code>DictRolloutStorage(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   storage_size: int = 256, batch_size: int = 64, num_envs: int = 8,\n   discount: float = 0.999, gae_lambda: float = 0.95\n)\n</code></pre></p> <p>Dict Rollout storage for on-policy algorithms and dictionary observations.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : The observation space of environment.</li> <li>action_space (gym.Space) : The action space of environment.</li> <li>device (str) : Device to convert the data.</li> <li>storage_size (int) : The capacity of the storage. Here it refers to the length of per rollout.</li> <li>batch_size (int) : Batch size of samples.</li> <li>num_envs (int) : The number of parallel environments.</li> <li>discount (float) : The discount factor.</li> <li>gae_lambda (float) : Weighting coefficient for generalized advantage estimation (GAE).</li> </ul> <p>Returns</p> <p>Dict rollout storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/dict_rollout_storage/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the storage.</p>"},{"location":"api_docs/xploit/storage/dict_rollout_storage/#add","title":".add","text":"<p>source <pre><code>.add(\n   observations: Dict[str, th.Tensor], actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, infos: Dict,\n   next_observations: Dict[str, th.Tensor], log_probs: th.Tensor,\n   values: th.Tensor\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>observations (Dict[str, th.Tensor]) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>terminateds (th.Tensor) : Termination signals.</li> <li>truncateds (th.Tensor) : Truncation signals.</li> <li>infos (Dict) : Extra information.</li> <li>next_observations (Dict[str, th.Tensor]) : Next observations.</li> <li>log_probs (th.Tensor) : Log of the probability evaluated at <code>actions</code>.</li> <li>values (th.Tensor) : Estimated values.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/dict_rollout_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample()\n</code></pre></p> <p>Sample data from storage.</p>"},{"location":"api_docs/xploit/storage/her_replay_storage/","title":"HerReplayStorage","text":""},{"location":"api_docs/xploit/storage/her_replay_storage/#herreplaystorage","title":"HerReplayStorage","text":"<p>source <pre><code>HerReplayStorage(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   storage_size: int = 1000000, num_envs: int = 1, batch_size: int = 1024,\n   goal_selection_strategy: str = 'future', num_goals: int = 4,\n   reward_fn: Callable = lambdax: x, copy_info_dict: bool = False\n)\n</code></pre></p> <p>Hindsight experience replay (HER) storage for off-policy algorithms. Based on: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/her/her_replay_buffer.py</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>device (str) : Device to convert the data.</li> <li>storage_size (int) : The capacity of the storage.</li> <li>num_envs (int) : The number of parallel environments.</li> <li>batch_size (int) : Batch size of samples.</li> <li>goal_selection_strategy (str) : A goal selection strategy of [\"future\", \"final\", \"episode\"].</li> <li>num_goals (int) : The number of goals to sample.</li> <li>reward_fn (Callable) : Function to compute new rewards based on state and goal, whose definition is     same as https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/envs/bit_flipping_env.py#L190 copy_info_dict (bool) whether to copy the info dictionary and pass it to compute_reward() method.</li> </ul> <p>Returns</p> <p>Dict replay storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/her_replay_storage/#add","title":".add","text":"<p>source <pre><code>.add(\n   observations: Dict[str, th.Tensor], actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, infos: Dict[str, Any],\n   next_observations: Dict[str, th.Tensor]\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>observations (Dict[str, th.Tensor]) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>terminateds (th.Tensor) : Termination flag.</li> <li>truncateds (th.Tensor) : Truncation flag.</li> <li>infos (Dict[str, Any]) : Additional information.</li> <li>next_observations (Dict[str, th.Tensor]) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/her_replay_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample()\n</code></pre></p> <p>Sample from the storage.</p>"},{"location":"api_docs/xploit/storage/her_replay_storage/#update","title":".update","text":"<p>source <pre><code>.update(\n   *args, **kwargs\n)\n</code></pre></p> <p>Update the storage if necessary.</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/","title":"NStepReplayStorage","text":""},{"location":"api_docs/xploit/storage/nstep_replay_storage/#nstepreplaystorage","title":"NStepReplayStorage","text":"<p>source <pre><code>NStepReplayStorage(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   storage_size: int = 1000000, num_envs: int = 1, batch_size: int = 256,\n   num_workers: int = 4, pin_memory: bool = True, n_step: int = 3, discount: float = 0.99,\n   fetch_every: int = 1000, save_snapshot: bool = False\n)\n</code></pre></p> <p>N-step replay storage. Implemented based on: https://github.com/facebookresearch/drqv2/blob/main/replay_buffer.py</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>device (str) : Device to convert replay data.</li> <li>storage_size (int) : Max number of element in the storage.</li> <li>num_envs (int) : The number of parallel environments.</li> <li>batch_size (int) : Batch size of samples.</li> <li>num_workers (int) : Subprocesses to use for data loading.</li> <li>pin_memory (bool) : Pin memory or not.</li> <li>nstep (int) : The number of transitions to consider when computing n-step returns</li> <li>discount (float) : The discount factor for future rewards.</li> <li>fetch_every (int) : Loading interval.</li> <li>save_snapshot (bool) : Save loaded file or not.</li> </ul> <p>Returns</p> <p>N-step replay storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the storage.</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/#add","title":".add","text":"<p>source <pre><code>.add(\n   observations: th.Tensor, actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, infos: Dict[str, Any],\n   next_observations: th.Tensor\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>observations (th.Tensor) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>terminateds (th.Tensor) : Termination flag.</li> <li>truncateds (th.Tensor) : Truncation flag.</li> <li>infos (Dict[str, Any]) : Additional information.</li> <li>next_observations (th.Tensor) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/#replay_iter","title":".replay_iter","text":"<p>source <pre><code>.replay_iter()\n</code></pre></p> <p>Create iterable dataloader.</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample()\n</code></pre></p> <p>Sample from the storage.</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/#update","title":".update","text":"<p>source <pre><code>.update(\n   *args\n)\n</code></pre></p> <p>Update the storage if necessary.</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/","title":"PrioritizedReplayStorage","text":""},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#prioritizedreplaystorage","title":"PrioritizedReplayStorage","text":"<p>source <pre><code>PrioritizedReplayStorage(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   storage_size: int = 1000000, batch_size: int = 1024, num_envs: int = 1,\n   alpha: float = 0.6, beta: float = 0.4\n)\n</code></pre></p> <p>Prioritized replay storage with proportional prioritization for off-policy algorithms. Since the storage updates the priorities of the samples based on the TD error, users should include the <code>indices</code> and <code>weights</code> in the returned information of the <code>.update</code> method of the agent. An example is:     return {\"indices\": indices, \"weights\": weights, ..., \"Actor Loss\": actor_loss, ...}</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>device (str) : Device to convert the data.</li> <li>storage_size (int) : The capacity of the storage.</li> <li>num_envs (int) : The number of parallel environments.</li> <li>batch_size (int) : Batch size of samples.</li> <li>alpha (float) : Prioritization value.</li> <li>beta (float) : Importance sampling value.</li> </ul> <p>Returns</p> <p>Prioritized replay storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the storage.</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#annealing_beta","title":".annealing_beta","text":"<p>source <pre><code>.annealing_beta()\n</code></pre></p> <p>Linearly increases beta from the initial value to 1 over global training steps.</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#add","title":".add","text":"<p>source <pre><code>.add(\n   observations: th.Tensor, actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, infos: Dict[str, Any],\n   next_observations: th.Tensor\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>observations (th.Tensor) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>terminateds (th.Tensor) : Termination flag.</li> <li>truncateds (th.Tensor) : Truncation flag.</li> <li>infos (Dict[str, Any]) : Additional information.</li> <li>next_observations (th.Tensor) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample()\n</code></pre></p> <p>Sample from the storage.</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#update","title":".update","text":"<p>source <pre><code>.update(\n   metrics: Dict\n)\n</code></pre></p> <p>Update the priorities.</p> <p>Args</p> <ul> <li>metrics (Dict) : Training metrics from agent to udpate the priorities:     indices (np.ndarray): The indices of current batch data.     priorities (np.ndarray): The priorities of current batch data.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/vanilla_distributed_storage/","title":"VanillaDistributedStorage","text":""},{"location":"api_docs/xploit/storage/vanilla_distributed_storage/#vanilladistributedstorage","title":"VanillaDistributedStorage","text":"<p>source <pre><code>VanillaDistributedStorage(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   storage_size: int = 100, num_storages: int = 80, num_envs: int = 45,\n   batch_size: int = 32\n)\n</code></pre></p> <p>Vanilla distributed storage for distributed algorithms like IMPALA.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : The observation space of environment.</li> <li>action_space (gym.Space) : The action space of environment.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>storage_size (int) : The capacity of the storage. Here it refers to the length of per rollout.</li> <li>num_storages (int) : The number of shared-memory storages.</li> <li>num_envs (int) : The number of parallel environments.</li> <li>batch_size (int) : The batch size.</li> </ul> <p>Returns</p> <p>Vanilla distributed storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/vanilla_distributed_storage/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the storage.</p>"},{"location":"api_docs/xploit/storage/vanilla_distributed_storage/#add","title":".add","text":"<p>source <pre><code>.add(\n   idx: int, timestep: int, actor_output: Dict[str, Any], env_output: Dict[str,\n   Any]\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>idx (int) : The index of storage.</li> <li>timestep (int) : The timestep of rollout.</li> <li>actor_output (Dict) : Actor output.</li> <li>env_output (Dict) : Environment output.</li> </ul> <p>Returns</p> <p>None</p>"},{"location":"api_docs/xploit/storage/vanilla_distributed_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   free_queue: mp.SimpleQueue, full_queue: mp.SimpleQueue, lock = threading.Lock()\n)\n</code></pre></p> <p>Sample transitions from the storage.</p> <p>Args</p> <ul> <li>free_queue (Queue) : Free queue for communication.</li> <li>full_queue (Queue) : Full queue for communication.</li> <li>lock (Lock) : Thread lock.</li> </ul> <p>Returns</p> <p>Batched samples.</p>"},{"location":"api_docs/xploit/storage/vanilla_distributed_storage/#update","title":".update","text":"<p>source <pre><code>.update(\n   *args, **kwargs\n)\n</code></pre></p> <p>Update the storage</p>"},{"location":"api_docs/xploit/storage/vanilla_replay_storage/","title":"VanillaReplayStorage","text":""},{"location":"api_docs/xploit/storage/vanilla_replay_storage/#vanillareplaystorage","title":"VanillaReplayStorage","text":"<p>source <pre><code>VanillaReplayStorage(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   storage_size: int = 1000000, batch_size: int = 1024, num_envs: int = 1\n)\n</code></pre></p> <p>Vanilla replay storage for off-policy algorithms.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : Observation space.</li> <li>action_space (gym.Space) : Action space.</li> <li>device (str) : Device to convert the data.</li> <li>storage_size (int) : The capacity of the storage.</li> <li>num_envs (int) : The number of parallel environments.</li> <li>batch_size (int) : Batch size of samples.</li> </ul> <p>Returns</p> <p>Vanilla replay storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/vanilla_replay_storage/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the storage.</p>"},{"location":"api_docs/xploit/storage/vanilla_replay_storage/#add","title":".add","text":"<p>source <pre><code>.add(\n   observations: th.Tensor, actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, infos: Dict[str, Any],\n   next_observations: th.Tensor\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>observations (th.Tensor) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>terminateds (th.Tensor) : Termination flag.</li> <li>truncateds (th.Tensor) : Truncation flag.</li> <li>infos (Dict[str, Any]) : Additional information.</li> <li>next_observations (th.Tensor) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/vanilla_replay_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample()\n</code></pre></p> <p>Sample from the storage.</p>"},{"location":"api_docs/xploit/storage/vanilla_replay_storage/#update","title":".update","text":"<p>source <pre><code>.update(\n   *args\n)\n</code></pre></p> <p>Update the storage if necessary.</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/","title":"VanillaRolloutStorage","text":""},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#vanillarolloutstorage","title":"VanillaRolloutStorage","text":"<p>source <pre><code>VanillaRolloutStorage(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   storage_size: int = 256, batch_size: int = 64, num_envs: int = 8,\n   discount: float = 0.999, gae_lambda: float = 0.95\n)\n</code></pre></p> <p>Vanilla rollout storage for on-policy algorithms.</p> <p>Args</p> <ul> <li>observation_space (gym.Space) : The observation space of environment.</li> <li>action_space (gym.Space) : The action space of environment.</li> <li>device (str) : Device to convert the data.</li> <li>storage_size (int) : The capacity of the storage. Here it refers to the length of per rollout.</li> <li>batch_size (int) : Batch size of samples.</li> <li>num_envs (int) : The number of parallel environments.</li> <li>discount (float) : The discount factor.</li> <li>gae_lambda (float) : Weighting coefficient for generalized advantage estimation (GAE).</li> </ul> <p>Returns</p> <p>Vanilla rollout storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the storage.</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#add","title":".add","text":"<p>source <pre><code>.add(\n   observations: th.Tensor, actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, infos: Dict,\n   next_observations: th.Tensor, log_probs: th.Tensor, values: th.Tensor\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>observations (th.Tensor) : Observations.</li> <li>actions (th.Tensor) : Actions.</li> <li>rewards (th.Tensor) : Rewards.</li> <li>terminateds (th.Tensor) : Termination signals.</li> <li>truncateds (th.Tensor) : Truncation signals.</li> <li>infos (Dict) : Extra information.</li> <li>next_observations (th.Tensor) : Next observations.</li> <li>log_probs (th.Tensor) : Log of the probability evaluated at <code>actions</code>.</li> <li>values (th.Tensor) : Estimated values.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Update the terminal state of each env.</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#compute_returns_and_advantages","title":".compute_returns_and_advantages","text":"<p>source <pre><code>.compute_returns_and_advantages(\n   last_values: th.Tensor\n)\n</code></pre></p> <p>Perform generalized advantage estimation (GAE).</p> <p>Args</p> <ul> <li>last_values (th.Tensor) : Estimated values of the last step.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample()\n</code></pre></p> <p>Sample data from storage.</p>"},{"location":"api_docs/xplore/augmentation/gaussian_noise/","title":"GaussianNoise","text":""},{"location":"api_docs/xplore/augmentation/gaussian_noise/#gaussiannoise","title":"GaussianNoise","text":"<p>source <pre><code>GaussianNoise(\n   mu: float = 0, sigma: float = 1.0\n)\n</code></pre></p> <p>Gaussian noise operation for processing state-based observations.</p> <p>Args</p> <ul> <li>mu (float or th.Tensor) : mean of the distribution.</li> <li>scale (float or th.Tensor) : standard deviation of the distribution.</li> </ul> <p>Returns</p> <p>Augmented states.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/gaussian_noise/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/grayscale/","title":"GrayScale","text":""},{"location":"api_docs/xplore/augmentation/grayscale/#grayscale","title":"GrayScale","text":"<p>source </p> <p>Grayscale operation for image augmentation.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/grayscale/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/identity/","title":"Identity","text":""},{"location":"api_docs/xplore/augmentation/identity/#identity","title":"Identity","text":"<p>source </p> <p>Identity augmentation.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/identity/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_amplitude_scaling/","title":"RandomAmplitudeScaling","text":""},{"location":"api_docs/xplore/augmentation/random_amplitude_scaling/#randomamplitudescaling","title":"RandomAmplitudeScaling","text":"<p>source <pre><code>RandomAmplitudeScaling(\n   low: float = 0.6, high: float = 1.2\n)\n</code></pre></p> <p>Random amplitude scaling operation for processing state-based observations.</p> <p>Args</p> <ul> <li>low (float) : lower range (inclusive).</li> <li>high (float) : upper range (exclusive).</li> </ul> <p>Returns</p> <p>Augmented states.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_amplitude_scaling/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_colorjitter/","title":"RandomColorJitter","text":""},{"location":"api_docs/xplore/augmentation/random_colorjitter/#randomcolorjitter","title":"RandomColorJitter","text":"<p>source <pre><code>RandomColorJitter(\n   brightness: float = 0.4, contrast: float = 0.4, saturation: float = 0.4,\n   hue: float = 0.5\n)\n</code></pre></p> <p>Random ColorJitter operation for image augmentation.</p> <p>Args</p> <ul> <li>brightness (float) : How much to jitter brightness. Should be non negative numbers.</li> <li>contrast (float) : How much to jitter contrast. Should be non negative numbers.</li> <li>saturation (float) : How much to jitter saturation. Should be non negative numbers.</li> <li>hue (float) : How much to jitter hue. Should have 0&lt;= hue &lt;= 0.5 or -0.5 &lt;= min &lt;= max &lt;= 0.5.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_colorjitter/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_convolution/","title":"RandomConvolution","text":""},{"location":"api_docs/xplore/augmentation/random_convolution/#randomconvolution","title":"RandomConvolution","text":"<p>source </p> <p>Random Convolution operation for image augmentation. Note that imgs should be normalized and torch tensor.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_convolution/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_crop/","title":"RandomCrop","text":""},{"location":"api_docs/xplore/augmentation/random_crop/#randomcrop","title":"RandomCrop","text":"<p>source <pre><code>RandomCrop(\n   pad: int = 4, out: int = 84\n)\n</code></pre></p> <p>Random crop operation for processing image-based observations.</p> <p>Args</p> <ul> <li>pad (int) : Padding size.</li> <li>out (int) : Desired output size.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_crop/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_cutout/","title":"RandomCutout","text":""},{"location":"api_docs/xplore/augmentation/random_cutout/#randomcutout","title":"RandomCutout","text":"<p>source <pre><code>RandomCutout(\n   min_cut: int = 10, max_cut: int = 30\n)\n</code></pre></p> <p>Random Cutout operation for image augmentation.</p> <p>Args</p> <ul> <li>min_cut (int) : Min size of the cut shape.</li> <li>max_cut (int) : Max size of the cut shape.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_cutout/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_cutoutcolor/","title":"RandomCutoutColor","text":""},{"location":"api_docs/xplore/augmentation/random_cutoutcolor/#randomcutoutcolor","title":"RandomCutoutColor","text":"<p>source <pre><code>RandomCutoutColor(\n   min_cut: int = 10, max_cut: int = 30\n)\n</code></pre></p> <p>Random Cutout operation for image augmentation.</p> <p>Args</p> <ul> <li>min_cut (int) : min size of the cut shape.</li> <li>max_cut (int) : max size of the cut shape.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_cutoutcolor/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_flip/","title":"RandomFlip","text":""},{"location":"api_docs/xplore/augmentation/random_flip/#randomflip","title":"RandomFlip","text":"<p>source <pre><code>RandomFlip(\n   p: float = 0.2\n)\n</code></pre></p> <p>Random flip operation for image augmentation.</p> <p>Args</p> <ul> <li>p (float) : The image flip problistily in a batch.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_flip/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_rotate/","title":"RandomRotate","text":""},{"location":"api_docs/xplore/augmentation/random_rotate/#randomrotate","title":"RandomRotate","text":"<p>source <pre><code>RandomRotate(\n   p: float = 0.2\n)\n</code></pre></p> <p>Random rotate operation for processing image-based observations.</p> <p>Args</p> <ul> <li>p (float) : The image rotate problistily in a batch.</li> </ul> <p>Returns</p> <p>Random rotate image in a batch.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_rotate/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_shift/","title":"RandomShift","text":""},{"location":"api_docs/xplore/augmentation/random_shift/#randomshift","title":"RandomShift","text":"<p>source <pre><code>RandomShift(\n   pad: int = 4\n)\n</code></pre></p> <p>Random shift operation for processing image-based observations.</p> <p>Args</p> <ul> <li>pad (int) : Padding size.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_shift/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_translate/","title":"RandomTranslate","text":""},{"location":"api_docs/xplore/augmentation/random_translate/#randomtranslate","title":"RandomTranslate","text":"<p>source <pre><code>RandomTranslate(\n   size: int = 256, scale_factor: float = 0.75\n)\n</code></pre></p> <p>Random translate operation for processing image-based observations.</p> <p>Args</p> <ul> <li>size (int) : The scale size in translated images</li> <li>scale_factor (float) : The scale factor ratio in translated images. Should have 0.0 &lt;= scale_factor &lt;= 1.0</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_translate/#forward","title":".forward","text":"<p>source <pre><code>.forward(\n   x: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/distribution/bernoulli/","title":"Bernoulli","text":""},{"location":"api_docs/xplore/distribution/bernoulli/#bernoulli","title":"Bernoulli","text":"<p>source </p> <p>Bernoulli distribution for sampling actions for 'MultiBinary' tasks.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#probs","title":".probs","text":"<p>source <pre><code>.probs()\n</code></pre></p> <p>Return probabilities.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#logits","title":".logits","text":"<p>source <pre><code>.logits()\n</code></pre></p> <p>Returns the unnormalized log probabilities.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (th.Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\n   actions: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at actions.</p> <p>Args</p> <ul> <li>actions (th.Tensor) : The actions to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/categorical/","title":"Categorical","text":""},{"location":"api_docs/xplore/distribution/categorical/#categorical","title":"Categorical","text":"<p>source </p> <p>Categorical distribution for sampling actions for 'Discrete' tasks.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/categorical/#probs","title":".probs","text":"<p>source <pre><code>.probs()\n</code></pre></p> <p>Return probabilities.</p>"},{"location":"api_docs/xplore/distribution/categorical/#logits","title":".logits","text":"<p>source <pre><code>.logits()\n</code></pre></p> <p>Returns the unnormalized log probabilities.</p>"},{"location":"api_docs/xplore/distribution/categorical/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (th.Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/categorical/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\n   actions: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at actions.</p> <p>Args</p> <ul> <li>actions (th.Tensor) : The actions to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/categorical/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/categorical/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/categorical/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/","title":"DiagonalGaussian","text":""},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#diagonalgaussian","title":"DiagonalGaussian","text":"<p>source </p> <p>Diagonal Gaussian distribution for 'Box' tasks.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (th.Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#rsample","title":".rsample","text":"<p>source <pre><code>.rsample(\n   sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (th.Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#stddev","title":".stddev","text":"<p>source <pre><code>.stddev()\n</code></pre></p> <p>Returns the standard deviation of the distribution.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#variance","title":".variance","text":"<p>source <pre><code>.variance()\n</code></pre></p> <p>Returns the variance of the distribution.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\n   actions: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at actions.</p> <p>Args</p> <ul> <li>actions (th.Tensor) : The actions to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/multi_categorical/","title":"MultiCategorical","text":""},{"location":"api_docs/xplore/distribution/multi_categorical/#multicategorical","title":"MultiCategorical","text":"<p>source </p> <p>Multi-categorical distribution for sampling actions for 'MultiDiscrete' tasks.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/multi_categorical/#probs","title":".probs","text":"<p>source <pre><code>.probs()\n</code></pre></p> <p>Return probabilities.</p>"},{"location":"api_docs/xplore/distribution/multi_categorical/#logits","title":".logits","text":"<p>source <pre><code>.logits()\n</code></pre></p> <p>Returns the unnormalized log probabilities.</p>"},{"location":"api_docs/xplore/distribution/multi_categorical/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (th.Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/multi_categorical/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\n   actions: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at actions.</p> <p>Args</p> <ul> <li>actions (th.Tensor) : The actions to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/multi_categorical/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/multi_categorical/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/multi_categorical/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/","title":"NormalNoise","text":""},{"location":"api_docs/xplore/distribution/normal_noise/#normalnoise","title":"NormalNoise","text":"<p>source <pre><code>NormalNoise(\n   mu: Union[float, th.Tensor] = 0.0, sigma: Union[float, th.Tensor] = 1.0,\n   low: float = -1.0, high: float = 1.0, eps: float = 1e-06\n)\n</code></pre></p> <p>Gaussian action noise.</p> <p>Args</p> <ul> <li>mu (Union[float, th.Tensor]) : Mean of the noise.</li> <li>sigma (Union[float, th.Tensor]) : Standard deviation of the noise.</li> <li>low (float) : The lower bound of the noise.</li> <li>high (float) : The upper bound of the noise.</li> <li>eps (float) : A small value to avoid numerical instability.</li> </ul> <p>Returns</p> <p>Gaussian action noise instance.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   clip: Optional[float] = None, sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>clip (Optional[float]) : The clip range of the sampled noises.</li> <li>sample_shape (th.Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/","title":"OrnsteinUhlenbeckNoise","text":""},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#ornsteinuhlenbecknoise","title":"OrnsteinUhlenbeckNoise","text":"<p>source <pre><code>OrnsteinUhlenbeckNoise(\n   mu: Union[float, th.Tensor] = 0.0, sigma: Union[float, th.Tensor] = 1.0,\n   low: float = -1.0, high: float = 1.0, eps: float = 1e-06, theta: float = 0.15,\n   dt: float = 0.01\n)\n</code></pre></p> <p>Ornstein Uhlenbeck action noise. Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab</p> <p>Args</p> <ul> <li>mu (Union[float, th.Tensor]) : Mean of the noise.</li> <li>sigma (Union[float, th.Tensor]) : Standard deviation of the noise.</li> <li>low (float) : The lower bound of the noise.</li> <li>high (float) : The upper bound of the noise.</li> <li>eps (float) : A small value to avoid numerical instability.</li> <li>theta (float) : The rate of mean reversion.</li> <li>dt (float) : Timestep for the noise.</li> <li>stddev_schedule (str) : Use the exploration std schedule.</li> <li>stddev_clip (float) : The exploration std clip range.</li> </ul> <p>Returns</p> <p>Ornstein-Uhlenbeck noise instance.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   clip: Optional[float] = None, sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>clip (Optional[float]) : The clip range of the sampled noises.</li> <li>sample_shape (th.Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the noise.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/","title":"SquashedNormal","text":""},{"location":"api_docs/xplore/distribution/squashed_normal/#squashednormal","title":"SquashedNormal","text":"<p>source </p> <p>Squashed normal distribution for <code>Box</code> tasks.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (th.Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#rsample","title":".rsample","text":"<p>source <pre><code>.rsample(\n   sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (th.Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Return the transformed mean.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\n   actions: th.Tensor\n)\n</code></pre></p> <p>Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.</p> <p>Args</p> <ul> <li>actions (th.Tensor) : The actions to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/","title":"TruncatedNormalNoise","text":""},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#truncatednormalnoise","title":"TruncatedNormalNoise","text":"<p>source <pre><code>TruncatedNormalNoise(\n   mu: Union[float, th.Tensor] = 0.0, sigma: Union[float, th.Tensor] = 1.0,\n   low: float = -1.0, high: float = 1.0, eps: float = 1e-06,\n   stddev_schedule: str = 'linear(1.0, 0.1, 100000)'\n)\n</code></pre></p> <p>Truncated normal action noise. See Section 3.1 of \"Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning\".</p> <p>Args</p> <ul> <li>mu (Union[float, th.Tensor]) : Mean of the noise.</li> <li>sigma (Union[float, th.Tensor]) : Standard deviation of the noise.</li> <li>low (float) : The lower bound of the noise.</li> <li>high (float) : The upper bound of the noise.</li> <li>eps (float) : A small value to avoid numerical instability.</li> <li>stddev_schedule (str) : Use the exploration std schedule, available options are:     <code>linear(init, final, duration)</code> and <code>step_linear(init, final1, duration1, final2, duration2)</code>.</li> </ul> <p>Returns</p> <p>Truncated normal noise instance.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n   clip: Optional[float] = None, sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>clip (Optional[float]) : The clip range of the sampled noises.</li> <li>sample_shape (th.Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/reward/girm/","title":"GIRM","text":""},{"location":"api_docs/xplore/reward/girm/#girm","title":"GIRM","text":"<p>source <pre><code>GIRM(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   beta: float = 0.05, kappa: float = 2.5e-05, latent_dim: int = 128, lr: float = 0.001,\n   batch_size: int = 64, lambd: float = 0.5, lambd_recon: float = 1.0,\n   lambd_action: float = 1.0, kld_loss_beta: float = 1.0\n)\n</code></pre></p> <p>Intrinsic Reward Driven Imitation Learning via Generative Model (GIRM). See paper: http://proceedings.mlr.press/v119/yu20d/yu20d.pdf</p> <p>Args</p> <ul> <li>observation_space (Space) : The observation space of environment.</li> <li>action_space (Space) : The action space of environment.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for update.</li> <li>lambd (float) : The weighting coefficient for combining actions.</li> <li>lambd_recon (float) : Weighting coefficient of the reconstruction loss.</li> <li>lambd_action (float) : Weighting coefficient of the action loss.</li> <li>kld_loss_beta (float) : Weighting coefficient of the divergence loss.</li> </ul> <p>Returns</p> <p>Instance of GIRM.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/girm/#get_vae_loss","title":".get_vae_loss","text":"<p>source <pre><code>.get_vae_loss(\n   recon_x: th.Tensor, x: th.Tensor, mean: th.Tensor, logvar: th.Tensor\n)\n</code></pre></p> <p>Compute the vae loss.</p> <p>Args</p> <ul> <li>recon_x (th.Tensor) : Reconstructed x.</li> <li>x (th.Tensor) : Input x.</li> <li>mean (th.Tensor) : Sample mean.</li> <li>logvar (th.Tensor) : Log of the sample variance.</li> </ul> <p>Returns</p> <p>Loss values.</p>"},{"location":"api_docs/xplore/reward/girm/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\n   samples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/girm/#add","title":".add","text":"<p>source <pre><code>.add(\n   samples: Dict\n)\n</code></pre></p> <p>Add new samples to the intrinsic reward module.</p>"},{"location":"api_docs/xplore/reward/girm/#update","title":".update","text":"<p>source <pre><code>.update(\n   samples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/icm/","title":"ICM","text":""},{"location":"api_docs/xplore/reward/icm/#icm","title":"ICM","text":"<p>source <pre><code>ICM(\n   envs: VectorEnv, device: str = 'cpu', beta: float = 1.0, kappa: float = 0.0,\n   gamma: Optional[float] = None, rwd_norm_type: str = 'rms',\n   obs_norm_type: str = 'none', latent_dim: int = 128, lr: float = 0.001,\n   batch_size: int = 256, update_proportion: float = 1.0, encoder_model: str = 'mnih',\n   weight_init: str = 'orthogonal'\n)\n</code></pre></p> <p>Curiosity-driven Exploration by Self-supervised Prediction. See paper: http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf</p> <p>Args</p> <ul> <li>envs (VectorEnv) : The vectorized environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate of the weighting coefficient.</li> <li>gamma (Optional[float]) : Intrinsic reward discount rate, default is <code>None</code>.</li> <li>rwd_norm_type (str) : Normalization type for intrinsic rewards from ['rms', 'minmax', 'none'].</li> <li>obs_norm_type (str) : Normalization type for observations data from ['rms', 'none'].</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for training.</li> <li>update_proportion (float) : The proportion of the training data used for updating the forward dynamics models.</li> <li>encoder_model (str) : The network architecture of the encoder from ['mnih', 'pathak'].</li> <li>weight_init (str) : The weight initialization method from ['default', 'orthogonal'].</li> </ul> <p>Returns</p> <p>Instance of ICM.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/icm/#compute","title":".compute","text":"<p>source <pre><code>.compute(\n   samples: Dict[str, th.Tensor], sync: bool = True\n)\n</code></pre></p> <p>Compute the rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples. A python dict consists of multiple tensors,     whose keys are ['observations', 'actions', 'rewards', 'terminateds', 'truncateds', 'next_observations'].     For example, the data shape of 'observations' is (n_steps, n_envs, *obs_shape).</li> <li>sync (bool) : Whether to update the reward module after the <code>compute</code> function, default is <code>True</code>.</li> </ul> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/icm/#update","title":".update","text":"<p>source <pre><code>.update(\n   samples: Dict[str, th.Tensor]\n)\n</code></pre></p> <p>Update the reward module if necessary.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples same as the <code>compute</code> function.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xplore/reward/ngu/","title":"NGU","text":""},{"location":"api_docs/xplore/reward/ngu/#ngu","title":"NGU","text":"<p>source <pre><code>NGU(\n   envs: VectorEnv, device: str = 'cpu', beta: float = 1.0, kappa: float = 0.0,\n   gamma: float = None, rwd_norm_type: str = 'rms', obs_norm_type: str = 'rms',\n   latent_dim: int = 32, lr: float = 0.001, batch_size: int = 256, k: int = 10,\n   kernel_cluster_distance: float = 0.008, kernel_epsilon: float = 0.0001,\n   c: float = 0.001, sm: float = 8.0, mrs: float = 5.0, update_proportion: float = 1.0,\n   encoder_model: str = 'mnih', weight_init: str = 'default'\n)\n</code></pre></p> <p>Never Give Up: Learning Directed Exploration Strategies (NGU). See paper: https://arxiv.org/pdf/2002.06038</p> <p>Args</p> <ul> <li>envs (VectorEnv) : The vectorized environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate of the weighting coefficient.</li> <li>gamma (Optional[float]) : Intrinsic reward discount rate, default is <code>None</code>.</li> <li>rwd_norm_type (str) : Normalization type for intrinsic rewards from ['rms', 'minmax', 'none'].</li> <li>obs_norm_type (str) : Normalization type for observations data from ['rms', 'none'].</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for update.</li> <li>k (int) : Number of neighbors.</li> <li>kernel_cluster_distance (float) : The kernel cluster distance.</li> <li>kernel_epsilon (float) : The kernel constant.</li> <li>c (float) : The pseudo-counts constant.</li> <li>sm (float) : The kernel maximum similarity.</li> <li>mrs (float) : The maximum reward scaling.</li> <li>update_proportion (float) : The proportion of the training data used for updating the forward dynamics models.</li> </ul> <p>Returns</p> <p>Instance of NGU.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/ngu/#compute","title":".compute","text":"<p>source <pre><code>.compute(\n   samples: Dict[str, th.Tensor], sync: bool = True\n)\n</code></pre></p> <p>Compute the rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples. A python dict consists of multiple tensors,     whose keys are ['observations', 'actions', 'rewards', 'terminateds', 'truncateds', 'next_observations'].     For example, the data shape of 'observations' is (n_steps, n_envs, *obs_shape).</li> <li>sync (bool) : Whether to update the reward module after the <code>compute</code> function, default is <code>True</code>.</li> </ul> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/pseudo_counts/","title":"PseudoCounts","text":""},{"location":"api_docs/xplore/reward/pseudo_counts/#pseudocounts","title":"PseudoCounts","text":"<p>source <pre><code>PseudoCounts(\n   envs: VectorEnv, device: str = 'cpu', beta: float = 1.0, kappa: float = 0.0,\n   gamma: float = None, rwd_norm_type: str = 'rms', obs_norm_type: str = 'none',\n   latent_dim: int = 32, lr: float = 0.001, batch_size: int = 256, k: int = 10,\n   kernel_cluster_distance: float = 0.008, kernel_epsilon: float = 0.0001,\n   c: float = 0.001, sm: float = 8.0, update_proportion: float = 1.0,\n   encoder_model: str = 'mnih', weight_init: str = 'orthogonal'\n)\n</code></pre></p> <p>Pseudo-counts based on \"Never Give Up: Learning Directed Exploration Strategies (NGU)\". See paper: https://arxiv.org/pdf/2002.06038</p> <p>Args</p> <ul> <li>envs (VectorEnv) : The vectorized environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate of the weighting coefficient.</li> <li>gamma (Optional[float]) : Intrinsic reward discount rate, default is <code>None</code>.</li> <li>rwd_norm_type (str) : Normalization type for intrinsic rewards from ['rms', 'minmax', 'none'].</li> <li>obs_norm_type (str) : Normalization type for observations data from ['rms', 'none'].</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for update.</li> <li>k (int) : Number of neighbors.</li> <li>kernel_cluster_distance (float) : The kernel cluster distance.</li> <li>kernel_epsilon (float) : The kernel constant.</li> <li>c (float) : The pseudo-counts constant.</li> <li>sm (float) : The kernel maximum similarity.</li> <li>update_proportion (float) : The proportion of the training data used for updating the forward dynamics models.</li> <li>encoder_model (str) : The network architecture of the encoder from ['mnih', 'pathak'].</li> <li>weight_init (str) : The weight initialization method from ['default', 'orthogonal'].</li> </ul> <p>Returns</p> <p>Instance of PseudoCounts.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/pseudo_counts/#watch","title":".watch","text":"<p>source <pre><code>.watch(\n   observations: th.Tensor, actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, next_observations: th.Tensor\n)\n</code></pre></p> <p>Watch the interaction processes and obtain necessary elements for reward computation.</p> <p>Args</p> <ul> <li>observations (th.Tensor) : Observations data with shape (n_envs, *obs_shape).</li> <li>actions (th.Tensor) : Actions data with shape (n_envs, *action_shape).</li> <li>rewards (th.Tensor) : Extrinsic rewards data with shape (n_envs).</li> <li>terminateds (th.Tensor) : Termination signals with shape (n_envs).</li> <li>truncateds (th.Tensor) : Truncation signals with shape (n_envs).</li> <li>next_observations (th.Tensor) : Next observations data with shape (n_envs, *obs_shape).</li> </ul> <p>Returns</p> <p>Feedbacks for the current samples.</p>"},{"location":"api_docs/xplore/reward/pseudo_counts/#pseudo_counts","title":".pseudo_counts","text":"<p>source <pre><code>.pseudo_counts(\n   embeddings: th.Tensor, memory: List[th.Tensor]\n)\n</code></pre></p> <p>Pseudo counts.</p> <p>Args</p> <ul> <li>embeddings (th.Tensor) : Encoded observations.</li> <li>memory (List[th.Tensor]) : Episodic memory.</li> </ul> <p>Returns</p> <p>Conut values.</p>"},{"location":"api_docs/xplore/reward/pseudo_counts/#compute","title":".compute","text":"<p>source <pre><code>.compute(\n   samples: Dict[str, th.Tensor], sync: bool = True\n)\n</code></pre></p> <p>Compute the rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples. A python dict consists of multiple tensors,     whose keys are ['observations', 'actions', 'rewards', 'terminateds', 'truncateds', 'next_observations'].     For example, the data shape of 'observations' is (n_steps, n_envs, *obs_shape).</li> <li>sync (bool) : Whether to update the reward module after the <code>compute</code> function, default is <code>True</code>.</li> </ul> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/pseudo_counts/#update","title":".update","text":"<p>source <pre><code>.update(\n   samples: Dict[str, th.Tensor]\n)\n</code></pre></p> <p>Update the reward module if necessary.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples same as the <code>compute</code> function.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xplore/reward/re3/","title":"RE3","text":""},{"location":"api_docs/xplore/reward/re3/#re3","title":"RE3","text":"<p>source <pre><code>RE3(\n   envs: VectorEnv, device: str = 'cpu', beta: float = 1.0, kappa: float = 0.0,\n   gamma: float = None, rwd_norm_type: str = 'rms', obs_norm_type: str = 'rms',\n   latent_dim: int = 128, storage_size: int = 1000, k: int = 5,\n   average_entropy: bool = False, encoder_model: str = 'mnih',\n   weight_init: str = 'orthogonal'\n)\n</code></pre></p> <p>State Entropy Maximization with Random Encoders for Efficient Exploration (RE3). See paper: http://proceedings.mlr.press/v139/seo21a/seo21a.pdf</p> <p>Args</p> <ul> <li>envs (VectorEnv) : The vectorized environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate of the weighting coefficient.</li> <li>gamma (Optional[float]) : Intrinsic reward discount rate, default is <code>None</code>.</li> <li>rwd_norm_type (str) : Normalization type for intrinsic rewards from ['rms', 'minmax', 'none'].</li> <li>obs_norm_type (str) : Normalization type for observations data from ['rms', 'none'].</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>storage_size (int) : The size of the storage for random embeddings.</li> <li>k (int) : Use the k-th neighbors.</li> <li>average_entropy (bool) : Use the average of entropy estimation.</li> <li>encoder_model (str) : The network architecture of the encoder from ['mnih', 'pathak'].</li> <li>weight_init (str) : The weight initialization method from ['default', 'orthogonal'].</li> </ul> <p>Returns</p> <p>Instance of RE3.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/re3/#watch","title":".watch","text":"<p>source <pre><code>.watch(\n   observations: th.Tensor, actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, next_observations: th.Tensor\n)\n</code></pre></p> <p>Watch the interaction processes and obtain necessary elements for reward computation.</p> <p>Args</p> <ul> <li>observations (th.Tensor) : Observations data with shape (n_envs, *obs_shape).</li> <li>actions (th.Tensor) : Actions data with shape (n_envs, *action_shape).</li> <li>rewards (th.Tensor) : Extrinsic rewards data with shape (n_envs).</li> <li>terminateds (th.Tensor) : Termination signals with shape (n_envs).</li> <li>truncateds (th.Tensor) : Truncation signals with shape (n_envs).</li> <li>next_observations (th.Tensor) : Next observations data with shape (n_envs, *obs_shape).</li> </ul> <p>Returns</p> <p>Feedbacks for the current samples.</p>"},{"location":"api_docs/xplore/reward/re3/#compute","title":".compute","text":"<p>source <pre><code>.compute(\n   samples: Dict[str, th.Tensor], sync: bool = True\n)\n</code></pre></p> <p>Compute the rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples. A python dict consists of multiple tensors,     whose keys are ['observations', 'actions', 'rewards', 'terminateds', 'truncateds', 'next_observations'].     For example, the data shape of 'observations' is (n_steps, n_envs, *obs_shape).</li> <li>sync (bool) : Whether to update the reward module after the <code>compute</code> function, default is <code>True</code>.</li> </ul> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/re3/#update","title":".update","text":"<p>source <pre><code>.update(\n   samples: Dict[str, th.Tensor]\n)\n</code></pre></p> <p>Update the reward module if necessary.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples same as the <code>compute</code> function.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xplore/reward/revd/","title":"REVD","text":""},{"location":"api_docs/xplore/reward/revd/#revd","title":"REVD","text":"<p>source <pre><code>REVD(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   beta: float = 0.05, kappa: float = 2.5e-05, latent_dim: int = 128, alpha: float = 0.5,\n   k: int = 5, average_divergence: bool = False\n)\n</code></pre></p> <p>Rewarding Episodic Visitation Discrepancy for Exploration in Reinforcement Learning (REVD). See paper: https://openreview.net/pdf?id=V2pw1VYMrDo</p> <p>Args</p> <ul> <li>observation_space (Space) : The observation space of environment.</li> <li>action_space (Space) : The action space of environment.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>alpha (alpha) : The order of R\u00e9nyi divergence.</li> <li>k (int) : Use the k-th neighbors.</li> <li>average_divergence (bool) : Use the average of divergence estimation.</li> </ul> <p>Returns</p> <p>Instance of REVD.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/revd/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\n   samples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/revd/#add","title":".add","text":"<p>source <pre><code>.add(\n   samples: Dict\n)\n</code></pre></p> <p>Add new samples to the intrinsic reward module.</p>"},{"location":"api_docs/xplore/reward/revd/#update","title":".update","text":"<p>source <pre><code>.update(\n   samples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/ride/","title":"RIDE","text":""},{"location":"api_docs/xplore/reward/ride/#ride","title":"RIDE","text":"<p>source <pre><code>RIDE(\n   envs: VectorEnv, device: str = 'cpu', beta: float = 1.0, kappa: float = 0.0,\n   gamma: float = None, rwd_norm_type: str = 'rms', obs_norm_type: str = 'none',\n   latent_dim: int = 128, lr: float = 0.001, batch_size: int = 256, k: int = 10,\n   kernel_cluster_distance: float = 0.008, kernel_epsilon: float = 0.0001,\n   c: float = 0.001, sm: float = 8.0, update_proportion: float = 1.0,\n   encoder_model: str = 'mnih', weight_init: str = 'orthogonal'\n)\n</code></pre></p> <p>RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments. See paper: https://arxiv.org/pdf/2002.12292</p> <p>Args</p> <ul> <li>envs (VectorEnv) : The vectorized environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate of the weighting coefficient.</li> <li>gamma (Optional[float]) : Intrinsic reward discount rate, default is <code>None</code>.</li> <li>rwd_norm_type (str) : Normalization type for intrinsic rewards from ['rms', 'minmax', 'none'].</li> <li>obs_norm_type (str) : Normalization type for observations data from ['rms', 'none'].</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for training.</li> <li>k (int) : Number of neighbors.</li> <li>kernel_cluster_distance (float) : The kernel cluster distance.</li> <li>kernel_epsilon (float) : The kernel constant.</li> <li>c (float) : The pseudo-counts constant.</li> <li>sm (float) : The kernel maximum similarity.</li> <li>update_proportion (float) : The proportion of the training data used for updating the forward dynamics models.</li> <li>encoder_model (str) : The network architecture of the encoder from ['mnih', 'pathak'].</li> <li>weight_init (str) : The weight initialization method from ['default', 'orthogonal'].</li> </ul> <p>Returns</p> <p>Instance of RIDE.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/ride/#pseudo_counts","title":".pseudo_counts","text":"<p>source <pre><code>.pseudo_counts(\n   embeddings: th.Tensor, memory: List[th.Tensor]\n)\n</code></pre></p> <p>Pseudo counts.</p> <p>Args</p> <ul> <li>embeddings (th.Tensor) : Encoded observations.</li> <li>memory (List[th.Tensor]) : Episodic memory.</li> </ul> <p>Returns</p> <p>Conut values.</p>"},{"location":"api_docs/xplore/reward/ride/#watch","title":".watch","text":"<p>source <pre><code>.watch(\n   observations: th.Tensor, actions: th.Tensor, rewards: th.Tensor,\n   terminateds: th.Tensor, truncateds: th.Tensor, next_observations: th.Tensor\n)\n</code></pre></p> <p>Watch the interaction processes and obtain necessary elements for reward computation.</p> <p>Args</p> <ul> <li>observations (th.Tensor) : Observations data with shape (n_envs, *obs_shape).</li> <li>actions (th.Tensor) : Actions data with shape (n_envs, *action_shape).</li> <li>rewards (th.Tensor) : Extrinsic rewards data with shape (n_envs).</li> <li>terminateds (th.Tensor) : Termination signals with shape (n_envs).</li> <li>truncateds (th.Tensor) : Truncation signals with shape (n_envs).</li> <li>next_observations (th.Tensor) : Next observations data with shape (n_envs, *obs_shape).</li> </ul> <p>Returns</p> <p>Feedbacks for the current samples.</p>"},{"location":"api_docs/xplore/reward/ride/#compute","title":".compute","text":"<p>source <pre><code>.compute(\n   samples: Dict[str, th.Tensor], sync: bool = True\n)\n</code></pre></p> <p>Compute the rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples. A python dict consists of multiple tensors,     whose keys are ['observations', 'actions', 'rewards', 'terminateds', 'truncateds', 'next_observations'].     For example, the data shape of 'observations' is (n_steps, n_envs, *obs_shape).</li> <li>sync (bool) : Whether to update the reward module after the <code>compute</code> function, default is <code>True</code>.</li> </ul> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/ride/#update","title":".update","text":"<p>source <pre><code>.update(\n   samples: Dict[str, th.Tensor]\n)\n</code></pre></p> <p>Update the reward module if necessary.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples same as the <code>compute</code> function.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xplore/reward/rise/","title":"RISE","text":""},{"location":"api_docs/xplore/reward/rise/#rise","title":"RISE","text":"<p>source <pre><code>RISE(\n   observation_space: gym.Space, action_space: gym.Space, device: str = 'cpu',\n   beta: float = 0.05, kappa: float = 2.5e-05, latent_dim: int = 128,\n   storage_size: int = 10000, num_envs: int = 1, alpha: float = 0.5, k: int = 5,\n   average_entropy: bool = False\n)\n</code></pre></p> <p>R\u00e9nyi State Entropy Maximization for Exploration Acceleration in Reinforcement Learning (RISE). See paper: https://ieeexplore.ieee.org/abstract/document/9802917/</p> <p>Args</p> <ul> <li>observation_space (Space) : The observation space of environment.</li> <li>action_space (Space) : The action space of environment.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>storage_size (int) : The size of the storage for random embeddings.</li> <li>num_envs (int) : The number of parallel environments.</li> <li>alpha (alpha) : The The order of R\u00e9nyi entropy.</li> <li>k (int) : Use the k-th neighbors.</li> <li>average_entropy (bool) : Use the average of entropy estimation.</li> </ul> <p>Returns</p> <p>Instance of RISE.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/rise/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\n   samples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/rise/#update","title":".update","text":"<p>source <pre><code>.update(\n   samples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/rise/#add","title":".add","text":"<p>source <pre><code>.add(\n   samples: Dict\n)\n</code></pre></p> <p>Calculate the random embeddings and insert them into the storage.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/rnd/","title":"RND","text":""},{"location":"api_docs/xplore/reward/rnd/#rnd","title":"RND","text":"<p>source <pre><code>RND(\n   envs: VectorEnv, device: str = 'cpu', beta: float = 1.0, kappa: float = 0.0,\n   gamma: Optional[float] = None, rwd_norm_type: str = 'rms', obs_norm_type: str = 'rms',\n   latent_dim: int = 128, lr: float = 0.001, batch_size: int = 256,\n   update_proportion: float = 1.0, encoder_model: str = 'mnih',\n   weight_init: str = 'orthogonal'\n)\n</code></pre></p> <p>Exploration by Random Network Distillation (RND). See paper: https://arxiv.org/pdf/1810.12894.pdf</p> <p>Args</p> <ul> <li>envs (VectorEnv) : The vectorized environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate of the weighting coefficient.</li> <li>gamma (Optional[float]) : Intrinsic reward discount rate, default is <code>None</code>.</li> <li>rwd_norm_type (str) : Normalization type for intrinsic rewards from ['rms', 'minmax', 'none'].</li> <li>obs_norm_type (str) : Normalization type for observations data from ['rms', 'none'].</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for training.</li> <li>update_proportion (float) : The proportion of the training data used for updating the forward dynamics models.</li> <li>encoder_model (str) : The network architecture of the encoder from ['mnih', 'pathak'].</li> <li>weight_init (str) : The weight initialization method from ['default', 'orthogonal'].</li> </ul> <p>Returns</p> <p>Instance of RND.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/rnd/#compute","title":".compute","text":"<p>source <pre><code>.compute(\n   samples: Dict[str, th.Tensor], sync: bool = True\n)\n</code></pre></p> <p>Compute the rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples. A python dict consists of multiple tensors,     whose keys are ['observations', 'actions', 'rewards', 'terminateds', 'truncateds', 'next_observations'].     For example, the data shape of 'observations' is (n_steps, n_envs, *obs_shape).</li> <li>sync (bool) : Whether to update the reward module after the <code>compute</code> function, default is <code>True</code>.</li> </ul> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/rnd/#update","title":".update","text":"<p>source <pre><code>.update(\n   samples: Dict[str, th.Tensor]\n)\n</code></pre></p> <p>Update the reward module if necessary.</p> <p>Args</p> <ul> <li>samples (Dict[str, th.Tensor]) : The collected samples same as the <code>compute</code> function.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"tutorials/","title":"WELCOME TO RLLTE TUTORIALS \ud83d\udc4b\ud83d\udc4b\ud83d\udc4b","text":""},{"location":"tutorials/#general","title":"General","text":"<ul> <li>RL Algorithm Decoupling</li> <li>Fast Algorithm Development</li> </ul>"},{"location":"tutorials/#model-training","title":"Model Training","text":"<ul> <li>Quick Start</li> <li>Module Replacement for An Implemented Algorithm</li> <li>Intrinsic Reward Shaping for Enhancing Exploration</li> <li>Observation Augmentation for Sample Efficiency and Generalization</li> <li>Pre-training with Intrinsic Rewards</li> </ul>"},{"location":"tutorials/#model-evaluation","title":"Model Evaluation","text":"<ul> <li>Performance Evaluation of Single Algorithm</li> <li>Performance Comparison of Multiple Algorithms</li> <li>Metrics Visualization</li> </ul>"},{"location":"tutorials/#model-deployment","title":"Model Deployment","text":"<ul> <li>with NVIDIA TensorRT</li> <li>with HUAWEI CANN</li> </ul>"},{"location":"tutorials/#customization","title":"Customization","text":"<ul> <li>Make A Custom Environment</li> <li>Make A Custom Module</li> </ul>"},{"location":"tutorials/custom/environment/","title":"Custom Environment","text":"Open in Colab   View on GitHub"},{"location":"tutorials/custom/environment/#environment-definition","title":"Environment definition","text":"<p>To use custom environments in RLLTE, it suffices to follow the gymnasium interface and prepare your environment following Tutorials: Make Your Own Custom Environment. A example is: example.py<pre><code>import gymnasium as gym\nimport numpy as np\n\nclass CustomEnv(gym.Env):\n    def __init__(self, total_length) -&gt; None:\n        super().__init__()\n        self.observation_space = gym.spaces.Box(\n            shape=(9, 84, 84),\n            high=255.0,\n            low=0.,\n            dtype=np.uint8\n        )\n        self.action_space = gym.spaces.Box(\n            shape=(7,),\n            high=1.,\n            low=-1.,\n            dtype=np.float32\n        )\n        self.total_length = total_length\n        self.count = 0\n\n    def step(self, action):\n        obs = self.observation_space.sample()\n        reward = np.random.rand()\n        if self.count &lt; self.total_length:\n            terminated = truncated = False\n        else:\n            terminated = truncated = True\n        info = {\"discount\": 0.99}\n        self.count += 1\n\n        return obs, reward, terminated, truncated, info\n\n    def reset(self, seed=None, options=None):\n        self.count = 0\n        return self.observation_space.sample(), {\"discount\": 0.99}\n</code></pre></p>"},{"location":"tutorials/custom/environment/#use-make_rllte_env","title":"Use <code>make_rllte_env</code>","text":"<p>In RLLTE, the environments are assumed to be vectorized and a <code>make_rllte_env</code> function is used to warp the environments: example.py<pre><code>from rllte.env.utils import make_rllte_env\n# create vectorized environments\nenv = make_rllte_env(env_id=CustomEnv, \n                     device=device, \n                     env_kwargs={'total_length': 499} # set env arguments\n                     )\n</code></pre> After that, you can use the custom environment in application directly. train.py<pre><code>from rllte.agent import DrQv2\nfrom rllte.env.utils import make_rllte_env\n\nif __name__ == \"__main__\":\n    # env setup\n    device = \"cuda:0\"\n    env = make_rllte_env(env_id=CustomEnv, \n                        device=device, \n                        env_kwargs={'total_length': 499} # set env arguments\n                        )\n    eval_env = make_rllte_env(env_id=CustomEnv, \n                            device=device, \n                            env_kwargs={'total_length': 499} # set env arguments\n                            )\n    agent = DrQv2(env=env, \n                eval_env=eval_env, \n                device=device,\n                tag=\"drqv2_dmc_pixel\")\n    agent.train(num_train_steps=5000)\n</code></pre></p>"},{"location":"tutorials/custom/module/","title":"Custom Module","text":"Open in Colab   View on GitHub  <p>RLLTE is an extremely open platform that supports custom modules, including <code>encoder</code>, <code>storage</code>, <code>policy</code>, etc. Just write a new module based on the <code>BaseClass</code>, then we can insert it into an agent directly. Suppose we want to build a new encoder entitled <code>CustomEncoder</code>. An example is example.py<pre><code>from rllte.agent import PPO\nfrom rllte.env import make_atari_env\nfrom rllte.common.prototype import BaseEncoder\nfrom gymnasium.spaces import Space\nfrom torch import nn\nimport torch as th\n\nclass CustomEncoder(BaseEncoder):\n    \"\"\"Custom encoder.\n\n    Args:\n        observation_space (Space): The observation space of environment.\n        feature_dim (int): Number of features extracted.\n\n    Returns:\n        The new encoder instance.\n    \"\"\"\n    def __init__(self, observation_space: Space, feature_dim: int = 0) -&gt; None:\n        super().__init__(observation_space, feature_dim)\n\n        obs_shape = observation_space.shape\n        assert len(obs_shape) == 3\n\n        self.trunk = nn.Sequential(\n            nn.Conv2d(obs_shape[0], 32, 3, stride=2), nn.ReLU(),\n            nn.Conv2d(32, 32, 3, stride=2), nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        with th.no_grad():\n            sample = th.ones(size=tuple(obs_shape)).float()\n            n_flatten = self.trunk(sample.unsqueeze(0)).shape[1]\n\n        self.trunk.extend([nn.Linear(n_flatten, feature_dim), nn.ReLU()])\n\n    def forward(self, obs: th.Tensor) -&gt; th.Tensor:\n        h = self.trunk(obs / 255.0)\n\n        return h.view(h.size()[0], -1)\n\nif __name__ == \"__main__\":\n    # env setup\n    device = \"cuda:0\"\n    env = make_atari_env(device=device)\n    eval_env = make_atari_env(device=device)\n    # create agent\n    feature_dim = 512\n    agent = PPO(env=env, \n                eval_env=eval_env, \n                device=device,\n                tag=\"ppo_atari\",\n                feature_dim=feature_dim)\n    # create a new encoder\n    encoder = CustomEncoder(observation_space=env.observation_space, \n                         feature_dim=feature_dim)\n    # set the new encoder\n    agent.set(encoder=encoder)\n    # start training\n    agent.train(num_train_steps=5000)\n</code></pre> Run <code>example.py</code> and you'll see the old <code>MnihCnnEncoder</code> has been replaced by <code>CustomEncoder</code>: <pre><code>[08/04/2023 03:47:24 PM] - [INFO.] - Invoking RLLTE Engine...\n[08/04/2023 03:47:24 PM] - [INFO.] - ================================================================================\n[08/04/2023 03:47:24 PM] - [INFO.] - Tag               : ppo_atari\n[08/04/2023 03:47:24 PM] - [INFO.] - Device            : NVIDIA GeForce RTX 3090\n[08/04/2023 03:47:24 PM] - [DEBUG] - Agent             : PPO\n[08/04/2023 03:47:24 PM] - [DEBUG] - Encoder           : CustomEncoder\n[08/04/2023 03:47:24 PM] - [DEBUG] - Policy            : OnPolicySharedActorCritic\n[08/04/2023 03:47:24 PM] - [DEBUG] - Storage           : VanillaRolloutStorage\n[08/04/2023 03:47:24 PM] - [DEBUG] - Distribution      : Categorical\n[08/04/2023 03:47:24 PM] - [DEBUG] - Augmentation      : False\n[08/04/2023 03:47:24 PM] - [DEBUG] - Intrinsic Reward  : False\n[08/04/2023 03:47:24 PM] - [DEBUG] - ================================================================================\n...\n</code></pre> As for customizing modules like <code>Storage</code> and <code>Distribution</code>, etc., users should consider compatibility with specific algorithms.</p>"},{"location":"tutorials/general/decoupling/","title":"RL Algorithm Decoupling","text":"<p>The actual performance of an RL algorithm is affected by various factors (e.g., different network architectures and experience usage  strategies), which are difficult to quantify.</p> <p>Huang S, Dossa R F J, Raffin A, et al. The 37 Implementation Details of Proximal Policy Optimization[J]. The ICLR Blog Track 2023, 2022.</p> <p>RLLTE decouples RL algorithms into minimum primitives from the perspective of exploitation and exploration and provides abundant modules for development:</p> <ul> <li>Xploit: Modules that focus on exploitation in RL.<ul> <li>Encoder: Modules for processing observations and extracting features;</li> <li>Policy: Modules for interaction and learning;</li> <li>Storage: Modules for storing and replaying collected experiences;</li> </ul> </li> <li>Xplore: Modules that focus on exploration in RL.<ul> <li>Distribution: Modules for sampling actions;</li> <li>Augmentation: Modules for observation augmentation;</li> <li>Reward: Intrinsic reward modules for enhancing exploration.</li> </ul> </li> </ul> <p>Therefore, the core of RLLTE is not designed to provide specific RL algorithms but a toolkit for producing algorithms. Developers are free to use various built-in or customized modules to build RL algorithms.</p> <ul> <li> <p>See Fast Algorithm Development</p> </li> </ul> <p>In particular, developers are allowed to replace modules of an implemented algorithm.</p> <ul> <li> <p>See Module Replacement for An Implemented Algorithm</p> </li> </ul> <p>RLLTE is an extremely open framework that allows developers to try anything.</p>"},{"location":"tutorials/general/fast/","title":"Fast Algorithm Development","text":"Open in Colab   View on GitHub  <p>Developers only need three steps to implement an RL algorithm with RLLTE:</p> <p>Workflow</p> <ol> <li>Select an algorithm prototype;</li> <li>Select desired modules;</li> <li>Write an update function.</li> </ol> <p>The following example illustrates how to write an Advantage Actor-Critic (A2C) agent to solve Atari games.</p>"},{"location":"tutorials/general/fast/#set-prototype","title":"Set prototype","text":"<p>Firstly, we select <code>OnPolicyAgent</code> as the prototype <pre><code>from rllte.common.prototype import OnPolicyAgent\n\nclass A2C(OnPolicyAgent):\n    def __init__(self, env, tag, device, num_steps):\n        # here we only use four arguments\n        super().__init__(env=env, tag=tag, device=device, num_steps=num_steps)\n</code></pre></p>"},{"location":"tutorials/general/fast/#set-necessary-modules","title":"Set necessary modules","text":"<p>Now we need an <code>encoder</code> to process observations, a learnable <code>policy</code> to generate actions, and a <code>storage</code> to store and sample experiences. <pre><code>from rllte.xploit.encoder import MnihCnnEncoder\nfrom rllte.xploit.policy import OnPolicySharedActorCritic\nfrom rllte.xploit.storage import VanillaRolloutStorage\nfrom rllte.xplore.distribution import Categorical\n</code></pre></p>"},{"location":"tutorials/general/fast/#set-update-function","title":"Set update function","text":"<p>Run the <code>.describe</code> function of the selected policy and you will see the following output: <pre><code>OnPolicySharedActorCritic.describe()\n\n# Output:\n# ================================================================================\n# Name       : OnPolicySharedActorCritic\n# Structure  : self.encoder (shared by actor and critic), self.actor, self.critic\n# Forward    : obs -&gt; self.encoder -&gt; self.actor -&gt; actions\n#            : obs -&gt; self.encoder -&gt; self.critic -&gt; values\n#            : actions -&gt; log_probs\n# Optimizers : self.optimizers['opt'] -&gt; (self.encoder, self.actor, self.critic)\n# ================================================================================\n</code></pre> This will illustrate the structure of the policy and indicate the optimizable parts. Finally, merge these modules and write an <code>.update</code> function: <pre><code>from torch import nn\nimport torch as th\n\nclass A2C(OnPolicyAgent):\n    def __init__(self, env, tag, seed, device, num_steps) -&gt; None:\n        super().__init__(env=env, tag=tag, seed=seed, device=device, num_steps=num_steps)\n        # create modules\n        encoder = MnihCnnEncoder(observation_space=env.observation_space, feature_dim=512)\n        policy = OnPolicySharedActorCritic(observation_space=env.observation_space,\n                                           action_space=env.action_space,\n                                           feature_dim=512,\n                                           opt_class=th.optim.Adam,\n                                           opt_kwargs=dict(lr=2.5e-4, eps=1e-5),\n                                           init_fn=\"xavier_uniform\"\n                                           )\n        storage = VanillaRolloutStorage(observation_space=env.observation_space,\n                                        action_space=env.action_space,\n                                        device=device,\n                                        storage_size=self.num_steps,\n                                        num_envs=self.num_envs,\n                                        batch_size=256\n                                        )\n        dist = Categorical()\n        # set all the modules\n        self.set(encoder=encoder, policy=policy, storage=storage, distribution=dist)\n\n    def update(self):\n        for _ in range(4):\n            for batch in self.storage.sample():\n                # evaluate the sampled actions\n                new_values, new_log_probs, entropy = self.policy.evaluate_actions(obs=batch.observations, actions=batch.actions)\n                # policy loss part\n                policy_loss = - (batch.adv_targ * new_log_probs).mean()\n                # value loss part\n                value_loss = 0.5 * (new_values.flatten() - batch.returns).pow(2).mean()\n                # update\n                self.policy.optimizers['opt'].zero_grad(set_to_none=True)\n                (value_loss * 0.5 + policy_loss - entropy * 0.01).backward()\n                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n                self.policy.optimizers['opt'].step()\n</code></pre></p>"},{"location":"tutorials/general/fast/#start-training","title":"Start training","text":"<p>Now we can start training by train.py<pre><code>from rllte.env import make_atari_env\nif __name__ == \"__main__\":\n    device = \"cuda\"\n    env = make_atari_env(\"AlienNoFrameskip-v4\", num_envs=8, seed=0, device=device)\n    agent = A2C(env=env, tag=\"a2c_atari\", seed=0, device=device, num_steps=128)\n    agent.train(num_train_steps=10000000)\n</code></pre> Run <code>train.py</code> and you will see the following output: <pre><code>[08/04/2023 02:19:06 PM] - [INFO.] - Invoking RLLTE Engine...\n[08/04/2023 02:19:06 PM] - [INFO.] - ================================================================================\n[08/04/2023 02:19:06 PM] - [INFO.] - Tag               : a2c_atari\n[08/04/2023 02:19:06 PM] - [INFO.] - Device            : NVIDIA GeForce RTX 3090\n[08/04/2023 02:19:07 PM] - [DEBUG] - Agent             : A2C\n[08/04/2023 02:19:07 PM] - [DEBUG] - Encoder           : MnihCnnEncoder\n[08/04/2023 02:19:07 PM] - [DEBUG] - Policy            : OnPolicySharedActorCritic\n[08/04/2023 02:19:07 PM] - [DEBUG] - Storage           : VanillaRolloutStorage\n[08/04/2023 02:19:07 PM] - [DEBUG] - Distribution      : Categorical\n[08/04/2023 02:19:07 PM] - [DEBUG] - Augmentation      : False\n[08/04/2023 02:19:07 PM] - [DEBUG] - Intrinsic Reward  : False\n[08/04/2023 02:19:07 PM] - [DEBUG] - ================================================================================\n[08/04/2023 02:19:09 PM] - [TRAIN] - S: 1024        | E: 8           | L: 44          | R: 99.000      | FPS: 407.637   | T: 0:00:02    \n[08/04/2023 02:19:10 PM] - [TRAIN] - S: 2048        | E: 16          | L: 50          | R: 109.000     | FPS: 594.725   | T: 0:00:03    \n[08/04/2023 02:19:11 PM] - [TRAIN] - S: 3072        | E: 24          | L: 47          | R: 96.000      | FPS: 692.433   | T: 0:00:04    \n[08/04/2023 02:19:12 PM] - [TRAIN] - S: 4096        | E: 32          | L: 36          | R: 93.000      | FPS: 755.935   | T: 0:00:05    \n[08/04/2023 02:19:13 PM] - [TRAIN] - S: 5120        | E: 40          | L: 55          | R: 99.000      | FPS: 809.577   | T: 0:00:06    \n[08/04/2023 02:19:14 PM] - [TRAIN] - S: 6144        | E: 48          | L: 46          | R: 34.000      | FPS: 847.310   | T: 0:00:07    \n[08/04/2023 02:19:15 PM] - [TRAIN] - S: 7168        | E: 56          | L: 49          | R: 43.000      | FPS: 878.628   | T: 0:00:08   \n...\n</code></pre></p> <p>As shown in this example, only a few dozen lines of code are needed to create RL agents with RLLTE. </p>"},{"location":"tutorials/md/cann/","title":"with CANN","text":"<p>AscendCL provides a collection of C language APIs for use in the development of DNN inference apps on Compute Architecture for Neural Networks (CANN). These APIs are designed for model and operator loading and execution, as well as media data processing, facilitating deep learning inference computing, graphics and image preprocessing, and single-operator accelerated computing on the Ascend CANN platform.</p>"},{"location":"tutorials/md/cann/#prerequisites","title":"Prerequisites","text":"<p>Get the complete repository from GitHub: <pre><code>git clone https://github.com/RLE-Foundation/rllte\n</code></pre></p> <p>Download and install the following necessary libraries: CANN 6.0.1</p>"},{"location":"tutorials/md/cann/#model-preparation","title":"Model preparation","text":"<p>Take Ascend310 for example: <pre><code>atc --model=model/test_model.onnx --framework=5 --output=test_model --input_format=NCHW --log=info --soc_version=Ascend310 --input_shape=\"input:1,9,84,84\"\n</code></pre> More details can be found in Learning Wizard.</p>"},{"location":"tutorials/md/cann/#c-development","title":"C++ development","text":"<ul> <li>Include the header file <code>#include \"acl/acl.h\"</code></li> <li>The main workflow is  <pre><code>int main()\n{   \n    // 1. Define a resource initialization function for AscendCL initialization and runtime resource allocation (specifying a compute device).\n    InitResource();\n\n    // 2. Define a model loading function for loading the image classification model.\n    const char *modelPath = \"../model/test_model.om\";\n    LoadModel(modelPath);\n\n    // 3. Define a function for prepare data to the memory and transferring the data to the device.\n    LoadData()\n\n    // 4. Define an inference function for executing inference.\n    Inference();\n\n    // 5. Define a function for processing inference result data to print the class indexes of the top 5 confidence values of the test image.\n    PrintResult();\n\n    // 6. Define a function for unloading the image classification model.\n    UnloadModel();\n\n    // 7. Define a function for freeing the memory and destroying inference-related data to prevent memory leak.\n    UnloadData();\n\n    // 8. Define a resource deinitialization function for AscendCL deinitialization and runtime resource deallocation (releasing a compute device).\n    DestroyResource();\n}\n</code></pre></li> </ul>"},{"location":"tutorials/md/cann/#build-and-run","title":"Build and run","text":"<pre><code>cd ascend\nexport APP_SOURCE_PATH=&lt;path_to_rllte_deployment&gt;/ascend\nexport DDK_PATH=&lt;path_to_ascend_toolkit&gt;\nexport NPU_HOST_LIB=&lt;path_to_ascend_devlib&gt;\nchmod +x sample_build.sh\n./sample_build.sh\n./chmod +x sample_run.sh\n./sample_run.sh\n</code></pre>"},{"location":"tutorials/md/deployment/","title":"Model Deployment","text":"<p>Currently, rllte supports model deployment by:</p> <ul> <li>NVIDIA TensorRT</li> <li>HUAWEI CANN</li> </ul> <p>The following content shows how to convert a rllte model into a <code>.onnx</code> model and deploy it on the corresponding devices.</p>"},{"location":"tutorials/md/deployment/#with-nvidia-tensorrt","title":"with NVIDIA TensorRT","text":""},{"location":"tutorials/md/deployment/#prerequisites","title":"Prerequisites","text":"<p>Get the complete repository from GitHub: <pre><code>git clone https://github.com/RLE-Foundation/rllte\n</code></pre></p> <p>Download the following necessary libraries:</p> <ul> <li>CUDA Toolkit Documentation v12.0</li> <li>cuDNN v8.8.0 for CUDA 12.0</li> <li>TensorRT 8.6.0 EA</li> </ul> <p>Meanwhile, install the following Python packages:</p> <ul> <li>pycuda==2022.2.2</li> <li>tensorrt==8.6.0</li> <li>numpy==1.24.2</li> <li>torch==2.0.0</li> <li>onnx==1.14.0</li> </ul> <p>The following two examples can used to verify your installation:</p> C++ Port<pre><code>cd deloyment/c++\nmkdir build &amp;&amp; cd build\ncmake .. &amp;&amp; make\n./DeployerTest ../../model/test_model.onnx\n</code></pre> Python Port<pre><code>cd deloyment/python\npython3 pth2onnx.py ../model/test_model.pth\n./trtexec --onnx=test_model.onnx --saveEngine=test_model.trt --skipInference\npython3 infer.py test_model.plan\n</code></pre>"},{"location":"tutorials/md/deployment/#use-in-your-c-project","title":"Use in Your C++ Project","text":"<p>The following code illustrates how to include our library in you project: example.cpp<pre><code>// Including the header file in your cpp file.\n#inlude \"RLLTEDeployer.h\n\n// Declear an instance of Options, and configurate the parameters.\nOptions options;\noptions.deviceIndex = 0;  \noptions.doesSupportDynamicBatchSize = false;  \noptions.maxWorkspaceSize = 4000000000; \noptions.precision = Precision::FP16;\n\n// Declear an instance of Options, and configurate the parameters.\nRLLTEDeployer deployer(options);\n\n// Use the build member function to convert the onnx model to the TensorRT static model (plan).\ndeployer.build(path_of_onnx_model);\n\n// Use the loadPlan member function to load the converted model. If a path is given, \n// then it will search the path, or it will just search the current working directory.\ndeployer.loadPlan();\n\n// Use infer member funtion to execute the infer process. The input is the tensor with \n// relevant data type, and the output is a pointer with relevant data size and data type. T\n// he infer result will be moved to the output.\ndeployer.infer&lt;float&gt;(input, output, 1);\ndeployer.infer&lt;float16_t&gt;(input, output, 1);\ndeployer.infer&lt;int8&gt;(input, output, 1);\n\n...\n</code></pre> Please refer to the DeployerTest.cpp for the complete code.</p>"},{"location":"tutorials/md/deployment/#with-cmake","title":"with <code>CMake</code>","text":"CMakeLists.txt<pre><code>find_package(CUDA REQUIRED)\ninclude_directories(${CUDA_INCLUDE_DIRS} ${Path_of_RLLTEDeployer_h}})\ntarget_link_libraries(YOUREXECUTEFILE ${PATH_OF_libRLLTEDeployer_so)\n</code></pre>"},{"location":"tutorials/md/deployment/#with-docker","title":"with <code>Docker</code>","text":"<p>Install the NVIDIA docker via (make sure the NVIDIA driver is installed): install_docker.sh<pre><code>sudo apt-get install ca-certificates gnupg lsb-release\nsudo mkdir -p /etc/apt/keyrings\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin \ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\n\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\nsudo groupadd docker\nsudo gpasswd -a $USER docker\n</code></pre></p> <p>Restart your device, and run the following command. <pre><code>sudo service docker restart\n</code></pre></p> <p>Now you can run your model via: run_docker.sh<pre><code>docker pull jakeshihaoluo/rllte_deployment_env:0.0.1\ndocker run -it -v ${path_to_the_repo}:/rllte --gpus all jakeshihaoluo/rllte_deployment_env:0.0.1\ncd /rllte/deloyment/c++\nmkdir build &amp;&amp; cd build\ncmake .. &amp;&amp; make\n./DeployerTest ../../model/test_model.onnx\n</code></pre></p>"},{"location":"tutorials/md/deployment/#with-huawei-cann","title":"with HUAWEI CANN","text":"<p>incoming...</p>"},{"location":"tutorials/md/tensorrt/","title":"with TensorRT","text":""},{"location":"tutorials/md/tensorrt/#prerequisites","title":"Prerequisites","text":"<p>Get the complete repository from GitHub: <pre><code>git clone https://github.com/RLE-Foundation/rllte\n</code></pre></p> <p>Download the following necessary libraries:</p> <ul> <li>CUDA Toolkit Documentation v12.0</li> <li>cuDNN v8.8.0 for CUDA 12.0</li> <li>TensorRT 8.6.0 EA</li> </ul> <p>Meanwhile, install the following Python packages:</p> <ul> <li>pycuda==2022.2.2</li> <li>tensorrt==8.6.0</li> <li>numpy==1.24.2</li> <li>torch==2.0.0</li> <li>onnx==1.14.0</li> </ul> <p>The following two examples can used to verify your installation:</p> C++ Port<pre><code>cd deloyment/c++\nmkdir build &amp;&amp; cd build\ncmake .. &amp;&amp; make\n./DeployerTest ../../model/test_model.onnx\n</code></pre> Python Port<pre><code>cd deloyment/python\npython3 pth2onnx.py ../model/test_model.pth\n./trtexec --onnx=test_model.onnx --saveEngine=test_model.trt --skipInference\npython3 infer.py test_model.plan\n</code></pre>"},{"location":"tutorials/md/tensorrt/#use-in-your-c-project","title":"Use in Your C++ Project","text":"<p>The following code illustrates how to include our library in you project: example.cpp<pre><code>// Including the header file in your cpp file.\n#inlude \"RLLTEDeployer.h\n\n// Declear an instance of Options, and configurate the parameters.\nOptions options;\noptions.deviceIndex = 0;  \noptions.doesSupportDynamicBatchSize = false;  \noptions.maxWorkspaceSize = 4000000000; \noptions.precision = Precision::FP16;\n\n// Declear an instance of Options, and configurate the parameters.\nRLLTEDeployer deployer(options);\n\n// Use the build member function to convert the onnx model to the TensorRT static model (plan).\ndeployer.build(path_of_onnx_model);\n\n// Use the loadPlan member function to load the converted model. If a path is given, \n// then it will search the path, or it will just search the current working directory.\ndeployer.loadPlan();\n\n// Use infer member funtion to execute the infer process. The input is the tensor with \n// relevant data type, and the output is a pointer with relevant data size and data type. T\n// he infer result will be moved to the output.\ndeployer.infer&lt;float&gt;(input, output, 1);\ndeployer.infer&lt;float16_t&gt;(input, output, 1);\ndeployer.infer&lt;int8&gt;(input, output, 1);\n\n...\n</code></pre> Please refer to the DeployerTest.cpp for the complete code.</p>"},{"location":"tutorials/md/tensorrt/#with-cmake","title":"with <code>CMake</code>","text":"CMakeLists.txt<pre><code>find_package(CUDA REQUIRED)\ninclude_directories(${CUDA_INCLUDE_DIRS} ${Path_of_RLLTEDeployer_h}})\ntarget_link_libraries(YOUREXECUTEFILE ${PATH_OF_libRLLTEDeployer_so)\n</code></pre>"},{"location":"tutorials/md/tensorrt/#with-docker","title":"with <code>Docker</code>","text":"<p>Install the NVIDIA docker via (make sure the NVIDIA driver is installed): install_docker.sh<pre><code>sudo apt-get install ca-certificates gnupg lsb-release\nsudo mkdir -p /etc/apt/keyrings\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin \ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\n\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\nsudo groupadd docker\nsudo gpasswd -a $USER docker\n</code></pre></p> <p>Restart your device, and run the following command. <pre><code>sudo service docker restart\n</code></pre></p> <p>Now you can run your model via: run_docker.sh<pre><code>docker pull jakeshihaoluo/rllte_deployment_env:0.0.1\ndocker run -it -v ${path_to_the_repo}:/rllte --gpus all jakeshihaoluo/rllte_deployment_env:0.0.1\ncd /rllte/deloyment/c++\nmkdir build &amp;&amp; cd build\ncmake .. &amp;&amp; make\n./DeployerTest ../../model/test_model.onnx\n</code></pre></p>"},{"location":"tutorials/me/comp/","title":"Performance Comparison of Multiple Algorithms","text":"Open in Colab   View on GitHub"},{"location":"tutorials/me/comp/#download-data","title":"Download Data","text":"<p>Suppose we want to evaluate algorithm performance on the Procgen benchmark. First, download the data from  rllte-hub: example.py<pre><code># load packages\nfrom rllte.evaluation import Performance, Comparison, min_max_normalize\nfrom rllte.hub.datasets import Procgen, Atari\nimport numpy as np\n# load scores\nprocgen = Procgen()\nprocgen_scores = procgen.load_scores()\nprint(procgen_scores.keys())\n# get ppo-normalized scores\nppo_norm_scores = dict()\nMIN_SCORES = np.zeros_like(procgen_scores['ppo'])\nMAX_SCORES = np.mean(procgen_scores['ppo'], axis=0)\nfor algo in procgen_scores.keys():\n    ppo_norm_scores[algo] = min_max_normalize(procgen_scores[algo],\n                                              min_scores=MIN_SCORES,\n                                              max_scores=MAX_SCORES)\n\n# Output:\n# dict_keys(['ppg', 'mixreg', 'ppo', 'idaac', 'plr', 'ucb-drac'])\n</code></pre> For each algorithm, this will return a <code>NdArray</code> of size (<code>10</code> x <code>16</code>) where scores[n][m] represent the score on run <code>n</code> of task <code>m</code>.</p>"},{"location":"tutorials/me/comp/#performance-comparison","title":"Performance Comparison","text":"<p><code>Comparison</code> module allows you to compare the performance between two algorithms: example.py<pre><code>comp = Comparison(scores_x=ppo_norm_scores['PPG'],\n                  scores_y=ppo_norm_scores['PPO'],\n                  get_ci=True)\ncomp.compute_poi()\n\n# Output:\n# (0.8153125, array([[0.779375  ], [0.85000781]]))\n</code></pre> This indicates the overall probability of imporvement of <code>PPG</code> over <code>PPO</code> is <code>0.8153125</code>.</p> <p>Available metrics:</p> Metric Remark <code>.compute_poi</code> Compute the overall probability of imporvement of algorithm <code>X</code> over <code>Y</code>."},{"location":"tutorials/me/perf/","title":"Performance Evaluation of Single Algorithm","text":"Open in Colab   View on GitHub  <p>RLLTE provides evaluation methods based on:</p> <p>Agarwal R, Schwarzer M, Castro P S, et al. Deep reinforcement learning at the edge of the statistical precipice[J]. Advances in neural information processing systems, 2021, 34: 29304-29320.</p> <p>We reconstruct and improve the code of the official repository rliable, achieving higher convenience and efficiency.</p>"},{"location":"tutorials/me/perf/#download-data","title":"Download Data","text":"<p>Suppose we want to evaluate algorithm performance on the Procgen benchmark. First, download the data from  rllte-hub: example.py<pre><code># load packages\nfrom rllte.evaluation import Performance, Comparison, min_max_normalize\nfrom rllte.hub.datasets import Procgen, Atari\nimport numpy as np\n# load scores\nprocgen = Procgen()\nprocgen_scores = procgen.load_scores()\nprint(procgen_scores.keys())\n# get ppo-normalized scores\nppo_norm_scores = dict()\nMIN_SCORES = np.zeros_like(procgen_scores['ppo'])\nMAX_SCORES = np.mean(procgen_scores['ppo'], axis=0)\nfor algo in procgen_scores.keys():\n    ppo_norm_scores[algo] = min_max_normalize(procgen_scores[algo],\n                                              min_scores=MIN_SCORES,\n                                              max_scores=MAX_SCORES)\n\n# Output:\n# dict_keys(['ppg', 'mixreg', 'ppo', 'idaac', 'plr', 'ucb-drac'])\n</code></pre> For each algorithm, this will return a <code>NdArray</code> of size (<code>10</code> x <code>16</code>) where scores[n][m] represent the score on run <code>n</code> of task <code>m</code>.</p>"},{"location":"tutorials/me/perf/#performance-evaluation","title":"Performance Evaluation","text":"<p>Initialize the performance evaluator: example.py<pre><code>perf = Performance(scores=ppo_norm_scores['PPO'], \n                   get_ci=True # get confidence intervals\n                   )\nperf.aggregate_mean()\n\n# Output:\n# Computing confidence interval for aggregate MEAN...\n# (1.0, array([[0.9737281 ], [1.02564405]]))\n</code></pre> Available metrics:</p> Metric Remark <code>.aggregate_mean</code> Computes mean of sample mean scores per task. <code>.aggregate_median</code> Computes median of sample mean scores per task. <code>.aggregate_og</code> Computes optimality gap across all runs and tasks. <code>.aggregate_iqm</code> Computes the interquartile mean across runs and tasks. <code>.create_performance_profile</code> Computes the performance profiles."},{"location":"tutorials/me/visual/","title":"Metrics Visualization","text":"Open in Colab   View on GitHub"},{"location":"tutorials/me/visual/#download-data","title":"Download Data","text":"<p>Suppose we want to visualize algorithm performance on the Procgen benchmark. First, download the data from  rllte-hub: example.py<pre><code># load packages\nfrom rllte.evaluation import Performance, Comparison, min_max_normalize\nfrom rllte.hub.datasets import Procgen, Atari\nfrom rllte.evaluation import (plot_interval_estimates,\n                              plot_probability_improvement,\n                              plot_sample_efficiency_curve,\n                              plot_performance_profile)\nimport numpy as np\n# load scores\nprocgen = Procgen()\nprocgen_scores = procgen.load_scores()\nprint(procgen_scores.keys())\n# get ppo-normalized scores\nppo_norm_scores = dict()\nMIN_SCORES = np.zeros_like(procgen_scores['ppo'])\nMAX_SCORES = np.mean(procgen_scores['ppo'], axis=0)\nfor algo in procgen_scores.keys():\n    ppo_norm_scores[algo] = min_max_normalize(procgen_scores[algo],\n                                              min_scores=MIN_SCORES,\n                                              max_scores=MAX_SCORES)\n\n# Output:\n# dict_keys(['ppg', 'mixreg', 'ppo', 'idaac', 'plr', 'ucb-drac'])\n</code></pre> For each algorithm, this will return a <code>NdArray</code> of size (<code>10</code> x <code>16</code>) where scores[n][m] represent the score on run <code>n</code> of task <code>m</code>.</p>"},{"location":"tutorials/me/visual/#visualization","title":"Visualization","text":""},{"location":"tutorials/me/visual/#plot_interval_estimates","title":"<code>.plot_interval_estimates</code>","text":"<p><code>.plot_interval_estimates</code> can plot various performance metrics of algorithms with stratified confidence intervals. Take Procgen for example, we want to plot four reliable metrics computed by <code>Performance</code> evaluator: example.py<pre><code># construct a performance dict\naggregate_performance_dict = {\n    \"MEAN\": {},\n    \"MEDIAN\": {},\n    \"IQM\": {},\n    \"OG\": {}\n}\nfor algo in ppo_norm_scores.keys():\n    perf = Performance(scores=ppo_norm_scores[algo], get_ci=True)\n    aggregate_performance_dict['MEAN'][algo] = perf.aggregate_mean()\n    aggregate_performance_dict['MEDIAN'][algo] = perf.aggregate_median()\n    aggregate_performance_dict['IQM'][algo] = perf.aggregate_iqm()\n    aggregate_performance_dict['OG'][algo] = perf.aggregate_og()\n\n# plot all the four metrics of all the algorithms\nfig, axes = plot_interval_estimates(aggregate_performance_dict,\n                                    metric_names=['MEAN', 'MEDIAN', 'IQM', 'OG'],\n                                    algorithms=['PPO', 'MixReg', 'UCB-DrAC', 'PLR', 'PPG', 'IDAAC'],\n                                    xlabel=\"PPO-Normalized Score\")\nfig.savefig('./plot_interval_estimates1.png', format='png', bbox_inches='tight')\n\n# plot two metrics of all the algorithms\nfig, axes = plot_interval_estimates(aggregate_performance_dict,\n                        metric_names=['MEAN', 'MEDIAN'],\n                        algorithms=['PPO', 'MixReg', 'UCB-DrAC', 'PLR', 'PPG', 'IDAAC'],\n                        xlabel=\"PPO-Normalized Score\")\nfig.savefig('./plot_interval_estimates2.png', format='png', bbox_inches='tight')\n\n# plot two metrics of three algorithms\nfig, axes = plot_interval_estimates(aggregate_performance_dict,\n                        metric_names=['MEAN', 'MEDIAN'],\n                        algorithms=['ppg', 'mixreg', 'ppo'],\n                        xlabel=\"PPO-Normalized Score\",\n                        xlabel_y_coordinate=-0.4)\nfig.savefig('./plot_interval_estimates3.png', format='png', bbox_inches='tight')\n</code></pre> The output figures are:</p>"},{"location":"tutorials/me/visual/#plot_probability_improvement","title":"<code>.plot_probability_improvement</code>","text":"<p><code>.plot_probability_improvement</code> plots probability of improvement with stratified confidence intervals. An example is: example.py<pre><code># construct a comparison dict\npairs = [['IDAAC', 'PPG'], ['IDAAC', 'UCB-DrAC'], ['IDAAC', 'PPO'],\n    ['PPG', 'PPO'], ['UCB-DrAC', 'PLR'], \n    ['PLR', 'MixReg'], ['UCB-DrAC', 'MixReg'],  ['MixReg', 'PPO']]\n\nprobability_of_improvement_dict = {}\nfor pair in pairs:\n    comp = Comparison(scores_x=ppo_norm_scores[pair[0]], \n                      scores_y=ppo_norm_scores[pair[1]],\n                      get_ci=True)\n    probability_of_improvement_dict['_'.join(pair)] = comp.compute_poi()\n\nfig, ax = plot_probability_improvement(poi_dict=probability_of_improvement_dict)\nfig.savefig('./plot_probability_improvement.png', format='png', bbox_inches='tight')\n</code></pre> The output figure is:</p>"},{"location":"tutorials/me/visual/#plot_performance_profile","title":"<code>.plot_performance_profile</code>","text":"<p><code>.plot_performance_profile</code> plots performance profiles with stratified confidence intervals. An example is: example.py<pre><code>profile_dict = dict()\nprocgen_tau = np.linspace(0.5, 3.6, 101)\n\nfor algo in ppo_norm_scores.keys():\n    perf = Performance(scores=ppo_norm_scores[algo], get_ci=True, reps=2000)\n    profile_dict[algo] = perf.create_performance_profile(tau_list=procgen_tau)\n\nfig, axes = plot_performance_profile(profile_dict, \n                         procgen_tau,\n                         figsize=(7, 5),\n                         xlabel=r'PPO-Normalized Score $(\\tau)$',\n                         )\nfig.savefig('./plot_performance_profile.png', format='png', bbox_inches='tight')\n</code></pre> The output figure is:</p>"},{"location":"tutorials/me/visual/#plot_sample_efficiency_curve","title":"<code>.plot_sample_efficiency_curve</code>","text":"<p><code>.plot_sample_efficiency_curve</code> plots an aggregate metric with CIs as a function of environment frames. An example is: example.py<pre><code># get Atari games' curve data\nale_all_frames_scores_dict = Atari().load_curves()\nprint(ale_all_frames_scores_dict.keys())\nprint(ale_all_frames_scores_dict['C51'].shape)\n# Output:\n# dict_keys(['C51', 'DQN (Adam)', 'DQN (Nature)', 'Rainbow', 'IQN', 'REM', 'M-IQN', 'DreamerV2'])\n# (5, 55, 200)\n# 200 data points of 55 games over 5 random seeds\n\nframes = np.array([1, 10, 25, 50, 75, 100, 125, 150, 175, 200]) - 1\n\nsampling_dict = dict()\nfor algo in ale_all_frames_scores_dict.keys():\n    sampling_dict[algo] = [[], [], []]\n    for frame in frames:\n        perf = Performance(ale_all_frames_scores_dict[algo][:, :, frame],\n                           get_ci=True, \n                           reps=2000)\n        value, CIs = perf.aggregate_iqm()\n        sampling_dict[algo][0].append(value)\n        sampling_dict[algo][1].append(CIs[0]) # lower bound\n        sampling_dict[algo][2].append(CIs[1]) # upper bound\n\n    sampling_dict[algo][0] = np.array(sampling_dict[algo][0]).reshape(-1)\n    sampling_dict[algo][1] = np.array(sampling_dict[algo][1]).reshape(-1)\n    sampling_dict[algo][2] = np.array(sampling_dict[algo][2]).reshape(-1)\n\nalgorithms = ['C51', 'DQN (Adam)', 'DQN (Nature)', 'Rainbow', 'IQN', 'REM', 'M-IQN', 'DreamerV2']\nfig, axes = plot_sample_efficiency_curve(\n    sampling_dict,\n    frames+1, \n    figsize=(7, 4.5),\n    algorithms=algorithms,\n    xlabel=r'Number of Frames (in millions)',\n    ylabel='IQM Human Normalized Score')\nfig.savefig('./plot_sample_efficiency_curve.png', format='png', bbox_inches='tight')\n</code></pre> The output figure is:</p>"},{"location":"tutorials/mt/irs/","title":"Intrinsic Reward Shaping for Enhancing Exploration","text":"Open in Colab   View on GitHub"},{"location":"tutorials/mt/irs/#introduction","title":"Introduction","text":"<p>Since RLLTE decouples RL algorithms into minimum primitives from the perspective of exploitation and exploration, intrinsic reward shaping is supported by default. RLLTE eXplore (RLeXplore) is a unified, highly-modularized and plug-and-play toolkit that currently provides high-quality and reliable implementations of eight representative intrinsic reward algorithms. It used to be challenging to compare intrinsic reward algorithms due to various confounding factors, including distinct implementations, optimization strategies, and evaluation methodologies. Therefore, RLeXplore is designed to provide unified and standardized procedures for constructing, computing, and optimizing intrinsic reward modules.</p> <p>The workflow of RLeXplore is illustrated as follows:</p> <p>Commonly, at each time step, the agent receives observations from the environment and predicts actions. The environment then executes the actions and returns feedback to the agent, which consists of a next observation, a reward, and a terminal signal. During the data collection process, the .watch() function is used to monitor the agent-environment interactions. For instance, E3B [1] updates an estimate of an ellipsoid in an embedding space after observing every state. At the end of the data collection rollouts, .compute() computes the corresponding intrinsic rewards. Note that .compute() is only called once per rollout using batched operations, which makes RLeXplore a highly efficient framework. Additionally, RLeXplore provides several utilities for reward and observation normalization. Finally, the .update() function is called immediately after .compute() to update the reward module if necessary (e.g., train the forward dynamics models in Disagreement [2] or the predictor network in RND [3]). All operations are subject to the standard workflow of the Gymnasium API. </p> <pre><code>[1] Henaff M, Raileanu R, Jiang M, et al. Exploration via elliptical episodic bonuses[J]. Advances in Neural Information Processing Systems, 2022, 35: 37631-37646.\n[2] Pathak D, Agrawal P, Efros A A, et al. Curiosity-driven exploration by self-supervised prediction[C]//International conference on machine learning. PMLR, 2017: 2778-2787.\n[3] Burda Y, Edwards H, Storkey A, et al. Exploration by random network distillation[C]//Seventh International Conference on Learning Representations. 2019: 1-17.\n</code></pre>"},{"location":"tutorials/mt/irs/#workflow","title":"Workflow","text":"<ul> <li> <p>First, load the intrinsic module from RLLTE and other libraries: <pre><code>import gymnasium as gym\nimport numpy as np\nimport torch as th\n\nfrom rllte.env.utils import Gymnasium2Torch\nfrom rllte.xplore.reward import ICM\n</code></pre></p> </li> <li> <p>Second, create a fake Atari environment with image observations: <pre><code>class FakeAtari(gym.Env):\n    def __init__(self):\n        self.action_space = gym.spaces.Discrete(7)\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(4, 84, 84))\n        self.count = 0\n\n    def reset(self):\n        self.count = 0\n        return self.observation_space.sample(), {}\n\n    def step(self, action):\n        self.count += 1\n        if self.count &gt; 100 and np.random.rand() &lt; 0.1:\n            term = trunc = True\n        else:\n            term = trunc = False\n        return self.observation_space.sample(), 0, term, trunc, {}\n</code></pre></p> </li> <li> <p>Record the transitions data via .watch() and compute the intrinsic rewards via .compute(): <pre><code># set the parameters\ndevice = 'cuda' if th.cuda.is_available() else 'cpu'\nn_steps = 128\nn_envs = 8\n# create the vectorized environments\nenvs = gym.vector.AsyncVectorEnv([FakeAtari for _ in range(n_envs)])\n# wrap the environments to convert the observations to torch tensors\nenvs = Gymnasium2Torch(envs, device)\n# create the intrinsic reward module\nirs = ICM(envs, device)\n# reset the environments and get the initial observations\nobs, infos = envs.reset()\n# create a dictionary to store the samples\nsamples = {'observations':[], \n           'actions':[], \n           'rewards':[],\n           'terminateds':[],\n           'truncateds':[],\n           'next_observations':[]}\n# sampling loop\nfor _ in range(n_steps):\n    # sample random actions\n    actions = th.stack([th.as_tensor(envs.action_space.sample()) for _ in range(n_envs)])\n    # environment step\n    next_obs, rewards, terminateds, truncateds, infos = envs.step(actions)\n    # watch the interactions and get necessary information for the intrinsic reward computation\n    irs.watch(observations=obs, \n              actions=actions, \n              rewards=rewards,\n              terminateds=terminateds,\n              truncateds=truncateds,\n              next_observations=next_obs)\n    # store the samples\n    samples['observations'].append(obs)\n    samples['actions'].append(actions)\n    samples['rewards'].append(rewards)\n    samples['terminateds'].append(terminateds)\n    samples['truncateds'].append(truncateds)\n    samples['next_observations'].append(next_obs)\n    obs = next_obs\n# compute the intrinsic rewards\nsamples = {k: th.stack(v) for k, v in samples.items()}\nintrinsic_rewards = irs.compute(samples=samples)\nprint(intrinsic_rewards)\nprint(intrinsic_rewards.shape)\n\n# Output:\n# tensor([[6.5928, 5.5006, 5.3346,  ..., 5.3286, 6.5831, 5.1960],\n#         [3.4611, 3.4754, 5.3265,  ..., 4.9442, 5.3422, 3.7767],\n#         [3.7612, 3.7736, 6.5909,  ..., 3.7735, 4.9679, 6.5922],\n#         ...,\n#         [3.4737, 4.9781, 6.5358,  ..., 5.2204, 5.3287, 6.5794],\n#         [3.7659, 5.3463, 5.3620,  ..., 6.5735, 5.3437, 3.7666],\n#         [5.4956, 4.9599, 5.3435,  ..., 6.5689, 5.2174, 3.7587]],\n#        device='cuda:0')\n# torch.Size([128, 8])\n</code></pre> In this example, <code>Synchronous Mode</code> is used, in which the .update() will be automatically invoked in the .compute() function, usually for on-policy RL algorithms. For off-policy RL algorithms, we need to use the <code>Asynchronous Mode</code> and invoke the .update() separately: <pre><code># set the parameters\ndevice = 'cuda' if th.cuda.is_available() else 'cpu'\nn_steps = 128\nn_envs = 8\n# create the vectorized environments\nenvs = gym.vector.AsyncVectorEnv([FakeAtari for _ in range(n_envs)])\n# wrap the environments to convert the observations to torch tensors\nenvs = Gymnasium2Torch(envs, device)\n# create the intrinsic reward module\nirs = ICM(envs, device)\n# reset the environments and get the initial observations\nobs, infos = envs.reset()\n# create a dictionary to store the samples\nsamples = {'observations':[], \n           'actions':[], \n           'rewards':[],\n           'terminateds':[],\n           'truncateds':[],\n           'next_observations':[]}\n# sampling loop\nfor _ in range(n_steps):\n    # sample random actions\n    actions = th.stack([th.as_tensor(envs.action_space.sample()) for _ in range(n_envs)])\n    # environment step\n    next_obs, rewards, terminateds, truncateds, infos = envs.step(actions)\n    # watch the interactions and get necessary information for the intrinsic reward computation\n    irs.watch(observations=obs, \n              actions=actions, \n              rewards=rewards,\n              terminateds=terminateds,\n              truncateds=truncateds,\n              next_observations=next_obs)\n    # compute the intrinsic rewards at each step\n    intrinsic_rewards = irs.compute(samples={'observations':obs.unsqueeze(0), \n                                            'actions':actions.unsqueeze(0), \n                                            'rewards':rewards.unsqueeze(0),\n                                            'terminateds':terminateds.unsqueeze(0),\n                                            'truncateds':truncateds.unsqueeze(0),\n                                            'next_observations':next_obs.unsqueeze(0)}, \n                                            sync=False)\n    print(intrinsic_rewards, intrinsic_rewards.shape)\n    # store the samples\n    samples['observations'].append(obs)\n    samples['actions'].append(actions)\n    samples['rewards'].append(rewards)\n    samples['terminateds'].append(terminateds)\n    samples['truncateds'].append(truncateds)\n    samples['next_observations'].append(next_obs)\n    obs = next_obs\n# update the intrinsic reward module\nsamples = {k: th.stack(v) for k, v in samples.items()}\nirs.update(samples=samples)\n</code></pre></p> </li> </ul>"},{"location":"tutorials/mt/irs/#rlexplore-with-rllte","title":"RLeXplore with RLLTE","text":"<p>In RLLTE, you can also invoke the intrinsic reward module in all the implemented algorithms directly by <code>.set</code> function: example.py<pre><code>from rllte.agent import PPO\nfrom rllte.env import make_atari_env\nfrom rllte.xplore.reward import RE3\n\nif __name__ == \"__main__\":\n    # env setup\n    device = \"cuda:0\"\n    envs = make_atari_env(device=device)\n    # create agent\n    agent = PPO(env=envs, \n                device=device,\n                tag=\"ppo_atari\")\n    # create intrinsic reward\n    re3 = RE3(envs=envs, device=device)\n    # set the module\n    agent.set(reward=re3)\n    # start training\n    agent.train(num_train_steps=5000)\n</code></pre> Run <code>example.py</code> and you'll see the intrinsic reward module is invoked: <pre><code>[08/04/2023 03:54:10 PM] - [INFO.] - Invoking RLLTE Engine...\n[08/04/2023 03:54:10 PM] - [INFO.] - ================================================================================\n[08/04/2023 03:54:10 PM] - [INFO.] - Tag               : ppo_atari\n[08/04/2023 03:54:10 PM] - [INFO.] - Device            : NVIDIA GeForce RTX 3090\n[08/04/2023 03:54:11 PM] - [DEBUG] - Agent             : PPO\n[08/04/2023 03:54:11 PM] - [DEBUG] - Encoder           : MnihCnnEncoder\n[08/04/2023 03:54:11 PM] - [DEBUG] - Policy            : OnPolicySharedActorCritic\n[08/04/2023 03:54:11 PM] - [DEBUG] - Storage           : VanillaRolloutStorage\n[08/04/2023 03:54:11 PM] - [DEBUG] - Distribution      : Categorical\n[08/04/2023 03:54:11 PM] - [DEBUG] - Augmentation      : False\n[08/04/2023 03:54:11 PM] - [DEBUG] - Intrinsic Reward  : True, RE3\n[08/04/2023 03:54:11 PM] - [DEBUG] - ================================================================================\n</code></pre></p>"},{"location":"tutorials/mt/irs/#rlexplore-with-stable-baselines-cleanrl","title":"RLeXplore with Stable-baselines, CleanRL, ...","text":"<p>RLeXplore can be seamlessly integrated with existing RL libraries, such as Stable-Baselines3, CleanRL, etc. We provide specific examples on GitHub.</p>"},{"location":"tutorials/mt/irs/#custom-intrinsic-reward","title":"Custom Intrinsic Reward","text":"<p>Since RLeXplore provides a standardized workflow and modular components of intrinsic rewards, which facilitates the creation, modification, and testing of new ideas. See the example of creating custom intrinsic rewards on GitHub.</p>"},{"location":"tutorials/mt/irs/#benchmark-results","title":"Benchmark Results","text":"<p>We conducted extensive experiments to evaluate the performance of RLeXplore on multiple well-recognized exploration tasks, such as SuperMarioBros, MiniGrid, etc. Please view our Wandb space for the benchmark results.</p>"},{"location":"tutorials/mt/oa/","title":"Observation Augmentation for Sample Efficiency and Generalization","text":"Open in Colab   View on GitHub  <p>Observation augmentation is an efficient approach to improve sample efficiency and generalization, which is also a basic primitive of RLLTE.</p> <ul> <li>Laskin M, Lee K, Stooke A, et al. Reinforcement learning with augmented data[J]. Advances in neural information processing systems, 2020, 33: 19884-19895.</li> <li>Yarats D, Fergus R, Lazaric A, et al. Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning[C]//International Conference on Learning Representations. 2021.</li> </ul> <p>RLLTE implements the augmentation modules via a PyTorch-NN manner, and both imaged-based and state-based observations are supported. A code example is: example.py<pre><code>from rllte.agent import DrAC\nfrom rllte.env import make_atari_env\nfrom rllte.xplore.augmentation import RandomCrop\n\nif __name__ == \"__main__\":\n    # env setup\n    device = \"cuda:0\"\n    env = make_atari_env(device=device)\n    eval_env = make_atari_env(device=device)\n    # create agent\n    agent = DrAC(env=env, \n                eval_env=eval_env, \n                device=device,\n                tag=\"drac_atari\")\n    # create augmentation module\n    random_crop = RandomCrop()\n    # set the module\n    agent.set(augmentation=random_crop)\n    # start training\n    agent.train(num_train_steps=5000)\n</code></pre> Run <code>example.py</code> and you'll see the augmentation module is invoked: <pre><code>[08/04/2023 05:00:15 PM] - [INFO.] - Invoking RLLTE Engine...\n[08/04/2023 05:00:15 PM] - [INFO.] - ================================================================================\n[08/04/2023 05:00:15 PM] - [INFO.] - Tag               : drac_atari\n[08/04/2023 05:00:16 PM] - [INFO.] - Device            : NVIDIA GeForce RTX 3090\n[08/04/2023 05:00:16 PM] - [DEBUG] - Agent             : DrAC\n[08/04/2023 05:00:16 PM] - [DEBUG] - Encoder           : MnihCnnEncoder\n[08/04/2023 05:00:16 PM] - [DEBUG] - Policy            : OnPolicySharedActorCritic\n[08/04/2023 05:00:16 PM] - [DEBUG] - Storage           : VanillaRolloutStorage\n[08/04/2023 05:00:16 PM] - [DEBUG] - Distribution      : Categorical\n[08/04/2023 05:00:16 PM] - [DEBUG] - Augmentation      : True, RandomCrop\n[08/04/2023 05:00:16 PM] - [DEBUG] - Intrinsic Reward  : False\n[08/04/2023 05:00:16 PM] - [DEBUG] - ================================================================================\n...\n</code></pre></p> <p>Compatibility of augmentation</p> <p>Note that the module will only make difference when the algorithm supports data augmentation. Please refer to https://docs.rllte.dev/api/ for the compatibility.</p>"},{"location":"tutorials/mt/pre-training/","title":"Pre-training with Intrinsic Rewards","text":"Open in Colab   View on GitHub"},{"location":"tutorials/mt/pre-training/#pre-training","title":"Pre-training","text":"<p>Currently, RLLTE only supports online pre-training via intrinsic reward. To turn on the pre-training mode,  it suffices to write a <code>train.py</code> like: train.py<pre><code>from rllte.agent import PPO\nfrom rllte.env import make_atari_env\nfrom rllte.xplore.reward import RE3\n\nif __name__ == \"__main__\":\n    # env setup\n    device = \"cuda:0\"\n    env = make_atari_env(device=device)\n    # create agent and turn on pre-training mode\n    agent = PPO(env=env, \n                device=device,\n                tag=\"ppo_atari\",\n                pretraining=True)\n    # create intrinsic reward\n    re3 = RE3(envs=env, device=device)\n    # set the reward module\n    agent.set(reward=re3)\n    # start training\n    agent.train(num_train_steps=5000)\n</code></pre> Run <code>train.py</code> and you'll see the pre-training mode is on: <pre><code>[08/04/2023 05:05:54 PM] - [INFO.] - Invoking RLLTE Engine...\n[08/04/2023 05:05:54 PM] - [INFO.] - ================================================================================\n[08/04/2023 05:05:54 PM] - [INFO.] - Tag               : ppo_atari\n[08/04/2023 05:05:54 PM] - [INFO.] - Device            : NVIDIA GeForce RTX 3090\n[08/04/2023 05:05:54 PM] - [DEBUG] - Agent             : PPO\n[08/04/2023 05:05:54 PM] - [DEBUG] - Encoder           : MnihCnnEncoder\n[08/04/2023 05:05:54 PM] - [DEBUG] - Policy            : OnPolicySharedActorCritic\n[08/04/2023 05:05:54 PM] - [DEBUG] - Storage           : VanillaRolloutStorage\n[08/04/2023 05:05:54 PM] - [DEBUG] - Distribution      : Categorical\n[08/04/2023 05:05:54 PM] - [DEBUG] - Augmentation      : False\n[08/04/2023 05:05:54 PM] - [DEBUG] - Intrinsic Reward  : True, RE3\n[08/04/2023 05:05:54 PM] - [INFO.] - Pre-training Mode : On\n[08/04/2023 05:05:54 PM] - [DEBUG] - ================================================================================\n...\n</code></pre></p> <p>Tip</p> <p>When the pre-training mode is on, a <code>reward</code> module must be specified!</p> <p>For all supported reward modules, see API Documentation.</p>"},{"location":"tutorials/mt/pre-training/#fine-tuning","title":"Fine-tuning","text":"<p>Once the pre-training is finished, you can find the model parameters in the <code>pretrained</code> subfolder of the working directory. To  load the parameters, just turn off the pre-training mode and load the parameters with <code>.load()</code> function:</p> <p>train.py<pre><code>from rllte.agent import PPO\nfrom rllte.env import make_atari_env\n\nif __name__ == \"__main__\":\n    # env setup\n    device = \"cuda:0\"\n    env = make_atari_env(device=device)\n    eval_env = make_atari_env(device=device)\n    # create agent and turn off pre-training mode\n    agent = PPO(env=env, \n                eval_env=eval_env, \n                device=device,\n                tag=\"ppo_atari\",\n                pretraining=False)\n    # start training\n    agent.train(num_train_steps=5000,\n                init_model_path=\"/export/yuanmingqi/code/rllte/logs/ppo_atari/2023-06-05-02-42-12/pretrained/pretrained.pth\")\n</code></pre> Run <code>train.py</code> and you'll see the pre-trained model parameters are loaded: <pre><code>[08/04/2023 05:07:52 PM] - [INFO.] - Invoking RLLTE Engine...\n[08/04/2023 05:07:52 PM] - [INFO.] - ================================================================================\n[08/04/2023 05:07:52 PM] - [INFO.] - Tag               : ppo_atari\n[08/04/2023 05:07:52 PM] - [INFO.] - Device            : NVIDIA GeForce RTX 3090\n[08/04/2023 05:07:53 PM] - [DEBUG] - Agent             : PPO\n[08/04/2023 05:07:53 PM] - [DEBUG] - Encoder           : MnihCnnEncoder\n[08/04/2023 05:07:53 PM] - [DEBUG] - Policy            : OnPolicySharedActorCritic\n[08/04/2023 05:07:53 PM] - [DEBUG] - Storage           : VanillaRolloutStorage\n[08/04/2023 05:07:53 PM] - [DEBUG] - Distribution      : Categorical\n[08/04/2023 05:07:53 PM] - [DEBUG] - Augmentation      : False\n[08/04/2023 05:07:53 PM] - [DEBUG] - Intrinsic Reward  : False\n[08/04/2023 05:07:53 PM] - [DEBUG] - ================================================================================\n[08/04/2023 05:07:53 PM] - [INFO.] - Loading Initial Parameters from ./logs/ppo_atari/...\n...\n</code></pre></p>"},{"location":"tutorials/mt/quick_start/","title":"Quick Start","text":"Open in Colab   View on GitHub  <p>RLLTE provides reliable implementations for highly-recognized RL algorithms, and users can build applications with very simple code.</p>"},{"location":"tutorials/mt/quick_start/#on-nvidia-gpu","title":"On NVIDIA GPU","text":"<p>Suppose we want to use DrQ-v2 to solve a task of DeepMind Control Suite, and  it suffices to write a <code>train.py</code> like:</p> train.py<pre><code># import `env` and `agent` module\nfrom rllte.env import make_dmc_env \nfrom rllte.agent import DrQv2\n\nif __name__ == \"__main__\":\n    device = \"cuda:0\"\n    # create env, and `eval_env` is optional\n    env = make_dmc_env(env_id=\"cartpole_balance\", device=device)\n    eval_env = make_dmc_env(env_id=\"cartpole_balance\", device=device)\n    # create agent\n    agent = DrQv2(env=env, \n                  eval_env=eval_env, \n                  device='cuda',\n                  tag=\"drqv2_dmc_pixel\")\n    # start training\n    agent.train(num_train_steps=5000, log_interval=1000)\n</code></pre> <p>Run <code>train.py</code> and you will see the following output:</p> <p>Read the logs</p> <ul> <li>S: Number of environment steps. Note that <code>S</code> isn't equal to the number of frames in visual tasks, and <code>number_of_frames=number_of_steps * number_of_action_repeats</code></li> <li>E: Number of environment episodes.</li> <li>L: Average episode length.</li> <li>R: Average episode reward.</li> <li>FPS: Training FPS.</li> <li>T: Time costs.</li> </ul>"},{"location":"tutorials/mt/quick_start/#on-huawei-npu","title":"On HUAWEI NPU","text":"<p>Similarly, if we want to train an agent on HUAWEI NPU, it suffices to replace <code>cuda</code> with <code>npu</code>: train.py<pre><code>device = \"npu:0\"\n</code></pre></p> <p>Compatibility of NPU</p> <p>Please refer to https://docs.rllte.dev/api/ for the compatibility of NPU.</p>"},{"location":"tutorials/mt/quick_start/#load-the-trained-model","title":"Load the trained model","text":"<p>Once the training is finished, you can find <code>agent.pth</code> in the subfolder <code>model</code> of the specified working directory.</p> play.py<pre><code>import torch as th\n\n# load the model and specify the map location\nagent = th.load(\"agent.pth\", map_location=th.device('cpu'))\nobs = th.zeros(size=(1, 9, 84, 84))\naction = agent(obs)\nprint(action)\n\n# Output: tensor([[-1.0000]], grad_fn=&lt;TanhBackward0&gt;)\n</code></pre>"},{"location":"tutorials/mt/replacement/","title":"Module Replacement for An Implemented Algorithm","text":"Open in Colab   View on GitHub  <p>RLLTE allows developers to replace settled modules of implemented algorithms to make performance comparison and algorithm improvement.</p>"},{"location":"tutorials/mt/replacement/#use-built-in-modules","title":"Use built-in modules","text":"<p>For instance, we want to use PPO agent to solve Atari games, it suffices to write <code>train.py</code> like: train.py<pre><code>from rllte.agent import PPO\nfrom rllte.env import make_atari_env\n\nif __name__ == \"__main__\":\n    # env setup\n    device = \"cuda:0\"\n    env = make_atari_env(device=device)\n    eval_env = make_atari_env(device=device)\n    # create agent\n    agent = PPO(env=env, \n                eval_env=eval_env, \n                device=device,\n                tag=\"ppo_atari\")\n    # start training\n    agent.train(num_train_steps=5000)\n</code></pre> Run <code>train.py</code> and you'll see the following output: <pre><code>[08/04/2023 03:45:54 PM] - [INFO.] - Invoking RLLTE Engine...\n[08/04/2023 03:45:54 PM] - [INFO.] - ================================================================================\n[08/04/2023 03:45:54 PM] - [INFO.] - Tag               : ppo_atari\n[08/04/2023 03:45:54 PM] - [INFO.] - Device            : NVIDIA GeForce RTX 3090\n[08/04/2023 03:45:55 PM] - [DEBUG] - Agent             : PPO\n[08/04/2023 03:45:55 PM] - [DEBUG] - Encoder           : MnihCnnEncoder\n[08/04/2023 03:45:55 PM] - [DEBUG] - Policy            : OnPolicySharedActorCritic\n[08/04/2023 03:45:55 PM] - [DEBUG] - Storage           : VanillaRolloutStorage\n[08/04/2023 03:45:55 PM] - [DEBUG] - Distribution      : Categorical\n[08/04/2023 03:45:55 PM] - [DEBUG] - Augmentation      : False\n[08/04/2023 03:45:55 PM] - [DEBUG] - Intrinsic Reward  : False\n[08/04/2023 03:45:55 PM] - [DEBUG] - ================================================================================\n[08/04/2023 03:45:56 PM] - [EVAL.] - S: 0           | E: 0           | L: 23          | R: 24.000      | T: 0:00:02    \n[08/04/2023 03:45:57 PM] - [TRAIN] - S: 1024        | E: 8           | L: 44          | R: 99.000      | FPS: 346.187   | T: 0:00:02    \n[08/04/2023 03:45:58 PM] - [TRAIN] - S: 2048        | E: 16          | L: 58          | R: 207.000     | FPS: 514.168   | T: 0:00:03    \n[08/04/2023 03:45:59 PM] - [TRAIN] - S: 3072        | E: 24          | L: 43          | R: 70.000      | FPS: 619.411   | T: 0:00:04    \n[08/04/2023 03:46:00 PM] - [TRAIN] - S: 4096        | E: 32          | L: 43          | R: 67.000      | FPS: 695.523   | T: 0:00:05    \n[08/04/2023 03:46:00 PM] - [INFO.] - Training Accomplished!\n[08/04/2023 03:46:00 PM] - [INFO.] - Model saved at: /export/yuanmingqi/code/rllte/logs/ppo_atari/2023-08-04-03-45-54/model\n</code></pre></p> <p>Suppose we want to use a <code>ResNet-based</code> encoder, it suffices to replace the encoder module using <code>.set</code> function: train.py<pre><code>from rllte.agent import PPO\nfrom rllte.env import make_atari_env\nfrom rllte.xploit.encoder import EspeholtResidualEncoder\n\nif __name__ == \"__main__\":\n    # env setup\n    device = \"cuda:0\"\n    env = make_atari_env(device=device)\n    eval_env = make_atari_env(device=device)\n    # create agent\n    feature_dim = 512\n    agent = PPO(env=env, \n                eval_env=eval_env, \n                device=device,\n                tag=\"ppo_atari\",\n                feature_dim=feature_dim)\n    # create a new encoder\n    encoder = EspeholtResidualEncoder(\n        observation_space=env.observation_space,\n        feature_dim=feature_dim)\n    # set the new encoder\n    agent.set(encoder=encoder)\n    # start training\n    agent.train(num_train_steps=5000)\n</code></pre> Run <code>train.py</code> and you'll see the old <code>MnihCnnEncoder</code> has been replaced by <code>EspeholtResidualEncoder</code>: <pre><code>[08/04/2023 03:46:38 PM] - [INFO.] - Invoking RLLTE Engine...\n[08/04/2023 03:46:38 PM] - [INFO.] - ================================================================================\n[08/04/2023 03:46:38 PM] - [INFO.] - Tag               : ppo_atari\n[08/04/2023 03:46:38 PM] - [INFO.] - Device            : NVIDIA GeForce RTX 3090\n[08/04/2023 03:46:38 PM] - [DEBUG] - Agent             : PPO\n[08/04/2023 03:46:38 PM] - [DEBUG] - Encoder           : EspeholtResidualEncoder\n[08/04/2023 03:46:38 PM] - [DEBUG] - Policy            : OnPolicySharedActorCritic\n[08/04/2023 03:46:38 PM] - [DEBUG] - Storage           : VanillaRolloutStorage\n[08/04/2023 03:46:38 PM] - [DEBUG] - Distribution      : Categorical\n[08/04/2023 03:46:38 PM] - [DEBUG] - Augmentation      : False\n[08/04/2023 03:46:38 PM] - [DEBUG] - Intrinsic Reward  : False\n[08/04/2023 03:46:38 PM] - [DEBUG] - ================================================================================\n...\n</code></pre> For more replaceable modules, please refer to https://docs.rllte.dev/api/.</p>"},{"location":"tutorials/mt/replacement/#using-custom-modules","title":"Using custom modules","text":"<p>Developers can also perform replacement using custom modules, see Make A Custom Module for more details.</p>"}]}