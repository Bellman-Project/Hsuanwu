{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "The experiments section consists of two parts:\n",
    "- PART1: Demonstrating the performance of our implementations of various intrinsic rewards;\n",
    "- PART2: Discussion of various issues of the application of intrinsic rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary work\n",
    "\n",
    "- Mingqi:\n",
    "  - Update the rllte framework to adapt the latest reward class; [DONE]\n",
    "  - Prepare the paper framework and write some preliminary contents. [DONE]\n",
    "- Roger:\n",
    "  - Finish the rest of the transfer work; [DONE]\n",
    "  - Implement the *Disagree* reward module; [DONE]\n",
    "  - Check the correctness of the workflow of the implemented modules.\n",
    "\n",
    "- I have available training results of PPO on 57 atari games, so we don't need to train them again.\n",
    "- Use Super Mario 1-1 without extrinsic rewards to test correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Setting\n",
    "\n",
    "When not specified otherwise in the following Qs, this is the default config to use for ALL rewards in ALL cases. Until we answer Q1,Q2 and Q3. From there, we will use the best config for each reward.\n",
    "\n",
    "- image input preprocessing: x / 255.\n",
    "- reward normalization: rms\n",
    "- combination of int. and ext. rewards: R=E+I\n",
    "- reward filter: False\n",
    "- update proportion (see https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_rnd_envpool.py#L468): 1.0\n",
    "\n",
    "Make sure to debug them all in Super Mario world 1 level 1. Best environment to debug exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environments to use\n",
    "\n",
    "Train on **intrinsic rewards only** in these environments to measue how good for **exploration** the algorithms are\n",
    "- Atari hard exploration games\n",
    "    - Montezumas Revenge\n",
    "    - Pitfall\n",
    "    - PrivateEye\n",
    "    - Gravitar\n",
    "    - Venture\n",
    "    - Hero\n",
    "    - Frostbite\n",
    "    - Solaris\n",
    "- SuperMarioBros-1-1-v3\n",
    "\n",
    "Train on **intrinsic + extrinsic** in these environments to measure if they help achieving better performance\n",
    "- All other Atari games\n",
    "- Crafter\n",
    "- Procgen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART1\n",
    "\n",
    "Using the baseline settings, report the performance for all intrinsic rewards \n",
    "\n",
    "- backbone algorithms: PPO, SAC\n",
    "- rewards: all rewards modules\n",
    "- games: \n",
    "  + 9 atari games (image-based obs, discrete actions), 1e+7 steps\n",
    "    + Exp.: PPO+Int. Reward\n",
    "  \n",
    "  + Super Mario World 1 Level 1\n",
    "    + Exp.: PPO+Int. Reward (all rewards should work well here)\n",
    "\n",
    "  + 4 bullet games (state-based obs, box actions), 1e+6 steps\n",
    "    + Exp.: SAC+Int. Reward\n",
    "- workload:\n",
    "  + Mingqi: RE3, RISE, PseudosCounts, NGU, REVD\n",
    "  + Roger: RND, ICM, RIDE, E3B, Disagree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART2\n",
    "- General principles: one question only uses *1 kind of games that is most appropriate for the current question + 1 algo.*\n",
    "\n",
    "## Tunning Intrinsic Rewards\n",
    "The goal of this questions is to find which setting each of the intrinsic rewards needs to get best performance\n",
    "At the end of this section, we can identify for each reward, it's best config\n",
    "\n",
    "e.g. RND: obs_rms=True, rew_rms=True, forward_filter=True\n",
    "e.g. E3B: obs_rms=False, rew_rms=True, forward_filter=False\n",
    "\n",
    "### Q1: The impact of different Reward normalization mechanisms on final performance. (Roger)\n",
    "- RL algo: PPO (if you use E3B, use PPO not SAC)\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: \n",
    "  + vanilla\n",
    "  + rms\n",
    "  + min-max\n",
    "- games: \n",
    "  + 9 atari games same as part1\n",
    "  + SuperMarioBros-1-1-v3\n",
    "- framework: rllte\n",
    "\n",
    "### Q2: The impact of different Observation normalization mechanisms on final performance. (Roger)\n",
    "- RL algo: PPO\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: \n",
    "  + x/255.\n",
    "  + rms\n",
    "- games:\n",
    "  + 9 atari games same as part1\n",
    "  + SuperMarioBros-1-1-v3\n",
    "- framework: rllte\n",
    "  \n",
    "### Q3: The impact of ForwardRewardFilter mechanisms on final performance. (Roger)\n",
    "- RL algo: PPO\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates:\n",
    "  + Use RewardFilter + don't cut when done=True in value estimation\n",
    "  - Don't use RewardFilter + cut when done=True in value estimation\n",
    "- games: \n",
    "  + 9 atari games same as part1\n",
    "  + SuperMarioBros-1-1-v3\n",
    "- framework: rllte\n",
    "\n",
    "### Q4: The co-learning dynamics of policies and intrinsic rewards: (Roger)\n",
    "- The problem is that many intrinsic rewards require learning auxiliary models (e.g. inverse dynamics model, forward dynamics model, etc.) and it is not clear how to co-learn them with the policy.\n",
    "- RL algo: PPO\n",
    "- candidates: \n",
    "  + update_proportion = 0.25\n",
    "  + update_proportion = 0.5\n",
    "  + update_proportion = 0.75\n",
    "  + update_proportion = 0.1\n",
    "\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- games: \n",
    "  + 9 atari games same as part1\n",
    "  + SuperMarioBros-1-1-v3\n",
    "- framework: rllte\n",
    "\n",
    "## Optimizing the intrinsic rewards\n",
    "\n",
    "The goal of this section is to learn which are the best ways to optimize the RL algos with intrinsic rewards\n",
    "Starting in this section, for each reward we will use the best config found with Q1, Q2, Q3, Q4\n",
    "\n",
    "At the end of this section we know for each algorithm, how to configure it to better optimize the intrinsic rewards\n",
    "\n",
    "e.g. PPO+RND: Separate adv estimation + no LSTM\n",
    "e.g. PPO+E3B: Vanilla version R=ext+int + LSTM\n",
    "\n",
    "### Q5: The impact of different integration pattern on final performance, only for on-policy setting. (Mingqi)\n",
    "- RL algo: PPO \n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates:\n",
    "  + vanilla version: ext. reward + int. reward -> adv. estimation -> policy update, only one branch in the value network.\n",
    "  + cleanrl version: separate adv. estimation and two branches in the value network\n",
    "  + RE3 version: multiply the estimated advantages by the intrinsic rewards \n",
    "- games: \n",
    "  + 9 atari games same as part1\n",
    "  + SuperMarioBros-1-1-v3\n",
    "- framework: rllte\n",
    "\n",
    "### Q6: Is memory required for better optimizing intrinsic rewards? (Roger)\n",
    "- RL algo: PPO\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: \n",
    "  + LSTM policy\n",
    "  + Vanilla policy\n",
    "- games: \n",
    "  + 9 atari games same as part1\n",
    "  + SuperMarioBros-1-1-v3\n",
    "  + SuperMarioBrosRandomLevels\n",
    "- framework: rllte\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "The goal of this section is to study recent research questions of interest in the literature.\n",
    "\n",
    "e.g. Optimizing multiple intrinsic rewards together\n",
    "e.g. intrinsic rewards in contextual MDPs\n",
    "\n",
    "### Q7: The performance of mixed intrinsic rewards. (Mingqi)\n",
    "- RL algo: PPO\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates:\n",
    "  - global + episodic (1)\n",
    "  + RND+E3B\n",
    "  + ICM+E3B\n",
    "  + RIDE+E3B\n",
    "\n",
    "  - global + episodic (2)\n",
    "  + RND+RE3\n",
    "  + ICM+RE3\n",
    "  + RIDE+RE3\n",
    "\n",
    "  - global + global\n",
    "  + RND+ICM\n",
    "  + RND+RIDE\n",
    "  + ICM+RIDE\n",
    "\n",
    "- games: \n",
    "  + 9 atari games same as part1. \n",
    "  + SuperMarioBros-1-1-v3\n",
    "  + SuperMarioBrosRandomLevels\n",
    "  + Procgen subset\n",
    "- framework: rllte\n",
    "  \n",
    "\n",
    "### Q8: Which intrinsic rewards generalize better in Contextual MDPS? (Roger)\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: LSTM policy vs CNN policy.\n",
    "- games: \n",
    "  + Procgen subset\n",
    "  + SuperMarioBrosRandomLevels\n",
    "  + Crafter (?)\n",
    "- framework: rllte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31/01/2024 by Mingqi\n",
    "- transfer all the reward modules from `experimental` folder to `rllte.xplore.reward`, old moduels are moved to `rllte/explore/reward/backup`\n",
    "- updated the `on_policy_agent.py (Line 132-169)` to adapt to the new reward base. For the convenience of experiments, we compute the irs directly without using `if self.irs is not None`. And `R=E+I`\n",
    "- updated the `.compute (Line 132-169)` function of `BaseReward`, now it requires the samples to contain all the potentially useful data:\n",
    "    ``` py\n",
    "    for key in ['observations', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'next_observations']:\n",
    "        assert key in samples.keys(), f\"Key {key} is not in samples.\"\n",
    "    ```\n",
    "    It is simpler to understand and we can let `.compute, .watch, .update` have same arguments.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Fix pseudo_count() function for all rewards that use pseudo_counts\n",
    "    + Change fixed memory for deques\n",
    "- Implement RewardFilter as in (https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_rnd_envpool.py)\n",
    "    - When computing values and using ForwardFilter, dont cut the value estimation when done=True \n",
    "        + see Cleanrl script linked above Line 411-420\n",
    "- Change `off_policy_agent.py` to adapt to the new reward class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31/01/2024 by Roger\n",
    "- Added `normalize()` in the base reward class and `obs_rms` bool for all rewards (some with default to True and some to False)\n",
    "    + Implemented obs normalization logic in the base reward and calls to normalize in `compute()` and `update()`\n",
    "    + Added initialization of obs_norm parameters in `on_policy_agent.py (line 109-122)` based on cleanrl code\n",
    "\n",
    "- Added `update_proportion` parameter to control how big the updates are. Necessary to answer Q6\n",
    "    + Updated all `update()` functions for the rewards to use the `update_proportion` parameter\n",
    "\n",
    "- Added SuperMario bros and checked PPO can solve it. (it works in 1M steps only)\n",
    "\n",
    "- Added calls to `self.update()` at each `self.compute()` in rewards\n",
    "- Changed intrinsic reward Encoder to Mnih encoder to process 84x84 images\n",
    "- Big change to schedule to better define the study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01/02/2024 by Mingqi\n",
    "- fixed the `pseudo_counts.py` with a reasonable episodic memory;\n",
    "- fixed RIDE, NGU;\n",
    "- corrected the interpretations of all the arguments and code blocks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
