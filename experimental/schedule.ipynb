{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "The experiments section consists of two parts:\n",
    "- PART1: Demonstrating the performance of our implementations of various intrinsic rewards;\n",
    "- PART2: Discussion of various issues of the application of intrinsic rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary work\n",
    "\n",
    "- Mingqi:\n",
    "  - Update the rllte framework to adapt the latest reward class; [DONE]\n",
    "  - Prepare the paper framework and write some preliminary contents. [DONE]\n",
    "- Roger:\n",
    "  - Finish the rest of the transfer work; [DONE]\n",
    "  - Implement the *Disagree* reward module; [DONE]\n",
    "  - Check the correctness of the workflow of the implemented modules.\n",
    "\n",
    "- I have available training results of PPO on 57 atari games, so we don't need to train them again.\n",
    "- Use Super Mario 1-1 without extrinsic rewards to test correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Setting\n",
    "\n",
    "- image input preprocessing: x / 255.\n",
    "- reward normalization: rms\n",
    "- combination of int. and ext. rewards: R=E+I\n",
    "    - num value functions: 1\n",
    "- reward filter: False\n",
    "- update proportion (see https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_rnd_envpool.py#L468): 1.0\n",
    "\n",
    "Make sure to debug them all in Super Mario world 1 level 1. Best environment to debug exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environments to use\n",
    "\n",
    "Train on **intrinsic rewards only** in these environments to measue how good for **exploration** the algorithms are\n",
    "- SuperMarioBros-1-1-v3\n",
    "\n",
    "Train on **intrinsic + extrinsic** in these environments to measure if they help achieving better performance\n",
    "- MyWayHome-v1 (from envpool vizdoom)\n",
    "- Procgen Maze (1 seed. Normal / Memory distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART1\n",
    "\n",
    "Using the baseline settings, report the performance for all intrinsic rewards \n",
    "\n",
    "- backbone algorithms: PPO\n",
    "- rewards: all rewards modules\n",
    "- games: \n",
    "  + Super Mario World 1 Level 1\n",
    "    + Exp.: PPO+Int. Reward\n",
    "- workload:\n",
    "  + Roger: PPO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Intrinsic Reward  | Obs Normalization | Reward Norm. | Weight Init. | Update Proportion | Global vs Episodic | Memory Required |\n",
    "|------------------|-------------------|--------------|--------------|-------------------|--------------------|-----------------|\n",
    "| Pseudocounts     |T                  |RMS           |              |                   |                    |                 |\n",
    "| ICM              |T                  |RMS           |              |                   |                    |                 |\n",
    "| RND              |T                  |MINMAX        |              |                   |                    |                 |\n",
    "| E3B              |F                  |RMS           |              |                   |                    |                 |\n",
    "| RIDE             |T                  |RMS           |              |                   |                    |                 |\n",
    "| RE3              |F                  |MINMAX        |              |                   |                    |                 |\n",
    "| NGU              |T                  |MINMAX        |              |                   |                    |                 |\n",
    "| Disagreement     |T                  |RMS           |              |                   |                    |                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART2\n",
    "- General principles: one question only uses *1 kind of games that is most appropriate for the current question + 1 algo.*\n",
    "\n",
    "## Tunning Intrinsic Rewards\n",
    "The goal of this questions is to find which setting each of the intrinsic rewards needs to get best performance\n",
    "At the end of this section, we can identify for each reward, it's best config\n",
    "\n",
    "e.g. RND: obs_rms=True, rew_rms=True, forward_filter=True\n",
    "e.g. E3B: obs_rms=False, rew_rms=True, forward_filter=False\n",
    "\n",
    "### Q1: The impact of different Observation normalization mechanisms on final performance. (Roger)\n",
    "- RL algo: PPO\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: \n",
    "  + x/255.\n",
    "  + rms\n",
    "- games:\n",
    "  + SuperMarioBros-1-1-v3\n",
    "- framework: rllte\n",
    "\n",
    "### Q2: The impact of different Reward normalization mechanisms on final performance. (Roger)\n",
    "- RL algo: PPO (if you use E3B, use PPO not SAC)\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: \n",
    "  + vanilla\n",
    "  + rms\n",
    "  + min-max\n",
    "- games: \n",
    "  + SuperMarioBros-1-1-v3\n",
    "- framework: rllte\n",
    "\n",
    "### Q3: The co-learning dynamics of policies and intrinsic rewards: (Roger)\n",
    "- The problem is that many intrinsic rewards require learning auxiliary models (e.g. inverse dynamics model, forward dynamics model, etc.) and it is not clear how to co-learn them with the policy.\n",
    "- RL algo: PPO\n",
    "- candidates: \n",
    "  + update_proportion = 0.25\n",
    "  + update_proportion = 0.75\n",
    "  + update_proportion = 1.0\n",
    "\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- games: \n",
    "  + SuperMarioBros-1-1-v3\n",
    "- framework: rllte\n",
    "\n",
    "### Q4: The impact of ForwardRewardFilter mechanisms on final performance. (Roger)\n",
    "- RL algo: PPO\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates:\n",
    "  + Use RewardFilter + don't cut when done=True in value estimation\n",
    "  - Don't use RewardFilter + cut when done=True in value estimation\n",
    "- games: \n",
    "  + SuperMarioBros-1-1-v3\n",
    "- framework: rllte\n",
    "\n",
    "### Q5: Is memory required for better optimizing intrinsic rewards? (Roger)\n",
    "- RL algo: PPO\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: \n",
    "  + LSTM policy\n",
    "  + Vanilla policy\n",
    "- games: \n",
    "  + SuperMarioBros-1-1-v3\n",
    "  + SuperMarioBrosRandomLevels\n",
    "- framework: rllte\n",
    "\n",
    "## Optimizing the intrinsic rewards\n",
    "\n",
    "The goal of this section is to learn which are the best ways to optimize the RL algos with intrinsic rewards\n",
    "Starting in this section, for each reward we will use the best config found with Q1, Q2, Q3, Q4\n",
    "\n",
    "At the end of this section we know for each algorithm, how to configure it to better optimize the intrinsic rewards\n",
    "\n",
    "e.g. PPO+RND: Separate adv estimation\n",
    "e.g. PPO+E3B: Vanilla version R=ext+int\n",
    "\n",
    "### Q6: The impact of different integration pattern on final performance, only for on-policy setting. (Mingqi)\n",
    "- RL algo: PPO \n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates:\n",
    "  + vanilla version: ext. reward + int. reward -> adv. estimation -> policy update, only one branch in the value network.\n",
    "  + cleanrl version: separate adv. estimation and 2 different value networks (important!)\n",
    "    - this has to allow forwardFilter (Q3) in the intrinsic rewards GAE and normal GAE in the extrinsic rewards\n",
    "  + RE3 version: multiply the estimated advantages by the intrinsic rewards \n",
    "- games: \n",
    "  + MyWayHome-v1\n",
    "  + ProcGenMaze 1 seed Normal distribution\n",
    "  + ProcGenMaze infinite seeds Normal distribution\n",
    "  + ProcGenMaze 1 seed Memory distribution\n",
    "  + ProcGenMaze infinite seed Memory distribution\n",
    "- framework: rllte\n",
    "\n",
    "\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "The goal of this section is to study recent research questions of interest in the literature.\n",
    "\n",
    "e.g. Optimizing multiple intrinsic rewards together\n",
    "e.g. intrinsic rewards in contextual MDPs\n",
    "\n",
    "### Q7: The performance of mixed intrinsic rewards. (Mingqi)\n",
    "- RL algo: PPO\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates:\n",
    "  - global + episodic (1)\n",
    "  + RND+E3B\n",
    "  + ICM+E3B\n",
    "  + RIDE+E3B\n",
    "\n",
    "  - global + episodic (2)\n",
    "  + RND+RE3\n",
    "  + ICM+RE3\n",
    "  + RIDE+RE3\n",
    "\n",
    "  - global + global\n",
    "  + RND+ICM\n",
    "  + RND+RIDE\n",
    "  + ICM+RIDE\n",
    "\n",
    "- games: \n",
    "  + SuperMarioBros-1-1-v3\n",
    "  + SuperMarioBrosRandomLevels\n",
    "  + Procgen Maze 1 seed Memory Mode\n",
    "  + Procgen Maze Infinite seeds Memory Mode\n",
    "- framework: rllte\n",
    "  \n",
    "\n",
    "### Q8: Which intrinsic rewards generalize better in Contextual MDPS? (Roger)\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: LSTM policy vs CNN policy.\n",
    "- games: \n",
    "  + Procgen Maze Infinite seeds Memory Mode\n",
    "  + SuperMarioBrosRandomLevels\n",
    "- framework: rllte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31/01/2024 by Mingqi\n",
    "- transfer all the reward modules from `experimental` folder to `rllte.xplore.reward`, old moduels are moved to `rllte/explore/reward/backup`\n",
    "- updated the `on_policy_agent.py (Line 132-169)` to adapt to the new reward base. For the convenience of experiments, we compute the irs directly without using `if self.irs is not None`. And `R=E+I`\n",
    "- updated the `.compute (Line 132-169)` function of `BaseReward`, now it requires the samples to contain all the potentially useful data:\n",
    "    ``` py\n",
    "    for key in ['observations', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'next_observations']:\n",
    "        assert key in samples.keys(), f\"Key {key} is not in samples.\"\n",
    "    ```\n",
    "    It is simpler to understand and we can let `.compute, .watch, .update` have same arguments.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Fix pseudo_count() function for all rewards that use pseudo_counts [DONE]\n",
    "    + Change fixed memory for deques\n",
    "- Implement RewardFilter as in (https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_rnd_envpool.py) [DONE]\n",
    "    - When computing values and using ForwardFilter, dont cut the value estimation when done=True \n",
    "        + see Cleanrl script linked above Line 411-420\n",
    "- Change `off_policy_agent.py` to adapt to the new reward class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31/01/2024 by Roger\n",
    "- Added `normalize()` in the base reward class and `obs_rms` bool for all rewards (some with default to True and some to False)\n",
    "    + Implemented obs normalization logic in the base reward and calls to normalize in `compute()` and `update()`\n",
    "    + Added initialization of obs_norm parameters in `on_policy_agent.py (line 109-122)` based on cleanrl code\n",
    "\n",
    "- Added `update_proportion` parameter to control how big the updates are. Necessary to answer Q6\n",
    "    + Updated all `update()` functions for the rewards to use the `update_proportion` parameter\n",
    "\n",
    "- Added SuperMario bros and checked PPO can solve it. (it works in 1M steps only)\n",
    "\n",
    "- Added calls to `self.update()` at each `self.compute()` in rewards\n",
    "- Changed intrinsic reward Encoder to Mnih encoder to process 84x84 images\n",
    "- Big change to schedule to better define the study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01/02/2024 by Mingqi\n",
    "- fixed the `pseudo_counts.py` with a reasonable episodic memory;\n",
    "- fixed RIDE, NGU;\n",
    "- corrected the interpretations of all the arguments and code blocks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05/02/2024 by Mingqi\n",
    "\n",
    "- add the `RewardForwardFilter`, see `base_reward.py (Line68, 88, 105-107)`;\n",
    "- add the `RogerRolloutStorage` class that don't cut the rewards `(Line151-153)` when done is True, see the usage in `test_pc.py`;\n",
    "- I'm drawing the workflow chart of our method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09/02/2024 by Mingqi\n",
    "- add the `rllte_to_cleanrl.py` that combines RLeXplore and CleanRL;\n",
    "- add the `rllte_to_sbs3.py`that combines RLeXplore and SB3;\n",
    "- will finish the flowchart of RLeXplorea and provide an example figure of the learning curves."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
