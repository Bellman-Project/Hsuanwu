{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- backbone algo.: PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: The impact of different normalization mechanisms on final performance.\n",
    "\n",
    "- candidates: vanilla, running mean and std, min-max\n",
    "- reward: RND (counted-based with representation learning), ICM (Curiosity-driven with representation learning), RE3 (information theory-based without representation learning)\n",
    "- env: 9 atari games, 16 procgen games, mario games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: The impact of different integration pattern on final performance. (for on-policy setting)\n",
    "\n",
    "- vanilla version: ext. reward + int. reward -> adv. estimation -> policy update, only one branch in the value network.\n",
    "- cleanrl version: separate adv. estimation and two branches in the value network\n",
    "- RE3 version: multiply the estimated advantages by the intrinsic rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: The performance of mixed intrinsic rewards (different/same types).\n",
    "\n",
    "- candidates: mixture of different/same types of intrinsic rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
