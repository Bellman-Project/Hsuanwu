{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- backbone algo.: PPO / PPO+LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: The impact of different normalization mechanisms on final performance.\n",
    "\n",
    "- candidates: vanilla, running mean and std, min-max\n",
    "- reward: RND (counted-based with representation learning), ICM (Curiosity-driven with representation learning), RE3 (information theory-based without representation learning)\n",
    "    -- Q: Why only this 3?\n",
    "- env: 9 atari games, 16 procgen games, mario games\n",
    "    -- This amount of environments should be more than enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: The impact of different integration pattern on final performance. (for on-policy setting)\n",
    "\n",
    "- vanilla version: ext. reward + int. reward -> adv. estimation -> policy update, only one branch in the value network.\n",
    "- cleanrl version: separate adv. estimation and two branches in the value network\n",
    "- RE3 version: multiply the estimated advantages by the intrinsic rewards. -- Q: Multiply or add? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: The performance of mixed intrinsic rewards (different/same types).\n",
    "\n",
    "- candidates: mixture of different/same types of intrinsic rewards (especially episodic+global intrinsic rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Is memory required for better optimizing intrinsic rewards?\n",
    "- LSTM policy vs CNN policy (e.g. E3B doesnt work well without LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Which intrinsic rewards generalize better in Contextual MDPS?\n",
    "- Compare the performance of all algorithms in fixed envs (e.g. atari game, where env.reset() always takes you to the same worl )\n",
    "- With the performance in contextual envs (e.g. procgen game, where env.reset() takes you to different worlds, or mario levels (train in a subset, evaluate in hidden test levels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
