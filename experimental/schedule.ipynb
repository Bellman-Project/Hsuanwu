{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "The experiments section consists of two parts:\n",
    "- PART1: Demonstrating the performance of our implementations of various intrinsic rewards;\n",
    "- PART2: Discussion of various issues of the application of intrinsic rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary work\n",
    "\n",
    "- Mingqi:\n",
    "  - Update the rllte framework to adapt the latest reward class;\n",
    "  - Prepare the paper framework and write some preliminary contents.\n",
    "- Roger:\n",
    "  - Finish the rest of the transfer work;\n",
    "  - Implement the *Disagree* reward module;\n",
    "  - Check the correctness of the workflow of the implemented modules.\n",
    "\n",
    "- I have available training results of PPO on 57 atari games, so we don't need to train them again.\n",
    "- Use Super Mario 1-1 without extrinsic rewards to test correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Setting\n",
    "\n",
    "- image input preprocessing: \n",
    "    + RND, RE3, Disagreement, REVD, RISE: rms\n",
    "    + E3B, ICM, PseudoCounts, Ride: x / 255.\n",
    "- reward normalization: rms\n",
    "- combination of int. and ext. rewards: R=E+I\n",
    "- reward filter:\n",
    "    + True for: RND, ICM, Disagreement\n",
    "    + False for: E3B, PseudoCounts, RIDE, REVD, RISE, RE3\n",
    "- update proportion (see https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_rnd_envpool.py#L468): 100%\n",
    "\n",
    "- Note: It's possible that some of the baselines don't achieve good performance. In this case it might be necessary to adapt this settings.\n",
    "    + e.g. Maybe RIDE should have reward filter. Maybe all methods should use x/255. \n",
    "    + Make sure to debug them all in Super Mario world 1 level 1. Best environment to debug exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART1\n",
    "\n",
    "- backbone algorithms: PPO, SAC\n",
    "- rewards: all rewards modules\n",
    "- games: \n",
    "  + 9 atari games (image-based obs, discrete actions), 1e+7 steps\n",
    "    + Exp.: PPO+Int. Reward\n",
    "  + Super Mario World 1 Level 1\n",
    "    + Exp.: PPO+Int. Reward (all rewards should work well here)\n",
    "  + 4 bullet games (state-based obs, box actions), 1e+6 steps\n",
    "    + Exp.: SAC+Int. Reward\n",
    "- rewards: all rewards modules\n",
    "- workload:\n",
    "  + Mingqi: RE3, RISE, PseudosCounts, NGU, REVD\n",
    "  + Roger: RND, ICM, RIDE, E3B, Disagree\n",
    "- The 9 atari games are: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART2\n",
    "- General principles: one question only uses *1 kind of games that is most appropriate for the current question + 1 algo.*\n",
    "\n",
    "### Q1: The impact of different Reward normalization mechanisms on final performance. (Mingqi)\n",
    "- RL algo: PPO (if you use E3B, use PPO not SAC)\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: vanilla, running mean and std, min-max\n",
    "- games: 9 atari games same as part1 + Super Mario\n",
    "- framework: rllte\n",
    "  \n",
    "### Q2: The impact of different integration pattern on final performance, only for on-policy setting. (Mingqi)\n",
    "- RL algo: PPO \n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates:\n",
    "  - vanilla version: ext. reward + int. reward -> adv. estimation -> policy update, only one branch in the value network.\n",
    "  - cleanrl version: separate adv. estimation and two branches in the value network\n",
    "  - RE3 version: multiply the estimated advantages by the intrinsic rewards. -- Q: Multiply or add? \n",
    "- games: 9 atari games same as part1 + Super Mario\n",
    "- framework: cleanrl (Roger: Why not Rklte?)\n",
    "\n",
    "### Q3: The performance of mixed intrinsic rewards. (Mingqi)\n",
    "- rewards: ICM, RND, RIDE, RE3, Roger: I think we must use E3B here since https://arxiv.org/abs/2306.03236 uses RND+E3B and works well!\n",
    "- candidates: different/same types.\n",
    "- games: 9 atari games same as part1. Roger: + Super Mario (1 level only / Multiple levels). The Idea is that RND+E3B should work better when using multiple levels than not RND alone \n",
    "- framework: rllte\n",
    "  \n",
    "### Q4: Is memory required for better optimizing intrinsic rewards? (Roger)\n",
    "- RL algo: PPO\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: LSTM policy vs CNN policy.\n",
    "- games: 9 atari games same as part1 + Super Mario\n",
    "- framework: rllte\n",
    "\n",
    "### Q5: Which intrinsic rewards generalize better in Contextual MDPS? (Roger)\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: LSTM policy vs CNN policy.\n",
    "- games: procgen games / Super Mario Multi Level\n",
    "- framework: rllte\n",
    "\n",
    "### Q6: The co-learning dynamics of policies and intrinsic rewards: (Roger)\n",
    "- The problem is that many intrinsic rewards require learning auxiliary models (e.g. inverse dynamics model, forward dynamics model, etc.) and it is not clear how to co-learn them with the policy.\n",
    "- RL algo: PPO\n",
    "- candidates: 1 entire epoch over on-policy data every policy update | 1 single batch update every policy update (i.e. delayed updates) | Masked updates (i.e. like in cleanrl RND implementation)\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- games: 9 atari games same as part1 + Super Mario\n",
    "- framework: rllte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31/01/2024 by Mingqi\n",
    "- transfer all the reward modules from `experimental` folder to `rllte.xplore.reward`, old moduels are moved to `rllte/explore/reward/backup`\n",
    "- updated the `on_policy_agent.py (Line 132-169)` to adapt to the new reward base. For the convenience of experiments, we compute the irs directly without using `if self.irs is not None`. And `R=E+I`\n",
    "- updated the `.compute (Line 132-169)` function of `BaseReward`, now it requires the samples to contain all the potentially useful data:\n",
    "    ``` py\n",
    "    for key in ['observations', 'actions', 'rewards', 'terminateds', 'truncateds', 'infos', 'next_observations']:\n",
    "        assert key in samples.keys(), f\"Key {key} is not in samples.\"\n",
    "    ```\n",
    "    It is simpler to understand and we can let `.compute, .watch, .update` have same arguments.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Fix pseudo_count() function for all rewards that use pseudo_counts\n",
    "    + Change fixed memory for deques\n",
    "- Implement RewardFilter as in (https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_rnd_envpool.py)\n",
    "    - When computing values and using ForwardFilter, dont cut the value estimation when done=True \n",
    "        + see Cleanrl script linked above Line 411-420\n",
    "- Change `off_policy_agent.py` to adapt to the new reward class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31/01/2024 by Roger\n",
    "- Added `normalize()` in the base reward class and `obs_rms` bool for all rewards (some with default to True and some to False)\n",
    "    + Implemented obs normalization logic in the base reward and calls to normalize in `compute()` and `update()`\n",
    "    + Added initialization of obs_norm parameters in `on_policy_agent.py (line 109-122)` based on cleanrl code\n",
    "\n",
    "- Added `update_proportion` parameter to control how big the updates are. Necessary to answer Q6\n",
    "    + Updated all `update()` functions for the rewards to use the `update_proportion` parameter\n",
    "- Added calls to `self.update()` at each `self.compute()` in rewards\n",
    "- Changed intrinsic reward Encoder to Mnih encoder to process 84x84 images\n",
    "\n",
    "## Next Steps\n",
    "- How does obs_norm work for value-based algorithms? e.g. SAC?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
