{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "The experiments section consists of two parts:\n",
    "- PART1: Demonstrating the performance of our implementations of various intrinsic rewards;\n",
    "- PART2: Discussion of various issues of the application of intrinsic rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary work\n",
    "\n",
    "- Mingqi:\n",
    "  - Update the rllte framework to adapt the latest reward class;\n",
    "  - Prepare the paper framework and write some preliminary contents.\n",
    "- Roger:\n",
    "  - Finish the rest of the transfer work;\n",
    "  - Implement the *Disagree* reward module;\n",
    "  - Check the correctness of the workflow of the implemented modules.\n",
    "\n",
    "- I have available training results of PPO on 57 atari games, so we don't need to train them again.\n",
    "- Use Super Mario 1-1 without extrinsic rewards to test correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART1\n",
    "\n",
    "- backbone algorithms: PPO, SAC\n",
    "- rewards: all rewards modules\n",
    "- games: \n",
    "  + 9 atari games (image-based obs, discrete actions), 1e+7 steps\n",
    "    + Exp.: PPO+Int. Reward\n",
    "  + Super Mario World 1 Level 1\n",
    "    + Exp.: PPO+Int. Reward (all rewards should work well here)\n",
    "  + 4 bullet games (state-based obs, box actions), 1e+6 steps\n",
    "    + Exp.: SAC+Int. Reward\n",
    "- rewards: all rewards modules\n",
    "- workload:\n",
    "  + Mingqi: RE3, RISE, PseudosCounts, NGU, REVD\n",
    "  + Roger: RND, ICM, RIDE, E3B, Disagree\n",
    "- The 9 atari games are: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART2\n",
    "- General principles: one question only uses *1 kind of games that is most appropriate for the current question + 1 algo.*\n",
    "\n",
    "### Q1: The impact of different normalization mechanisms on final performance. (Mingqi)\n",
    "- RL algo: PPO (if you use E3B, use PPO not SAC)\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: vanilla, running mean and std, min-max\n",
    "- games: 9 atari games same as part1 + Super Mario\n",
    "- framework: rllte\n",
    "  \n",
    "### Q2: The impact of different integration pattern on final performance, only for on-policy setting. (Mingqi)\n",
    "- RL algo: PPO (if you use E3B, use PPO not SAC)\n",
    "- rewards: ICM, RND, RIDE, RE3, (+E3B?)\n",
    "- candidates:\n",
    "  - vanilla version: ext. reward + int. reward -> adv. estimation -> policy update, only one branch in the value network.\n",
    "  - cleanrl version: separate adv. estimation and two branches in the value network\n",
    "  - RE3 version: multiply the estimated advantages by the intrinsic rewards. -- Q: Multiply or add? \n",
    "- games: 9 atari games same as part1 + Super Mario\n",
    "- framework: cleanrl (Roger: Why not Rklte?)\n",
    "\n",
    "### Q3: The performance of mixed intrinsic rewards. (Mingqi)\n",
    "- rewards: ICM, RND, RIDE, RE3, Roger: I think we must use E3B here since https://arxiv.org/abs/2306.03236 uses RND+E3B and works well!\n",
    "- candidates: different/same types.\n",
    "- games: 9 atari games same as part1. Roger: + Super Mario (1 level only / Multiple levels). The Idea is that RND+E3B should work better when using multiple levels than not RND alone \n",
    "- framework: rllte\n",
    "  \n",
    "### Q4: Is memory required for better optimizing intrinsic rewards? (Roger)\n",
    "- RL algo: PPO\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: LSTM policy vs CNN policy.\n",
    "- games: 9 atari games same as part1 + Super Mario\n",
    "- framework: rllte\n",
    "\n",
    "### Q5: Which intrinsic rewards generalize better in Contextual MDPS? (Roger)\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- candidates: LSTM policy vs CNN policy.\n",
    "- games: procgen games / Super Mario Multi Level\n",
    "- framework: rllte\n",
    "\n",
    "### Q6: The co-learning dynamics of policies and intrinsic rewards: (Roger)\n",
    "- The problem is that many intrinsic rewards require learning auxiliary models (e.g. inverse dynamics model, forward dynamics model, etc.) and it is not clear how to co-learn them with the policy.\n",
    "- RL algo: PPO\n",
    "- candidates: 1 entire epoch over on-policy data every policy update | 1 single batch update every policy update (i.e. delayed updates) | Masked updates (i.e. like in cleanrl RND implementation)\n",
    "- rewards: ICM, RND, RIDE, RE3, E3B\n",
    "- games: 9 atari games same as part1 + Super Mario\n",
    "- framework: rllte"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
